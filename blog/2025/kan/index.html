<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> KAN (Kolmogorov-Arnold Networks) | Harshwardhan Sanjay Fartale </title> <meta name="author" content="Harshwardhan Sanjay Fartale"> <meta name="description" content="An Alternative to traditional MLPs"> <meta name="keywords" content="harshwardhan fartale, harshwardhan, portfolio, Harsh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%B0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emharsha1812.github.io/blog/2025/kan/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Harshwardhan</span> Sanjay Fartale </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Harshwardhan </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">KB </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/paperstack/">PaperStack </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <style>ul.task-list input[type="checkbox"]:checked{accent-color:aqua!important}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">KAN (Kolmogorov-Arnold Networks)</h1> <p class="post-meta"> Created in January 07, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/kan-deep-learning-research"> <i class="fa-solid fa-hashtag fa-sm"></i> kan,deep-learning,research</a>   ·   <a href="/blog/category/llm-machine-learning-python"> <i class="fa-solid fa-tag fa-sm"></i> llm,machine-learning,python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="1-mathematical-foundations--architecture">1. Mathematical Foundations &amp; Architecture</h2> <p><strong>Kolmogorov-Arnold Representation Theorem:</strong> KANs are founded on a classic result by A. N. Kolmogorov and V. Arnold, which states that <em>any continuous multivariate function can be represented as a finite superposition of univariate functions</em>(<a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov%E2%80%99s%20theorem%2C%20proposed%20in%201957%2C,The%20CFL%20condition%2C%20introduced%20by" rel="external nofollow noopener" target="_blank">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a>). In practical terms, this theorem guarantees that for a function \(f(x_1,\dots,x_n)\), there exist some continuous 1D functions \(\{\Phi_q\}\) and \(\{\Psi_{q,p}\}\) such that:</p> \[f(x_1,\ldots,x_n) \;=\; \sum_{q=0}^{2n} \; \Phi_q\!\Big( \sum_{p=1}^n \Psi_{q,p}(x_p) \Big)\,,\] <p>i.e. \(f\) can be decomposed into <strong>inner</strong> univariate functions \(\Psi_{q,p}\) (each depending on a single input variable $x_p$) and <strong>outer</strong> univariate functions \(\Phi_q\) aggregated by addition. This theorem provides a constructive blueprint for function approximation using single-variable building blocks, which is the key inspiration for KANs</p> <p><strong>KAN Architecture</strong> - Instead of the traditional neuron model with linear weighted sums and fixed activations, a KAN implements the above idea by making <strong>each edge</strong> of the network carry a <em>learnable univariate function</em>. In other words, every connection between neurons is parameterized as a nonlinear function (originally chosen as a B-spline) rather than a scalar weight. Each neuron simply sums up the outputs of the incoming edge-functions. Formally, if \(z_i^{(l)}\) denotes the \(i\)-th activation in layer \(l\), then a <strong>KAN layer</strong> computes each output neuron \(j\) as: (<a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than" rel="external nofollow noopener" target="_blank">OpenReview</a>) \(z_j^{(l+1)} \;=\; \sum_{i=1}^{N_l} f_{ij}^{(l)}\!\Big(z_i^{(l)}\Big)\,,\)</p> <p>where \(f_{ij}^{(l)}: \mathbb{R}\to\mathbb{R}\) is a learnable univariate function on the edge from neuron \(i\) (layer \(l\)) to neuron \(j\) (layer \(l+1\)). There are no separate linear weight matrices; the nonlinearity of \(f_{ij}\) itself provides the transformation. In the <em>shallowest</em> case (two-layer KAN), this architecture directly mirrors Kolmogorov’s decomposition: the first layer learns inner functions \(h_{p}(x_p)\) on each input dimension, and the second layer learns outer functions \(g_q(\cdot)\) that combine those results <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement" rel="external nofollow noopener" target="_blank">Quanta Magazine</a>.</p> <p><strong>Parameterized Functions (B-Splines):</strong> In practice, each learnable edge-function \(f_{ij}\) is parameterized as a spline (often a B-spline) with a set of control points that can be tuned during training. B-splines are piecewise polynomial curves defined by control points, offering a flexible yet smooth basis for approximating arbitrary 1D functions. By adjusting the control points, the shape of the spline changes locally without affecting the entire function. This choice ensures the learned activation functions are <em>smooth</em> addressing potential non-smoothness in Kolmogorov’s original construction and stable to train. Each edge thus has multiple parameters (the spline control values) instead of a single weight. For example, a KAN might initialize each \(f_{ij}\) as a near-linear spline and then let training mold each into the required nonlinear shape. This edge-centric design lets KANs <em>dynamically adapt their activation functions</em> to the data, rather than relying on a fixed function like ReLU or tanh.</p> <p><strong>Illustrative Pseudocode:</strong> The following pseudocode contrasts a single layer of an MLP vs. a KAN:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanmlp.svg-480.webp 480w,/assets/img/kanmlp.svg-800.webp 800w,/assets/img/kanmlp.svg-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/kanmlp.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Comparison of MLP Layer with KAN Layer in Pytorch </div> <p>In the KAN layer, <code class="language-plaintext highlighter-rouge">f_ij</code> is a learned function (e.g. a spline) specific to edge \((i\to j)\), replacing both the weight and the neuron’s activation for that connection. The neuron simply aggregates these contributions (here via summation). Deep KANs can be built by stacking such layers, allowing composition of these univariate transformations across multiple levels.</p> <h2 id="2-comparison-with-mlps">2. Comparison with MLPs</h2> <p><strong>Structural Differences:</strong> Traditional Multi-Layer Perceptrons (MLPs) use <em>linear weights</em> and <em>fixed activation functions at neurons</em>, whereas KANs use <em>no linear weights at all</em> – every “weight” is replaced by a flexible function on the input signal. In effect, MLPs learn parameters for <strong>nodes</strong> (the weight matrix between layers is trained, then a fixed nonlinearity like ReLU is applied), while KANs learn parameters for <strong>edges</strong> (each connection has a trainable nonlinear mapping). This leads to a duality: <em>MLP = fixed nonlinearity + learned linear weights; KAN = fixed linear sum + learned nonlinear functions</em>. The figure below (from Liu et al. 2024) illustrates this difference, highlighting that MLPs apply activations at neurons (circles) whereas KANs apply learned functions on each connecting edge before summing.(<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here" rel="external nofollow noopener" target="_blank">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanvsmlp-480.webp 480w,/assets/img/kanvsmlp-800.webp 800w,/assets/img/kanvsmlp-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/kanvsmlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Source: <a href="https://arxiv.org/pdf/2404.19756" target="_blank" rel="external nofollow noopener">Liu et al. (2024)</a> </div> <p><strong>Learnable Functions vs Fixed Weights:</strong> In an MLP, the transformation from layer to layer is \(\sigma(Wx + b)\), with \(\sigma\) (e.g. ReLU) fixed and \(W,b\) learned. In a KAN, the transformation is \(\sum_i f_i(x_i)\) (plus bias if needed), with each $f_i$ being learned and no separate \(W\). Essentially, KANs “allocate” more flexibility per connection, whereas MLPs rely on combining many fixed nonlinear units to build complexity. This means KANs move the bulk of learnable parameters into the activation functions themselves, often resulting in <em>fewer total connections</em> needed than an equivalent MLP (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node" rel="external nofollow noopener" target="_blank">Trying Kolmogorov-Arnold Networks in Practice</a>).</p> <p><strong>Expressive Power (Universal Approximation):</strong> Both MLPs and KANs are universal function approximators, but via different theorems. MLPs leverage the Universal Approximation Theorem (with enough neurons, an MLP can approximate any continuous function on a domain), while KANs directly leverage the Kolmogorov-Arnold (K-A) theorem to construct such approximations. In theory, a single hidden-layer KAN with sufficiently complex edge functions can exactly represent any continuous function (the K-A theorem provides an existence proof), whereas an MLP might require many more neurons or layers to approximate the same function with fixed activations. KANs thus excel at modeling functions with complex or “spiky” behavior in each input dimension, because each edge can carve out a detailed univariate relationship. In practice, KANs implement the K-A decomposition <em>explicitly</em>, using B-spline basis functions to approximate the required univariate mappings. This can translate to <em>greater expressivity per parameter</em>.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting" rel="external nofollow noopener" target="_blank">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>).</p> <p><strong>Parameter Efficiency &amp; Neural Scaling:</strong> A striking reported advantage is that <em>much smaller KANs can achieve accuracy comparable or superior to much larger MLPs</em> on certain tasks. Each KAN edge function (with, say, $k$ control points) can encode a nonlinear relation that an MLP might need multiple neurons and layers to capture. Empirically, Liu <em>et al.</em> (2024) found KANs follow faster <strong>neural scaling laws</strong> – the error decreases more rapidly as model size increases, compared to MLPs. In other words, to reach a given accuracy, a KAN required fewer trainable parameters than an MLP in their tests. The flexibility of splines allows KANs to fit complex patterns without blowing up the network width/depth. One study noted that KANs can <em>match</em> MLP performance at equal parameter counts, and sometimes exceed it, though they require careful tuning (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=KANs%20definitely%20don%27t%20feel%20like,compared%20to%20regular%20neural%20networks" rel="external nofollow noopener" target="_blank">Trying Kolmogorov-Arnold Networks in Practice</a>). The original KAN paper demonstrated that a KAN with significantly fewer nodes could outperform a dense ReLU network on function-fitting benchmarks.</p> <p><strong>Continuous Learning and Locality:</strong> Because each KAN weight is a localized function (with local control points), learning in a KAN can be more localized. This has implications for <strong>continual learning</strong>. In standard nets, fine-tuning on new data often alters weights globally and can erode old capabilities (catastrophic forgetting). In KANs, adding new data primarily adjusts the spline control points <em>in relevant regions of the input space</em>, leaving other regions (and other functions) mostly unchanged. For example, if a KAN-based language model learns a new vocabulary or coding style, only certain edge-functions for those inputs might reshape, while others retain their previously learned shape. This property means KANs can integrate new knowledge without overwriting all weights, potentially enabling more <strong>seamless continual learning</strong>. MLPs, by contrast, have distributed representations where a single weight doesn’t correspond to an isolated input relationship, making targeted updates harder.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,local%20control%20point%20parameters%20change" rel="external nofollow noopener" target="_blank">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>)</p> <p><strong>Interpretability:</strong> A major motivation for KANs is interpretability. In an MLP, each weight by itself is usually not meaningful, and neurons combine many weights making interpretation difficult. In a KAN, each edge’s function $f_{ij}(x)$ can be visualized as a curve, directly showing how the input from neuron $i$ influences neuron $j$ across the range of values. After training, one can <em>extract these learned univariate functions</em> and inspect them.They might correspond to intuitive relations (e.g. an edge function might learn a sinusoidal shape if the output depends sinusoidally on an input).This transparency is especially useful in scientific or engineering tasks where understanding the learned model is as important as its accuracy. MLPs lack this fine-grained interpretability, since their learned mapping is entangled across many parameters. Thus, KANs offer a more human-understandable model: as the saying goes, they turn the <strong>“black box”</strong> into a collection of readable 1D transformations.</p> <p><strong>Summary:</strong> KANs and MLPs both approximate complex functions, but KANs do so by <em>baking learnable math into the connections</em>. This difference yields advantages in function approximation fidelity, parameter efficiency, and interpretability. However, it also comes with computational challenges (will uupdate later). In essence, KANs can be seen as a <strong>new paradigm</strong>: they trade the simple, generic structure of MLPs for a structure with built-in mathematical richness (the Kolmogorov-Arnold basis). This seemingly small change – moving from scalar weights to learned functions – has profound implications on how the network learns and what it can represent (<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here" rel="external nofollow noopener" target="_blank">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>).</p> <p>The original paper can be found <a href="https://arxiv.org/pdf/2404.19756" rel="external nofollow noopener" target="_blank">here</a></p> <p>Last Updated - 25/02/2025</p> <h3 id="references">References</h3> <p>[1] - <a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov,the%20model%E2%80%99s%20flexibility%20and%20interpretability" rel="external nofollow noopener" target="_blank">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a></p> <p>[2] - <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement" rel="external nofollow noopener" target="_blank">Novel Architecture Makes Neural Networks More Understandable</a></p> <p>[3] - <a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than" rel="external nofollow noopener" target="_blank">OpenReview on KAN: Kolmogorov–Arnold Networks</a></p> <p>[4] - <a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node" rel="external nofollow noopener" target="_blank">Trying Kolmogorov-Arnold Networks in Practice</a></p> <p>[5] - <a href="https://www.datacamp.com/tutorial/kolmogorov-arnold-networks#:~:text=simpler%2C%20univariate%20ones,edges%20are%20used%20for%20approximation" rel="external nofollow noopener" target="_blank">Kolmogorov-Arnold Networks (KANs): A Guide With Implementation</a></p> <p>[6] - <a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting" rel="external nofollow noopener" target="_blank">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a></p> <p>[7] - <a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here" rel="external nofollow noopener" target="_blank">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/welcome/">Welcome!</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/iclrblog/">Favourite ICLR Blog Posts</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/reasoning/">Reasoning in Large Language Models Resources</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ai-journey/">My AI Journey</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nptel-ml/">Machine Learning and AI Course Resources by Indian Professors</a> </li> </div> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".task-list-item-checkbox").forEach(e=>{e.removeAttribute("disabled")})});</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Harshwardhan Sanjay Fartale. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-harshwardhan",title:"Harshwardhan",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-kb",title:"KB",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"My Resume. You can download it from the button right there \ud83d\udc49",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-paperstack",title:"PaperStack",description:"All the books I am hoping to read and complete in 2025",section:"Navigation",handler:()=>{window.location.href="/paperstack/"}},{id:"nav-teaching",title:"Teaching",description:"Highlights of teaching experience and contributions",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-favourite-iclr-blog-posts",title:"Favourite ICLR Blog Posts",description:"Compilation of my favourite iclr blog posts",section:"Posts",handler:()=>{window.location.href="/blog/2025/iclrblog/"}},{id:"post-reasoning-in-large-language-models-resources",title:"Reasoning in Large Language Models Resources",description:"Go-to papers for learning about reasoning in large language models",section:"Posts",handler:()=>{window.location.href="/blog/2025/reasoning/"}},{id:"post-my-ai-journey",title:"My AI Journey",description:"Complete Roadmap",section:"Posts",handler:()=>{window.location.href="/blog/2025/ai-journey/"}},{id:"post-machine-learning-and-ai-course-resources-by-indian-professors",title:"Machine Learning and AI Course Resources by Indian Professors",description:"A collection of links to essential courses on machine learning, deep learning, natural language processing, and artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2025/nptel-ml/"}},{id:"post-python-notes",title:"Python Notes",description:"A collection of Python notebooks for quick reference",section:"Posts",handler:()=>{window.location.href="/blog/2025/python-notes/"}},{id:"post-tech-blog-repository-my-learning-journey",title:"Tech Blog Repository - My Learning Journey",description:"A curated collection of influential technical blogs that shaped my understanding",section:"Posts",handler:()=>{window.location.href="/blog/2025/blogstack/"}},{id:"post-kan-kolmogorov-arnold-networks",title:"KAN (Kolmogorov-Arnold Networks)",description:"An Alternative to traditional MLPs",section:"Posts",handler:()=>{window.location.href="/blog/2025/kan/"}},{id:"post-let-39-s-build-an-llm-from-scratch",title:"Let's Build an LLM (from scratch)",description:"My attempt at building a large language model",section:"Posts",handler:()=>{window.location.href="/blog/2024/lets-build-a-llm/"}},{id:"post-a-visit-to-hungarian-mathematics",title:"A Visit to Hungarian Mathematics",description:"Why Hungarians are so darn good at mathematics ?",section:"Posts",handler:()=>{window.location.href="/blog/2024/hungarian-mathematics/"}},{id:"post-welcome",title:'Welcome! <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.",section:"Posts",handler:()=>{window.open("https://emharsha1812.github.io/tinkerwithml/","_blank")}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-codeitup",title:"CodeItUp",description:"A versatile online code editor with multi-language support",section:"Projects",handler:()=>{window.location.href="/projects/codeitup/"}},{id:"projects-llm-from-scratch",title:"LLM from Scratch",description:"A Large language model, trained from the ground up with code and learnings",section:"Projects",handler:()=>{window.location.href="/projects/llm/"}},{id:"projects-pulse",title:"PULSE",description:"A comprehensive Python library for synthetic sensor data generation",section:"Projects",handler:()=>{window.location.href="/projects/pulse/"}},{id:"projects-tinker-with-machine-learning",title:"Tinker with Machine Learning",description:"A curated place for all my AI learnings. Clearly explained",section:"Projects",handler:()=>{window.location.href="/projects/tinkerwithml/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%61%72%73%68%77%61%72%64%68%61%6E%66%61%72%74%61%6C%65.%6E%69%74%68@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-whatsapp",title:"WhatsApp",section:"Socials",handler:()=>{window.open("https://wa.me/919317439486","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emharsha1812","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/emharsha1812","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/emharsha1812","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>