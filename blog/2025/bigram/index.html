<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Training a simple bigram character level model on tiny stories | Harshwardhan Sanjay Fartale </title> <meta name="author" content="Harshwardhan Sanjay Fartale"> <meta name="description" content="Training a simple bigram character level model on tiny stories"> <meta name="keywords" content="harshwardhan fartale, harshwardhan, portfolio, Harsh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%B0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emharsha1812.github.io/blog/2025/bigram/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Harshwardhan</span> Sanjay Fartale </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Harshwardhan </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/stats/">stats </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/frompapertocode/">Paper2code </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/papershelf/">Library </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <style>ul.task-list input[type="checkbox"]:checked{accent-color:aqua!important}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">Training a simple bigram character level model on tiny stories</h1> <p class="post-meta"> Created in May 24, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/coding"> <i class="fa-solid fa-hashtag fa-sm"></i> coding,</a>   <a href="/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> python</a>   ·   <a href="/blog/category/coding"> <i class="fa-solid fa-tag fa-sm"></i> coding,</a>   <a href="/blog/category/python"> <i class="fa-solid fa-tag fa-sm"></i> python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="building-a-bigram-language-model-a-step-by-step-guide-to-character-level-text-generation">Building a Bigram Language Model: A Step-by-Step Guide to Character-Level Text Generation</h1> <p>I wrote this small snippet as part of my learning process from Andrej’s video (link).</p> <h2 id="what-is-a-bigram-language-model">What is a Bigram Language Model?</h2> <p>A bigram language model predicts the next character in a sequence based solely on the current character. It’s called “bigram” because it considers pairs of characters (bi = two).</p> <p>The model learns a probability distribution over all possible next characters given the current character, essentially building a lookup table that says “when I see character X, what’s the most likely next character?”</p> <h2 id="dataset-preparation-and-text-loading">Dataset Preparation and Text Loading</h2> <p>Our journey begins with loading and examining our text data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">stories.text</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span><span class="o">=</span><span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">text</span><span class="p">[:</span><span class="mi">500</span><span class="p">])</span>
</code></pre></div></div> <p>The Tiny Stories dataset contains simple, child-friendly stories that are perfect for training language models. We load the entire text file into memory as a single string. This approach works well for smaller datasets, though larger datasets would require more sophisticated data loading strategies.</p> <h2 id="character-level-tokenization">Character-Level Tokenization</h2> <p>Unlike word-based models, our character-level approach treats each individual character as a token. This has several advantages:</p> <ul> <li> <strong>Simplicity</strong>: No need for complex word segmentation</li> <li> <strong>Robustness</strong>: Can handle any text, including typos and rare words</li> <li> <strong>Fine-grained control</strong>: Learns spelling patterns and character relationships</li> </ul> <p>Let’s build our character vocabulary:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chars</span><span class="o">=</span><span class="nf">sorted</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">text</span><span class="p">)))</span>
<span class="n">vocab_size</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['\n', ' ', '!', '"', '#', '$', '&amp;', "'", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '&lt;', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¦', '©', '\xad', '±', '´', 'Â', 'Ã', 'â', 'ð', 'œ', 'Š', 'Ÿ', 'Ž', '˜', '“', '”', '‹', '€', '™']
101

 !"#$&amp;'()*+,-./0123456789:;&lt;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz¦©­±´ÂÃâðœŠŸŽ˜“”‹€™
</code></pre></div></div> <p>This code extracts all unique characters from our text and sorts them alphabetically. The vocabulary size tells us how many different characters our model needs to handle. Typically, this includes letters (both cases), numbers, punctuation, and whitespace characters.</p> <h2 id="building-the-tokenizer">Building the Tokenizer</h2> <p>Tokenization is the process of converting text into numerical representations that neural networks can process. We create two essential mappings:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stoi</span><span class="o">=</span><span class="p">{</span><span class="n">ch</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>  <span class="c1"># string to integer
</span><span class="n">itos</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>  <span class="c1"># integer to string
</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">])</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">stoi</code> (string-to-integer) dictionary maps each character to a unique integer ID, while <code class="language-plaintext highlighter-rouge">itos</code> (integer-to-string) provides the reverse mapping. Our encoder and decoder functions handle the conversion between text and numerical sequences.</p> <p>Testing our tokenizer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">exa</span><span class="o">=</span><span class="sh">"</span><span class="s">My name is Harshwardhan</span><span class="sh">"</span>
<span class="n">output</span><span class="o">=</span><span class="nf">encoder</span><span class="p">(</span><span class="n">exa</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">decoder</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[42, 80, 1, 69, 56, 68, 60, 1, 64, 74, 1, 37, 56, 73, 74, 63, 78, 56, 73, 59, 63, 56, 69]
My name is Harshwardhan
</code></pre></div></div> <p>This verification step ensures our encoding and decoding process is lossless - we can convert text to numbers and back to the original text perfectly.</p> <h2 id="converting-to-pytorch-tensors">Converting to PyTorch Tensors</h2> <p>Neural networks work with tensors, so we convert our encoded text into a PyTorch tensor:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nf">encoder</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([19212308])
tensor([48, 71, 70, 75, 14,  1, 48, 71, 70, 75,  1, 74, 56, 78,  1, 75, 63, 60,
         1, 74])
</code></pre></div></div> <p>The resulting tensor contains integer indices representing each character in our text. The shape tells us the total length of our dataset, while examining the first 100 elements helps us verify the conversion worked correctly.</p> <h2 id="dataset-splitting">Dataset Splitting</h2> <p>Machine learning requires separate training and validation sets to properly evaluate model performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">train</span><span class="o">=</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
<span class="n">validate</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span>
</code></pre></div></div> <p>We use a 90-10 split, dedicating 90% of our data to training and 10% to validation. The validation set helps us monitor whether our model is learning genuine patterns or simply memorizing the training data (overfitting).</p> <h2 id="understanding-context-windows">Understanding Context Windows</h2> <p>Language models don’t process entire texts at once. Instead, they work with fixed-size context windows.</p> <p>To give a context of what I am trying to say, here’s a snippet you can run to get an idea</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">block_size</span><span class="o">=</span><span class="mi">8</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">train</span><span class="p">[:</span><span class="n">block_size</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">block_size</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">when input is </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s"> the target: </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>when input is tensor([48]) the target: 71
when input is tensor([48, 71]) the target: 70
when input is tensor([48, 71, 70]) the target: 75
when input is tensor([48, 71, 70, 75]) the target: 14
when input is tensor([48, 71, 70, 75, 14]) the target: 1
when input is tensor([48, 71, 70, 75, 14,  1]) the target: 48
when input is tensor([48, 71, 70, 75, 14,  1, 48]) the target: 71
when input is tensor([48, 71, 70, 75, 14,  1, 48, 71]) the target: 70
</code></pre></div></div> <p>This code demonstrates a crucial concept: from a single sequence of length 8, we can create 8 different training examples. Each example uses a progressively longer context to predict the next character:</p> <ul> <li>Given just the first character, predict the second</li> <li>Given the first two characters, predict the third</li> <li>And so on…</li> </ul> <p>This approach maximizes the learning opportunities from our data and teaches the model to work with contexts of varying lengths.</p> <h2 id="batch-processing-for-efficient-training">Batch Processing for Efficient Training</h2> <p>Neural networks train more efficiently when processing multiple examples simultaneously. Our batch generation function creates random samples:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span>
<span class="n">block_size</span><span class="o">=</span><span class="mi">8</span>

<span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
    <span class="n">data</span><span class="o">=</span><span class="n">train</span> <span class="k">if</span> <span class="n">split</span><span class="o">==</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span> <span class="k">else</span> <span class="n">validate</span>
    <span class="n">ix</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="n">block_size</span><span class="p">,(</span><span class="n">batch_size</span><span class="p">,))</span>
    <span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span>
</code></pre></div></div> <p>This function randomly selects starting positions in our dataset and extracts sequences of length <code class="language-plaintext highlighter-rouge">block_size</code>. The result is two tensors:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">x</code>: Input sequences (what the model sees)</li> <li> <code class="language-plaintext highlighter-rouge">y</code>: Target sequences (what the model should predict)</li> </ul> <p>The random sampling ensures our model sees different parts of the text in each batch, promoting better generalization.</p> <h2 id="the-bigram-language-model-architecture">The Bigram Language Model Architecture</h2> <p>Now we build our neural network. Despite its simplicity, this model embodies key language modeling concepts:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</code></pre></div></div> <p>The core of our model is an embedding table - essentially a learned lookup table where each character is associated with a vector of probabilities for the next character. The embedding dimension equals our vocabulary size, creating a direct mapping from current character to next character probabilities.</p> <h2 id="forward-pass-and-loss-calculation">Forward Pass and Loss Calculation</h2> <p>The forward pass transforms input sequences into predictions and calculates the training loss:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1"># (B,T,C)
</span>    
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <p>The embedding table produces “logits” - raw prediction scores for each possible next character. When we have targets (during training), we calculate cross-entropy loss, which measures how well our predictions match the actual next characters.</p> <p>The reshaping operations (<code class="language-plaintext highlighter-rouge">view</code>) are necessary because PyTorch’s cross-entropy function expects 2D inputs, but our model produces 3D tensors (batch, time, characters).</p> <h2 id="text-generation">Text Generation</h2> <p>The generation function is where our trained model becomes useful:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># focus on last time step
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># convert to probabilities
</span>        <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># sample
</span>        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># append
</span>    <span class="k">return</span> <span class="n">idx</span>
</code></pre></div></div> <p>This function implements autoregressive generation:</p> <ol> <li>Get predictions for the current sequence</li> <li>Focus only on the last position (most recent character)</li> <li>Convert logits to probabilities using softmax</li> <li>Sample a character based on these probabilities</li> <li>Add the sampled character to our sequence</li> <li>Repeat</li> </ol> <p>The sampling step is crucial - rather than always picking the most likely character (which would be deterministic and repetitive), we sample according to the probability distribution, introducing controlled randomness that makes the generated text more interesting and varied.</p> <h2 id="training-loop">Training Loop</h2> <p>Training a neural network involves repeatedly showing it examples and adjusting its parameters to reduce prediction errors:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="k">for</span> <span class="n">steps</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">(</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">m</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p>Each training step follows a standard pattern:</p> <ol> <li> <strong>Forward pass</strong>: Feed data through the model to get predictions</li> <li> <strong>Loss calculation</strong>: Compare predictions to actual targets</li> <li> <strong>Backward pass</strong>: Calculate gradients showing how to improve</li> <li> <strong>Parameter update</strong>: Adjust model weights to reduce loss</li> </ol> <p>We use the AdamW optimizer, which adapts the learning rate for each parameter individually, leading to more stable and efficient training than basic gradient descent.</p> <h2 id="monitoring-progress">Monitoring Progress</h2> <p>Before training, our model generates mostly gibberish:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">decoder</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sha inth ge jonin out, peroamy aveppedan s lld het
</code></pre></div></div> <p>After 10,000 training steps, the same generation call produces much more coherent text. The loss value also decreases significantly, indicating that our model is learning the character patterns in our dataset.</p> <h2 id="key-insights-and-limitations">Key Insights and Limitations</h2> <p>Our bigram model, while simple, demonstrates several important concepts:</p> <p><strong>Strengths:</strong></p> <ul> <li> <strong>Simplicity</strong>: Easy to understand and implement</li> <li> <strong>Speed</strong>: Fast training and inference</li> <li> <strong>Foundational</strong>: Introduces core language modeling concepts</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Limited context</strong>: Only considers the immediately previous character</li> <li> <strong>No long-range dependencies</strong>: Cannot capture relationships between distant characters</li> <li> <strong>Basic patterns</strong>: Learns simple character transitions but misses complex linguistic structures</li> </ul> <p>Here is the link to the Google Colab Notebook -</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/welcome/">Welcome!</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/llmgenerated/">Can we really identify LLM Generated Text? The promise and limits of watermarking</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/flash/">FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/rsquared/">R squared in Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nptel-ml/">Machine Learning and AI Resources</a> </li> </div> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".task-list-item-checkbox").forEach(e=>{e.removeAttribute("disabled")})});</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Harshwardhan Sanjay Fartale. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-harshwardhan",title:"Harshwardhan",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"Projects that I built over the course of my journey",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-stats",title:"stats",description:"",section:"Navigation",handler:()=>{window.location.href="/stats/"}},{id:"nav-cv",title:"cv",description:"My Resume. You can download it from the button right there \ud83d\udc49",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-paper2code",title:"Paper2code",description:"A collection of paper-to-code implementations. This page serves as a testament to my skills in deciphering research papers and translating theoretical concepts into functional code.",section:"Navigation",handler:()=>{window.location.href="/frompapertocode/"}},{id:"nav-teaching",title:"teaching",description:"Highlights of teaching experience and contributions",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-library",title:"Library",description:"Curated reading list with quick filters and links to notes.",section:"Navigation",handler:()=>{window.location.href="/papershelf/"}},{id:"post-can-we-really-identify-llm-generated-text-the-promise-and-limits-of-watermarking",title:"Can we really identify LLM Generated Text? The promise and limits of watermarking...",description:"Exploring the theoretical and practical aspects of watermarking techniques for detecting AI-generated content, including trade-offs, failure modes, and information-theoretic limits",section:"Posts",handler:()=>{window.location.href="/blog/2025/llmgenerated/"}},{id:"post-flashattention-through-the-years-how-io-aware-kernels-reshaped-scalable-transformers",title:"FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers",description:"We present a technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with diagrams and Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms.",section:"Posts",handler:()=>{window.location.href="/blog/2025/flash/"}},{id:"post-r-squared-in-machine-learning",title:"R squared in Machine Learning",description:"Meaning, Explanation & more",section:"Posts",handler:()=>{window.location.href="/blog/2025/rsquared/"}},{id:"post-training-a-simple-bigram-character-level-model-on-tiny-stories",title:"Training a simple bigram character level model on tiny stories",description:"Training a simple bigram character level model on tiny stories",section:"Posts",handler:()=>{window.location.href="/blog/2025/bigram/"}},{id:"post-machine-learning-and-ai-resources",title:"Machine Learning and AI Resources",description:"A collection of links to essential courses on machine learning, deep learning, natural language processing, and artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2025/nptel-ml/"}},{id:"post-python-notes",title:"Python Notes",description:"A collection of Python notebooks for quick reference",section:"Posts",handler:()=>{window.location.href="/blog/2025/python-notes/"}},{id:"post-kan-kolmogorov-arnold-networks",title:"KAN (Kolmogorov-Arnold Networks)",description:"An Alternative to traditional MLPs",section:"Posts",handler:()=>{window.location.href="/blog/2025/kan/"}},{id:"post-a-visit-to-hungarian-mathematics",title:"A Visit to Hungarian Mathematics",description:"Why Hungarians are so darn good at mathematics ?",section:"Posts",handler:()=>{window.location.href="/blog/2024/hungarian-mathematics/"}},{id:"post-welcome",title:'Welcome! <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.",section:"Posts",handler:()=>{window.open("https://emharsha1812.github.io/tinkerwithml/","_blank")}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"papers2code-attention-is-all-you-need-transformer-implementation",title:"Attention Is All You Need - Transformer Implementation",description:"Complete PyTorch implementation of the Transformer architecture",section:"Papers2code",handler:()=>{window.location.href="/papers2code/attention-is-all-you-need"}},{id:"projects-codeitup",title:"CodeItUp",description:"A versatile online code editor with multi-language support",section:"Projects",handler:()=>{window.location.href="/projects/codeitup/"}},{id:"projects-alfred",title:"Alfred",description:"Your Local AI Coding Butler",section:"Projects",handler:()=>{window.location.href="/projects/llm/"}},{id:"projects-pulse",title:"PULSE",description:"A comprehensive Python library for synthetic sensor data generation",section:"Projects",handler:()=>{window.location.href="/projects/pulse/"}},{id:"projects-tinker-with-machine-learning",title:"Tinker with Machine Learning",description:"A curated place for all my AI learnings. Clearly explained",section:"Projects",handler:()=>{window.location.href="/projects/tinkerwithml/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%61%72%73%68%77%61%72%64%68%61%6E%66%61%72%74%61%6C%65.%6E%69%74%68@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-whatsapp",title:"WhatsApp",section:"Socials",handler:()=>{window.open("https://wa.me/919317439486","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emharsha1812","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/emharsha1812","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/emharsha1812","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>