<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Newton Boosting &amp; XGBoost | Harshwardhan Sanjay Fartale </title> <meta name="author" content="Harshwardhan Sanjay Fartale"> <meta name="description" content="A complete guide to XGBoost"> <meta name="keywords" content="harshwardhan fartale, harshwardhan, portfolio, Harsh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%B0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emharsha1812.github.io/blog/2025/xgboost/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Harshwardhan</span> Sanjay Fartale </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Harshwardhan </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">KB </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/stats/">stats </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/frompapertocode/">Paper2code </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <style>ul.task-list input[type="checkbox"]:checked{accent-color:aqua!important}</style> <div class="post"> <header class="post-header"> <h1 class="post-title">Newton Boosting &amp; XGBoost</h1> <p class="post-meta"> Created in September 18, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning,</a>   <a href="/blog/category/xgboost"> <i class="fa-solid fa-tag fa-sm"></i> xgboost,</a>   <a href="/blog/category/ensemble"> <i class="fa-solid fa-tag fa-sm"></i> ensemble</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="sequential-ensembles-and-newton-boosting--a-complete-technical-blog">Sequential ensembles and Newton boosting — a complete technical blog</h1> <p>This post is a self-contained, technical walkthrough of Newton boosting and XGBoost. It covers the mathematical foundations, algorithmic intuition, practical training tips, and concise code examples to get you started. At the end you’ll find a comprehensive set of interview questions grouped by difficulty to guide your preparation.</p> <hr> <h2 id="contents">Contents</h2> <ol> <li>Introduction</li> <li>Newton’s method (optimization refresher)</li> <li>Newton boosting: idea and mechanics</li> <li>Role of Hessians (intuition)</li> <li>XGBoost: what it is and why it works well</li> <li> <p>Regularized objective and tree algebra</p> <ul> <li>leaf weight derivation</li> <li>split gain formula</li> </ul> </li> <li>Efficient split finding: weighted quantile sketch (conceptual)</li> <li>Minimal code examples (XGBoost usage patterns)</li> <li>Training practices: learning rate selection, CV, early stopping</li> <li>Case study: ranking / LETOR (setup + notes)</li> <li>Summary</li> <li>Interview questions (Easy → Expert)</li> </ol> <hr> <h2 id="1-introduction">1. Introduction</h2> <p>The objective of Boosting is to build strong learners by sequentially combining many weak learners. Adaptive Boosting (AdaBoost) uses weights to identify the most missclassfied examples. Gradient Boosting uses gradients (residuals) to identify the most missclassified examples. The fundamental intuition behind both of these boosting methods is to target the most missclassified (worst behaving) examples at every iteration to improve classification</p> <h3 id="newton-boosting">Newton Boosting</h3> <p>Newton Boosting combines the advantages of Adaboost and gradient boosting and thus uses <em>weighted gradients</em> (or weighted residuals) to identify the most misclassified examples.</p> <p>This framework of Newton boosting can be applied to any loss functions, which means that any classification, regression or ranking problem can be boosted using weak learners.</p> <p>The fundamental motivation for devising Newton Boosting is for the optimization of the loss function. Gradient Descent is a first order optimization method, meaning that it uses first derivatives during optimization</p> <p>Newtons method or newtons descent is a <em>second order</em> optimization method, such that it uses both the first derivatives as well as the second derivatives information to compute the newton step</p> <p>Thus Newton boosting extends the idea of gradient boosting by also using second-order derivatives (Hessians), which encode curvature information and allow for more accurate, often faster updates.</p> <p>XGBoost is a high-performance implementation that leverages Newton-style updates together with practical regularization and engineering techniques to scale tree-based boosting to large problems</p> <hr> <h2 id="2-newtons-method-optimization-refresher">2. Newton’s method (optimization refresher)</h2> <p>Newton’s method is a second-order optimization technique for minimizing scalar functions. For a scalar objective \(f(w)\), Newton’s method uses a <em>quadratic approximation</em> around the current iterate \(w_t\):</p> <ul> <li>Taylor expansion to second order:</li> </ul> \[f(w) \approx f(w_t) + f'(w_t)(w - w_t) + \tfrac{1}{2} f''(w_t) (w - w_t)^2.\] <ul> <li>Setting derivative of this approximation to zero gives the classical Newton update:</li> </ul> \[w_{t+1} = w_t - \frac{f'(w_t)}{f''(w_t)}.\] <p>Compare this to gradient descent:</p> \[w_{t+1} = w_t - \alpha_t f'(w_t),\] <p>where \(\alpha_t\) is a step size. Newton’s step effectively sets an adaptive step size \(\alpha_t = 1/f''(w_t)\) (when \(f''(w_t) &gt; 0\)), so it moves more aggressively in flat regions (small curvature) and more conservatively in sharp-curvature regions (large curvature).</p> <p>In higher dimensions, \(f'(w)\) becomes the gradient \(\nabla f\) and \(f''(w)\) becomes the Hessian matrix \(H\). The Newton update generalizes to:</p> \[w_{t+1} = w_t - H^{-1}(w_t) \nabla f(w_t).\] <h3 id="intuition">Intuition</h3> <p>In gradient descent, first derivative information only allows us to construct a local linear approximation at best. While this gives us a descent direction, different step lengths can give us vastly different estimates and may ultimately slow down convergence</p> <p>Incorporating second-derivative information as Newton’s descent does, allows us to construct a local quadratic approximation. This <strong>extra</strong> information leads to a better local approximation, resulting in better shapes and faster conversions</p> <p>As we know, Gradient Boosting characterizes misclassified examples that need attention through residuals. A residual is simply another means to measure the extend of misclassification and is computed as the gradient of the loss function</p> <p>Newton Boosting does <em>weighted residuals</em> The residuals in Newton boosting are computed using the first derivative (gradient of the loss function) while the weights are computed using the Hessian of the loss function (the second derivative)</p> <p>Thus it thoroughly ensures that the misclassified examples recieve focus based on the extent of their missclassification</p> <h2 id="3-newton-boosting-idea-and-mechanics">3. Newton boosting: idea and mechanics</h2> <p>Newton boosting adapts Newton’s optimization idea to function-space boosting (i.e., iteratively building an additive model \(F(x) = \sum_{t} f_t(x)\) where each \(f_t\) is a weak learner such as a regression tree).</p> <p>Key idea:</p> <ul> <li>At iteration \(t\), given current model \(F_{t-1}(x)\), approximate the loss \(\ell(y, F(x))\) using a second-order Taylor expansion in \(F\):</li> </ul> \[\ell(y, F_{t-1}(x) + f_t(x)) \approx \ell(y, F_{t-1}(x)) + g(x)\, f_t(x) + \tfrac{1}{2} h(x)\, f_t(x)^2,\] <p>where</p> \[g(x) = \left.\frac{\partial \ell(y, F)}{\partial F}\right|_{F=F_{t-1}(x)}, \qquad h(x) = \left.\frac{\partial^2 \ell(y, F)}{\partial F^2}\right|_{F=F_{t-1}(x)}.\] <ul> <li>The second-order approximation converts the problem of finding \(f_t\) into minimizing a weighted squared-error objective:</li> </ul> \[\min_{f_t} \sum_{i=1}^n \left[ g_i f_t(x_i) + \tfrac{1}{2} h_i f_t(x_i)^2 \right] + \Omega(f_t),\] <p>where \(g_i\) and \(h_i\) denote gradient and Hessian at \(x_i\), and \(\Omega\) is a regularizer on \(f_t\) (e.g., penalize tree complexity).</p> <ul> <li>If the weak learner is a regression tree that predicts constant values in leaves, then for a leaf \(j\) with instances \(\mathcal{I}_j\) the optimal constant prediction \(w_j\) can be computed in closed form (see the next section). Thus tree fitting reduces to collecting aggregated \(G_j=\sum_{i\in\mathcal{I}_j} g_i\) and \(H_j=\sum_{i\in\mathcal{I}_j} h_i\) statistics and using them to evaluate candidate splits and leaf values.</li> </ul> <p>So, Newton boosting trains trees on Hessian-weighted residuals \(-g_i/h_i\) in effect, with \(h_i\) controlling step size per-example.</p> <hr> <h2 id="4-role-of-hessians-intuition">4. Role of Hessians (intuition)</h2> <p>Why do Hessians help?</p> <ul> <li>Hessians encode curvature of the loss w.r.t. model output. If \(h_i\) is large, the loss curvature around that example is steep; Newton’s method takes smaller steps there because the quadratic term penalizes large \(f_t(x_i)\).</li> <li>If \(h_i\) is small, the loss is relatively flat at that point and Newton can take larger corrective steps.</li> </ul> <p>Practically, weighting examples by \(h_i\) means the algorithm adjusts the influence of each training point on split decisions and leaf values according to local curvature, often giving more stable and faster convergence than first-order methods (especially for losses where curvature varies a lot across the domain).</p> <p>Note: very small \(h_i\) values can cause instability (division by near-zero). Implementations add a \(\lambda\) term (L2 regularization on leaf weights) to stabilize computations.</p> <hr> <h2 id="5-xgboost-what-it-is-and-why-it-works-well">5. XGBoost: what it is and why it works well</h2> <p>XGBoost (eXtreme Gradient Boosting) is an efficient and scalable implementation of tree-based boosting that:</p> <ul> <li>Uses second-order (Newton) approximations for fast, accurate updates.</li> <li>Optimizes a regularized objective that directly penalizes model complexity.</li> <li>Implements efficient split finding (approximate histograms / weighted quantile sketches) and data structures (block-based storage and cache-friendly layouts).</li> <li>Provides features like parallel learning, out-of-core computation, and a range of tuning options (learning rate, \(\gamma\), \(\lambda\), subsampling, column sampling, etc.).</li> </ul> <p>Two practical aspects that contribute heavily to XGBoost’s success:</p> <ol> <li> <strong>Regularized objective</strong> that makes tree leaf-value computation and split evaluation analytically tractable and robust.</li> <li> <strong>Engineering</strong>: clever data layouts and parallelization allow it to scale to large datasets.</li> </ol> <hr> <h2 id="6-regularized-objective-and-tree-algebra">6. Regularized objective and tree algebra</h2> <p>XGBoost trains an additive model by minimizing a regularized objective at each boosting round. For a dataset \({(x_i, y_i)}_{i=1}^n\) and model \(F_{t}(x) = F_{t-1}(x) + f_t(x)\), the objective is typically</p> \[\mathcal{L}^{(t)} = \sum_{i=1}^n \ell\big(y_i, F_{t-1}(x_i) + f_t(x_i)\big) + \Omega(f_t),\] <p>where a common choice of the regularizer for trees is</p> \[\Omega(f) = \gamma T + \tfrac{1}{2}\lambda \sum_{j=1}^{T} w_j^2,\] <p>with \(T\) the number of leaves in the tree and \(w_j\) the weight/value predicted by leaf \(j\); \(\gamma\) penalizes number of leaves and \(\lambda\) is an L2 penalty on leaf weights.</p> <p>Using a second-order Taylor expansion of \(\ell\) around \(F_{t-1}\):</p> \[\ell(y_i, F_{t-1} + f_t) \approx \ell(y_i, F_{t-1}) + g_i f_t(x_i) + \tfrac{1}{2} h_i f_t(x_i)^2,\] <p>with per-example gradients and Hessians \(g_i, h_i\) as defined earlier.</p> <h3 id="leaf-weight-derivation-closed-form">Leaf weight derivation (closed-form)</h3> <p>Assume \(f_t\) is a tree with leaves \(j=1\ldots T\), each predicting constant value \(w_j\). For leaf \(j\), define:</p> \[G_j = \sum_{i \in \mathcal{I}_j} g_i, \qquad H_j = \sum_{i \in \mathcal{I}_j} h_i,\] <p>where \(\mathcal{I}_j\) is the set of instance indices falling into leaf \(j\).</p> <p>Ignoring constants independent of \(w_j\), the objective contribution from leaf \(j\) is:</p> \[\mathcal{L}_j(w_j) = G_j w_j + \tfrac{1}{2} (H_j + \lambda) w_j^2 + \gamma.\] <p>Minimize w.r.t. \(w_j\) by setting derivative to zero:</p> \[\frac{\partial \mathcal{L}_j}{\partial w_j} = G_j + (H_j + \lambda) w_j = 0 \quad\Rightarrow\quad w_j^\ast = -\frac{G_j}{H_j + \lambda}.\] <p>So the optimal leaf weight is proportional to the negative aggregated gradient, scaled by aggregated Hessian plus regularization.</p> <p>The minimum objective (reduction in loss) for the leaf is</p> \[\mathcal{L}_j(w_j^\ast) = -\tfrac{1}{2}\frac{G_j^2}{H_j+\lambda} + \gamma.\] <p>This algebra gives two important operational formulas: the optimal leaf value and the leaf’s contribution to objective reduction.</p> <h3 id="split-gain-formula">Split gain formula</h3> <p>Given a node that would be split into left (L) and right (R) children, the improvement (gain) from performing that split is the difference between the parent’s score and the sum of child scores:</p> <p>Let \(G=G_L+G_R\) and \(H=H_L+H_R\). The gain is</p> \[\text{gain} = \tfrac{1}{2}\left(\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{G^2}{H + \lambda}\right) - \gamma.\] <p>A split is beneficial only if <code class="language-plaintext highlighter-rouge">gain</code> is positive (or above a threshold). This formula naturally includes Hessian-weighting and L2 regularization, and the \(\gamma\) term penalizes creating extra leaves.</p> <hr> <h2 id="7-efficient-split-finding-weighted-quantile-sketch-conceptual">7. Efficient split finding: weighted quantile sketch (conceptual)</h2> <p>For large datasets, exhaustively evaluating all candidate splits is expensive. Two common approximate approaches:</p> <ul> <li> <strong>Histogram-based binning:</strong> bin continuous feature values into a fixed number of buckets and evaluate splits only at bucket boundaries (used by LightGBM and others).</li> <li> <strong>Quantile sketching:</strong> approximate quantiles of feature values to pick candidate split positions. For Newton boosting, each instance has a weight (often the Hessian \(h_i\)), so the sketch must be <strong>weighted</strong>.</li> </ul> <p>XGBoost’s <strong>weighted quantile sketch</strong> finds approximate quantile boundaries while accounting for example weights (Hessians). The key idea is to build a compact summary of the weighted distribution of a feature and then propose split thresholds from that summary. This trades a small amount of exactness for massive gains in memory and time efficiency and allows XGBoost to scale to large data.</p> <p>In practice, implementations also:</p> <ul> <li>Presort or block data for cache-friendly access</li> <li>Chunk data and operate on blocks to reduce IO overhead (important for out-of-core training)</li> <li>Use parallel reduction to compute aggregated \(G_j\) and \(H_j\) for candidate splits</li> </ul> <hr> <h2 id="8-minimal-code-examples-xgboost-usage-patterns">8. Minimal code examples (XGBoost usage patterns)</h2> <p>Below are essential patterns you will commonly use. Keep these snippets minimal and idiomatic.</p> <h3 id="example-basic-scikit-learn-style-fit-classification">Example: basic scikit-learn-style fit (classification)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">Xtr</span><span class="p">,</span> <span class="n">Xte</span><span class="p">,</span> <span class="n">ytr</span><span class="p">,</span> <span class="n">yte</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">XGBClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                             <span class="n">objective</span><span class="o">=</span><span class="sh">'</span><span class="s">binary:logistic</span><span class="sh">'</span><span class="p">,</span> <span class="n">use_label_encoder</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span><span class="p">,</span> <span class="n">eval_metric</span><span class="o">=</span><span class="sh">'</span><span class="s">logloss</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ypred</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">Xte</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span> <span class="n">ypred</span><span class="p">))</span>
</code></pre></div></div> <h3 id="example-native-interface-with-dmatrix-and-xgbtrain">Example: native interface with DMatrix and xgb.train</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">xgboost</span> <span class="k">as</span> <span class="n">xgb</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">DMatrix</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">ytr</span><span class="p">)</span>
<span class="n">dtest</span>  <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">DMatrix</span><span class="p">(</span><span class="n">Xte</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">yte</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">objective</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">binary:logistic</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">'</span><span class="s">learning_rate</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">}</span>
<span class="n">bst</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">yp</span> <span class="o">=</span> <span class="n">bst</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">)</span>
</code></pre></div></div> <p>These patterns are enough to get going. Use the scikit-learn wrapper for quick experiments; use the native <code class="language-plaintext highlighter-rouge">DMatrix</code> and <code class="language-plaintext highlighter-rouge">xgb.train</code> for more fine-grained control.</p> <hr> <h2 id="9-training-practices-learning-rate-selection-cv-early-stopping">9. Training practices: learning rate selection, CV, early stopping</h2> <h3 id="learning-rate-shrinkage">Learning rate (shrinkage)</h3> <p>The learning rate \(\eta\) scales each tree’s contribution:</p> \[F_t(x) = F_{t-1}(x) + \eta f_t(x).\] <p>A smaller \(\eta\) makes learning more conservative and usually improves generalization when combined with a larger number of trees. Common practice: grid-search (or log-space search) over plausible \(\eta\) values and use cross-validation to pick the one that minimizes validation error.</p> <p>Example grid-search skeleton:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="n">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>

<span class="n">rates</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)])</span>
<span class="n">skf</span> <span class="o">=</span> <span class="nc">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># loop over rates and folds, track validation scores; choose best rate
</span></code></pre></div></div> <h3 id="cross-validation-with-xgboost">Cross-validation with XGBoost</h3> <p>Use <code class="language-plaintext highlighter-rouge">xgb.cv</code> for fast built-in CV:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">xgboost</span> <span class="k">as</span> <span class="n">xgb</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">DMatrix</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">ytr</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">objective</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">binary:logistic</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">'</span><span class="s">learning_rate</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">}</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nf">cv</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="sh">'</span><span class="s">logloss</span><span class="sh">'</span><span class="p">,</span>
                          <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">cv_results</span><span class="p">.</span><span class="nf">tail</span><span class="p">())</span>
</code></pre></div></div> <h3 id="early-stopping">Early stopping</h3> <p>Early stopping halts training once validation metric ceases to improve:</p> <ul> <li>With scikit-learn wrapper:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span><span class="p">,</span> <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">)],</span> <span class="n">eval_metric</span><span class="o">=</span><span class="sh">'</span><span class="s">auc</span><span class="sh">'</span><span class="p">,</span>
             <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>With <code class="language-plaintext highlighter-rouge">xgb.cv</code>, use <code class="language-plaintext highlighter-rouge">early_stopping_rounds</code> as shown above.</li> </ul> <p>Early stopping is an effective way to avoid wasting time on overfitting and to automatically pick a number of boosting rounds.</p> <hr> <h2 id="10-case-study-ranking--letor-setup--notes">10. Case study: ranking / LETOR (setup + notes)</h2> <p>Learning-to-rank tasks (e.g., LETOR datasets) are a common place to apply boosting for ranking metrics. Typical setup:</p> <ul> <li>Each example is a query-document pair with features describing query, document, and query-document matches.</li> <li>Labels are relevance levels (e.g., 0/1/2).</li> <li>Use a ranking objective (XGBoost supports pairwise and listwise ranking objectives, as well as custom losses).</li> </ul> <p>Basic data load pattern (LIBSVM-style features):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_svmlight_file</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">load_svmlight_file</span><span class="p">(</span><span class="sh">'</span><span class="s">Querylevelnorm.txt</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># split data ensuring query-group consistency if needed
</span></code></pre></div></div> <p>Notes for ranking experiments:</p> <ul> <li>Ensure that train/validation/test splits respect query groups (documents for a query should not be split across train/val/test in a way that leaks information).</li> <li>Evaluate with ranking metrics such as NDCG@k or MAP.</li> <li>Use randomized CV and multiple seeds for robust comparisons across XGBoost, LightGBM, CatBoost, etc.</li> </ul> <hr> <h2 id="11-summary">11. Summary</h2> <ul> <li>Newton boosting leverages second-order Taylor expansions (gradients + Hessians) to produce more informed updates than first-order gradient boosting.</li> <li>For tree-based weak learners, aggregated gradients \(G_j\) and Hessians \(H_j\) per leaf lead to closed-form optimal leaf weights \(w_j^\ast = -G_j/(H_j+\lambda)\).</li> <li>XGBoost combines Newton-style updates, a regularized objective, and engineering optimizations (weighted quantile sketch, block data layout, parallelism) to scale efficiently and generalize well.</li> <li>Key practical controls: learning rate, tree depth, number of leaves, L2 regularization (\(\lambda\)), complexity penalty (\(\gamma\)), subsampling, column sampling, early stopping.</li> <li>For production/robust experiments, use cross-validation, early stopping, and multiple random seeds. Monitor both training and validation curves and prefer conservative learning rates combined with appropriate regularization.</li> </ul> <hr> <h2 id="12-interview-questions--xgboost--newton-boosting">12. Interview Questions — XGBoost &amp; Newton Boosting</h2> <p>Below is a thorough set of interview-style questions organized by difficulty. Use them for active recall and mock interviews.</p> <h3 id="easy">Easy</h3> <ol> <li>What is boosting, and how does it differ from bagging?</li> <li>Explain the intuition behind gradient boosting.</li> <li>State the Newton update for a scalar function.</li> <li>What is the main conceptual difference between gradient boosting and Newton boosting?</li> <li>What is XGBoost and why did it become popular?</li> <li>What is a DMatrix in XGBoost?</li> <li>How do you convert XGBoost logistic outputs into binary labels?</li> <li>What do <code class="language-plaintext highlighter-rouge">max_depth</code> and <code class="language-plaintext highlighter-rouge">n_estimators</code> control?</li> <li>How does the learning rate affect ensemble learning?</li> <li>What is early stopping? Which param(s) control it?</li> </ol> <h3 id="medium">Medium</h3> <ol> <li>Derive the Newton update for function-space boosting using second-order Taylor expansion.</li> <li>For a regression tree leaf, derive the optimal leaf weight in terms of aggregated gradient and Hessian.</li> <li>Write the split gain formula that XGBoost uses for candidate splits.</li> <li>What roles do \(\lambda\) (L2 leaf regularization) and \(\gamma\) (leaf count penalty) play?</li> <li>Explain the weighted quantile sketch and why weights (Hessians) matter.</li> <li>How would you select a learning rate via cross-validation? Give a small experimental design.</li> <li>Compare <code class="language-plaintext highlighter-rouge">colsample_bytree</code> and <code class="language-plaintext highlighter-rouge">subsample</code>. When tune each?</li> <li>Why can Hessian weighting lead to more stable updates than first-order methods?</li> <li>How do you implement a custom loss in XGBoost — what derivatives must you provide?</li> <li>Explain numerical stability strategies when some \(h_i\) are very small.</li> </ol> <h3 id="hard">Hard</h3> <ol> <li>Show a full algebraic derivation of leaf weights and objective reduction for a full tree (sum over leaves).</li> <li>Discuss how split-finding must change when examples have Hessian weights.</li> <li>Prove (informally) why L2 regularization \(\lambda\) prevents divisions by tiny \(H_j\) (stabilization).</li> <li>Design an experiment comparing XGBoost and LightGBM on a ranking dataset (data splits, metrics, hyperparameter ranges).</li> <li>Discuss practical engineering patterns for out-of-core XGBoost training.</li> <li>How does the <code class="language-plaintext highlighter-rouge">dart</code> booster conceptually alter boosting updates? Why might it help?</li> <li>Explain the trade-offs between histogram-binning and quantile-approximation for split candidates.</li> <li>How do you combine tree-level regularizers with feature- or instance-level importance assessments?</li> <li>Describe approaches to calibrate XGBoost probability outputs for decision-critical systems.</li> <li>How does weighted sampling interact with Hessian-weighted split statistics?</li> </ol> <h3 id="expert">Expert</h3> <ol> <li>Derive the Newton boosting updates for the multiclass softmax loss, showing how classwise gradients and Hessians are used.</li> <li>Provide pseudo-code for a weighted quantile sketch algorithm that supports streaming weighted inserts.</li> <li>Analyze convergence properties: under what convexity and smoothness conditions does Newton boosting achieve quadratic (or superlinear) convergence?</li> <li>Design a distributed Newton-boosting system that aggregates Hessians and gradients with fault tolerance.</li> <li>Propose GPU-specific split-finding optimizations for XGBoost and analyze expected speedups.</li> <li>Show exhaustive unit tests to verify correctness of \(G\) and \(H\) aggregation for split-finding.</li> <li>Create an experiment that separates algorithmic differences between LightGBM and XGBoost (isolate split-finding vs objective differences).</li> <li>Give a robust debugging checklist for a case where validation performance is good but test performance is poor in an XGBoost pipeline.</li> <li>Propose a theoretically motivated learning-rate schedule for Newton boosting and analyze its expected impact.</li> <li>Explain whether Newton boosting ideas can extend to deep learning base learners and discuss obstacles.</li> </ol> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/welcome/">Welcome!</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ml-questions/">Commonly Asked Questions in ML Interviews</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ml-interview/">ML Interview Questions List</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/nondeterminism/">On Defeating Nondeterminism in LLM Inference</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/rsquared/">R squared in Machine Learning</a> </li> </div> <script>document.addEventListener("DOMContentLoaded",function(){document.querySelectorAll(".task-list-item-checkbox").forEach(e=>{e.removeAttribute("disabled")})});</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Harshwardhan Sanjay Fartale. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-harshwardhan",title:"Harshwardhan",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-kb",title:"KB",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"Projects that I built over the course of my journey",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-stats",title:"stats",description:"",section:"Navigation",handler:()=>{window.location.href="/stats/"}},{id:"nav-cv",title:"cv",description:"My Resume. You can download it from the button right there \ud83d\udc49",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-paper2code",title:"Paper2code",description:"A collection of paper-to-code implementations. This page serves as a testament to my skills in deciphering research papers and translating theoretical concepts into functional code.",section:"Navigation",handler:()=>{window.location.href="/frompapertocode/"}},{id:"nav-teaching",title:"teaching",description:"Highlights of teaching experience and contributions",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-newton-boosting-amp-xgboost",title:"Newton Boosting & XGBoost",description:"A complete guide to XGBoost",section:"Posts",handler:()=>{window.location.href="/blog/2025/xgboost/"}},{id:"post-commonly-asked-questions-in-ml-interviews",title:"Commonly Asked Questions in ML Interviews",description:"A list of all commonly asked questions I encountered in ML Interviews",section:"Posts",handler:()=>{window.location.href="/blog/2025/ml-questions/"}},{id:"post-ml-interview-questions-list",title:"ML Interview Questions List",description:"A list of all commonly asked questions I encountered in ML Interviews",section:"Posts",handler:()=>{window.location.href="/blog/2025/ml-interview/"}},{id:"post-on-defeating-nondeterminism-in-llm-inference",title:"On Defeating Nondeterminism in LLM Inference",description:"Exploring strategies to mitigate nondeterminism in large language model inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/nondeterminism/"}},{id:"post-r-squared-in-machine-learning",title:"R squared in Machine Learning",description:"Meaning, Explanation & more",section:"Posts",handler:()=>{window.location.href="/blog/2025/rsquared/"}},{id:"post-common-nlp-doubts",title:"Common NLP Doubts",description:"NLP Interview Questions",section:"Posts",handler:()=>{window.location.href="/blog/2025/nlp-questions/"}},{id:"post-training-a-simple-bigram-character-level-model-on-tiny-stories",title:"Training a simple bigram character level model on tiny stories",description:"Training a simple bigram character level model on tiny stories",section:"Posts",handler:()=>{window.location.href="/blog/2025/bigram/"}},{id:"post-machine-learning-and-ai-resources",title:"Machine Learning and AI Resources",description:"A collection of links to essential courses on machine learning, deep learning, natural language processing, and artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2025/nptel-ml/"}},{id:"post-python-notes",title:"Python Notes",description:"A collection of Python notebooks for quick reference",section:"Posts",handler:()=>{window.location.href="/blog/2025/python-notes/"}},{id:"post-kan-kolmogorov-arnold-networks",title:"KAN (Kolmogorov-Arnold Networks)",description:"An Alternative to traditional MLPs",section:"Posts",handler:()=>{window.location.href="/blog/2025/kan/"}},{id:"post-a-visit-to-hungarian-mathematics",title:"A Visit to Hungarian Mathematics",description:"Why Hungarians are so darn good at mathematics ?",section:"Posts",handler:()=>{window.location.href="/blog/2024/hungarian-mathematics/"}},{id:"post-welcome",title:'Welcome! <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.",section:"Posts",handler:()=>{window.open("https://emharsha1812.github.io/tinkerwithml/","_blank")}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"papers2code-attention-is-all-you-need-transformer-implementation",title:"Attention Is All You Need - Transformer Implementation",description:"Complete PyTorch implementation of the Transformer architecture",section:"Papers2code",handler:()=>{window.location.href="/papers2code/attention-is-all-you-need"}},{id:"projects-codeitup",title:"CodeItUp",description:"A versatile online code editor with multi-language support",section:"Projects",handler:()=>{window.location.href="/projects/codeitup/"}},{id:"projects-alfred",title:"Alfred",description:"Your Local AI Coding Butler",section:"Projects",handler:()=>{window.location.href="/projects/llm/"}},{id:"projects-pulse",title:"PULSE",description:"A comprehensive Python library for synthetic sensor data generation",section:"Projects",handler:()=>{window.location.href="/projects/pulse/"}},{id:"projects-tinker-with-machine-learning",title:"Tinker with Machine Learning",description:"A curated place for all my AI learnings. Clearly explained",section:"Projects",handler:()=>{window.location.href="/projects/tinkerwithml/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%61%72%73%68%77%61%72%64%68%61%6E%66%61%72%74%61%6C%65.%6E%69%74%68@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-whatsapp",title:"WhatsApp",section:"Socials",handler:()=>{window.open("https://wa.me/919317439486","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emharsha1812","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/emharsha1812","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/emharsha1812","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>