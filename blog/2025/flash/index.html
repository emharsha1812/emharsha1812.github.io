<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers | Harshwardhan Sanjay Fartale </title> <meta name="author" content="Harshwardhan Sanjay Fartale"> <meta name="description" content="We present a technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with diagrams and Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms."> <meta name="keywords" content="harshwardhan fartale, harshwardhan, portfolio, Harsh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%B0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://emharsha1812.github.io/blog/2025/flash/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers",
            "description": "We present a technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with diagrams and Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms.",
            "published": "November 25, 2025",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Harshwardhan</span> Sanjay Fartale </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Harshwardhan </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/stats/">stats </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/frompapertocode/">Paper2code </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/papershelf/">Library </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers</h1> <p>We present a technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with diagrams and Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#background">Background</a> </div> <div> <a href="#flashattentionv1">FlashAttentionV1</a> </div> <div> <a href="#flashattentionv2">FlashAttentionV2</a> </div> <div> <a href="#flashattentionv3">FlashAttentionV3</a> </div> <div> <a href="#flashattentionv4">FlashAttentionV4</a> </div> <div> <a href="#open-problems-and-future-directions">Open Problems and Future Directions</a> </div> <div> <a href="#appendix-a-proving-standard-attention-s-quadratic-complexity">Appendix A Proving Standard Attention's Quadratic Complexity</a> </div> <div> <a href="#appendix-b-proving-standard-attention-is-memory-bound">Appendix B Proving Standard Attention is Memory Bound</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>The fundamental concept that underpins the transformer architecture is <strong>Attention</strong>. This was originally developed as an enhancement to RNNs for machine translation <d-cite key="bahdanau2016neuralmachinetranslationjointly"> </d-cite>. However, in 2017, Vaswani et al. <d-cite key="vaswani2023attentionneed"></d-cite> showed that significantly improved performance could be obtained by eliminating the recurrence structure and instead focusing exclusively on the attention mechanism.</p> <p>The importance of this mechanism can be explained with the help of the following example</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_1-480.webp 480w,/assets/img/blog/Figure_1-800.webp 800w,/assets/img/blog/Figure_1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/Figure_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Attention weights showing how the model resolves ambiguity in word meaning through context. The arrows indicate strong attention connections between 'bank' and contextually relevant words.</figcaption> </figure> <p>Consider the sentence <strong>“I swam across the river to get to the other bank.”</strong> The word “<strong>bank</strong>” has multiple meanings—it could refer to a financial institution or a riverbank. The attention mechanism helps the model understand context by weighing relationships between words. In this case, the model attends strongly to words like “swam,” “across,” and “river,” which provide contextual clues that “bank” refers to a riverbank rather than a financial institution.</p> <p>Therefore, the Attention mechanism has become the single most important mechanism driving the growth of Large Language Models. Over the years, several variants of the attention mechanism have been proposed such as Multi Query Attention (MQA) (cite), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), etc. For instance, here’s a non-exhaustive taxonomy of efficient attention mechanisms</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attentiontaxonomy-480.webp 480w,/assets/img/attentiontaxonomy-800.webp 800w,/assets/img/attentiontaxonomy-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/attentiontaxonomy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure adapted from <d-cite key="sun2025efficient"></d-cite></figcaption> </figure> <p>However, the transformer’s attention mechanism has a fatal flaw: it scales quadratically both in time and memory with sequence length, resulting in $O(n^2 \cdot d_{\text{model}})$ time complexity and $O(n^2)$ memory complexity. For example, a 2,048-token sequence requires 16 MB of memory for the attention matrix; at 16,384 tokens, this balloons to 1GB per layer. A rigorous mathematical proof is presented in Appendix A. This quadratic scaling makes it prohibitive for processing long sequences beyond 8K-16K tokens without specialized optimizations <d-cite key="keles2022computationalcomplexityselfattention"></d-cite>. While many works aim for sub-quadratic attention using approximations, including Linformer <d-cite key="wang2020linformerselfattentionlinearcomplexity"></d-cite>, Performer <d-cite key="choromanski2022rethinkingattentionperformers"></d-cite>, and Reformer <d-cite key="kitaev2020reformerefficienttransformer"></d-cite>, these methods have seen limited use in large language models. These are approximate attention methods that reduce cost through low-rank projections, kernel approximations, or sparse routing. These assumptions improve asymptotic complexity, but they introduce accuracy and hardware-efficiency tradeoffs, so large-scale models still rely on exact attention.</p> <p>The FlashAttention series by Tri Dao <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention2"></d-cite><d-cite key="shah2024flashattention3"></d-cite> looks at the attention bottleneck from a systems angle. Instead of approximating attention and hurting model quality, the idea is to rethink how attention moves data through the GPU. Modern GPUs have a very uneven memory hierarchy, so the cost of moving data often dominates the cost of doing the math. FlashAttention reduces this movement and gets closer to the limits of the hardware. In this blog, we walk through how this idea has evolved. FlashAttention v1 introduced tiled exact attention. FlashAttention v2 improved how work is split across the GPU. FlashAttention v3 added warp specialization, asynchrony, and low precision on Hopper to push utilization even higher. We also refer to what is known about FlashAttention 4, though an official paper is not public yet.</p> <h2 id="background">Background</h2> <h3 id="gpu-memory-hierarchy">GPU Memory Hierarchy</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_2-480.webp 480w,/assets/img/blog/Figure_2-800.webp 800w,/assets/img/blog/Figure_2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/blog/Figure_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Memory Hierarchy with Bandwidth &amp; Memory Size</figcaption> </figure> <p>Every modern processor faces the same fundamental challenge: fast storage is expense and small, while large storage is slow and cheap. Modern GPUs organise memory into a hierarchal form which has five distinct levels, each with different characteristics, different access patterns, and different implications for your code. Starting from the fastest and smallest and working towards the slowest and largest, these levels are: registers, shared memory, L1 cache, L2 cache, and global memory. (Figure 2). At the stop of the memory hierarchy sit registers, the fstest storage available on a GPU. Each thread runninng on the GPU has access to a private set of registers - typically up to 255 registers per thread on modern NVIDIA architectures. These registers feed directly into the computatinal units. When a thread performs an arithmatic operation, the operands come from registers and the result goes back to registers. There is no seperate “register access” operation visible to the programmer; registers are simply where the active data lives. The register file on a single Streaming Multiprocessor contains 65,536 registers with each register holding 32 bits. This gives 256 kilobytes of register storage per SM, and these registers are dynamically shared among all threads running on that SM</p> <p>Unlike registers, which are private to each individual threads, shared memory (SRAM) is shared among all threads in a thread block. It is the primary mechanism for threads to cooperate and community and is <strong>explicitly managed by the programmer rather than automatically managed by the compiler</strong>. Shared memory provides a staging area for data that multiple threads need to access. Rather than having each thread read the same value from slow global memory, one thread can read it once into shared memory, synchronize with the other threads, and then all threads can read from fast shared memory. Secondly, it enables algorithms that require threads to exchange data. This is used by FlashAttention explicitly</p> <p>At the bottom of the hierarchy sits global memory - the large DRAM pool that provides the bulk of a GPU’s storage capacity. An H100 for example, has 80GBs of HBM memory, operating at around 3,000 gigabytes per second of bandwidth. THis is where your input data starts, where your output data goes, and where any persistent state lives. It is also by far the slowest level of heirarchy, with individual access latencies reaching 400 to 800 clock cycles depending on architecture and access pattern.</p> <p>When we look at the GPU memory hierarchy in detail, we see a sharp difference in both latency and bandwidth across levels. Registers and shared memory sit close to the compute units and respond within a few cycles. HBM sits hundreds of cycles away with higher bandwidth but much higher latency. This separation means that the location of data often dictates runtime. As an example, the A100 GPU has 40GB of high bandwidth memory (HBM2e) with bandwidth 1.6TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s <d-cite key="nvidia2020a100"></d-cite>. This shows that the on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size.</p> <h3 id="attention-is-memory-bound-despite-on2-compute">Attention is memory bound despite $O(N^{2})$ compute</h3> <p>The efficiency of a kernel is governed by its arithmatic intensity, defined as the number of floating-point operations (FLOPs) performed per byte of memory access. Arithmatic Intensity is commonly used to measure whether operations can be classified as either compute-bound or memory-bound. A process is memory-bound when the execution speed is limited by how fast data can be moved between memory (HBM/RAM) and the processor cores, rather than by how fast the cores can compute. Typical examples include elementwise operations such as activation, dropout and reduction operations such as sum, softmax, batch norm, layer norm. A process is compute-bound when the execution speed is limited by the raw processing power (FLOPS) of the cores such as Matrix Multiplication, Convolution</p> \[\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}}\] <p>Standard Attention involves three primary stages</p> <ol> <li>Matrix Multiplication: $ S = Q K^T $</li> <li>Softmax: $ P = \text{softmax}(S) $</li> <li>Matrix Multiplication: $ O = P V $</li> </ol> <p>The matrix multiplications ($QK^T$ and $PV$) are compute-bound operations with high arithmetic intensity ($O(N^2 d)$ FLOPs vs $O(N^2)$ IO). However, the intermediate Softmax operation is memory-bound. It requires reading the entire $N \times N$ matrix $S$ from HBM, performing reduction operations, and writing the resulting $P$ matrix back to HBM. This $O(N^2)$ memory traffic saturates the HBM bandwidth, leaving the powerful Tensor Cores idle. (Detailed Proof provided in Appendix B)</p> <h2 id="flashattention---v1">FlashAttention - V1</h2> <p>One of the hardware-efficient mechanisms now widely adopted across different providers is Fast and Memory-Efficient Exact Attention with IO-Awareness, or FlashAttention. The “IO-Awareness” part of the title describes its core technical principle: optimizing data movement between GPU memory hierarchies.</p> <p>FlashAttention addresses the dual challenges of speed and memory consumption in transformers, especially on long sequences, by rethinking attention algorithms through the lens of GPU memory hierarchy awareness. The key insight is minimizing data movement between high-bandwidth memory (HBM) and on-chip SRAM.</p> <p>FlashAttention v1, published at Neural Information Processing Systems 2022 by Tri Dao and collaborators, introduced two key innovations: <strong>tiled</strong> attention that processes blocks of queries, keys, and values entirely in SRAM, and an <strong>online softmax algorithm</strong> that computes exact softmax incrementally without materializing the full attention matrix.</p> <h3 id="online-softmax-algorithm-enables-incremental-computation">Online Softmax Algorithm Enables Incremental Computation</h3> <p>Standard softmax computation requires three sequential passes over the data, making it inherently memory-intensive. The first pass finds the maximum value for numerical stability. The second pass computes exponentials and accumulates the normalization sum. The third pass normalizes each element. This three-pass structure can be expressed mathematically as follows:</p> <p>Given a vector $x \in \mathbb{R}^n$, the numerically stable softmax is computed in three passes:</p> \[\text{Pass 1:} \quad m = \max_{i} x_i\] \[\text{Pass 2:} \quad Z = \sum_{i=1}^{n} e^{\,x_i - m}\] \[\text{Pass 3:} \quad \text{softmax}(x)_i = \frac{e^{\,x_i - m}}{Z}\] <p>This approach ensures numerical stability by preventing overflow in the exponential computation. However, this three-pass dependency creates a critical bottleneck: we must materialize the entire attention matrix in HBM before proceeding. The softmax operation requires global information—specifically, the denominator in Pass 2 must sum over all $N$ elements. This seemingly requires loading the full row into memory before computing any output, making the process extremely memory I/O intensive and defeating attempts to tile the computation efficiently.</p> <p>The breakthrough came from <strong>online softmax</strong>, an algorithmic technique that computes softmax incrementally in blocks while maintaining running statistics for the maximum and the sum of exponentials. This method was originally discovered by Milakov and Gimelshein <d-cite key="milakov2018online"></d-cite> and later applied to sparse attention patterns by Child et al. <d-cite key="child2019generatinglongsequencessparse"></d-cite>. FlashAttention’s key innovation was adapting online softmax to work within a tiled attention algorithm, enabling exact softmax computation without materializing the full attention matrix. Instead of waiting for the global maximum $m_N$ across all elements before computing the normalization sum, the algorithm maintains running statistics that are updated as each new block of data is processed. This incremental approach transforms softmax into a streaming computation that can be fused with the surrounding matrix operations.</p> <h3 id="algebra-of-online-softmax">Algebra of Online Softmax</h3> <p>Let vector $x$ be split into two blocks $x^{(1)}$ and $x^{(2)}$. We compute local statistics for each block. The block size is chosen such that</p> <p>Local max: $m_1 = \max(x^{(1)})$, $m_2 = \max(x^{(2)})$</p> <p>Thus the local unnormalised sum becomes: $l_1 = \sum e^{x^{(1)} - m_1}$, $l_2 = \sum e^{x^{(2)} - m_2}$</p> <p>To combine these, we define the global max $m_{new} = \max(m_1, m_2)$. The global sum $l_{new}$ can be updated without revisiting the raw data of block 1 which is \(l_{new} = e^{m_1 - m_{new}} l_1 + e^{m_2 - m_{new}} l_2\)</p> <p>Let $O_{old}$ be the current accumulated output scaled by the old normalization factor. The correct update is: \(O_{new} = \text{diag}(l_{new})^{-1} \left( \text{diag}(l_{old}) e^{m_{old} - m_{new}} O_{old} + e^{m_{cur} - m_{new}} P_{cur} V_{cur} \right)\)</p> <p>The quadratic complexity of standard self-attention arises from three fundamental operations that each contribute $O(n^2)$ cost:</p> <ol> <li> <strong>Score computation:</strong> $O(n^2 d_k)$ - computing all pairwise similarities</li> <li> <strong>Softmax normalization:</strong> $O(n^2)$ - normalizing each of $n^2$ scores</li> <li> <strong>Value aggregation:</strong> $O(n^2 d_v)$ - aggregating all $n^2$ weighted values</li> </ol> <p>This</p> <h2 id="open-problems-and-future-directions">Open Problems and Future Directions</h2> <p>Despite these advances, several challenges remain. One is scaling beyond on-chip limits. Current FlashAttention relies on fitting entire blocks in hundreds of KB of SRAM. But LLMs push contexts to hundreds of thousands or even a million tokens, far beyond what fits on one GPU’s chip. Techniques like PagedAttention <d-cite key="kwon2023efficientmemorymanagementlarge"> </d-cite> (streaming attention from host memory in blocks) or Hydragen <d-cite key="juravsky2024hydragenhighthroughputllminference"> </d-cite> (optimizing shared prefixes) are only beginning to address this, but a fully general solution for trillion-token context still awaits. In theory, FlashAttention is IO-optimal for a given SRAM size so beyond-chip hierarchies (CPU memory, disk) must come into play, raising new algorithmic questions about streaming, compression, and multi-node attention.</p> <p>Another issue is programming and scheduling. The rapid improvements have largely come from hand-tuned CUDA kernels. Future efficiency will require better integration with compilers and ML frameworks. As Tri Dao notes, there is ongoing effort to make these optimizations “easily programmable” since current designs rely on manual warp scheduling and custom intrinsics. Moving to other platforms (AMD GPUs, TPUs, even CPUs) adds complexity: for example, AMD ROCm now supports FlashAttention via Triton, but the performance gap and engineering effort remain substantial. We see similar concerns in learned kernels: will XLA, MLIR, or DSLs be able to generate these tiled and overlapped patterns? Bridging the gap between compile-time scheduling (static tiling) and runtime adaptivity is an open compiler/hardware co-design problem.</p> <p>Finally, precision limits and numerical issues persist. Pushing to FP4 or mixed-integer kernels could double throughput again, but needs new algorithmic care (e.g. stochastic rounding, specialized normalization). FlashAttention-4’s lesson – that even math functions can be rethought in software – suggests any future hardware bottleneck (e.g. FP4 support) will inspire creative software solution</p> <h3 id="implications-for-long-context-llms">Implications for Long Context LLMs</h3> <p>Together, these principles directly serve the long-context frontier. Faster, memory-frugal attention means models can actually use very large windows of text. Today’s FlashAttention-enabled LLMs already handle contexts of 128K–1M tokens by carefully overlapping computation and memory. Tomorrow’s algorithms will push farther: for instance, if attention kernels reach multi-petaflop rates on next-gen GPUs, then 10× longer sequences become feasible in practice. In short, hardware-software co-design is the key enabler for ultra-long-context LLMs. By combining IO-efficient kernels (tiling and recompute), parallel pipelines, and smart approximations or sparsity, the community is paving the way for Transformer attention to scale to truly massive contexts with manageable compute and memory costs</p> <h2 id="appendix-a">Appendix A</h2> <p>The standard self-attention mechanism used in Transformers is formulated as \(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</p> <p>Where:</p> <ul> <li>$Q \in \mathbb{R}^{n \times d_k}$ is the query matrix</li> <li>$K \in \mathbb{R}^{n \times d_k}$ is the key matrix</li> <li>$V \in \mathbb{R}^{n \times d_v}$ is the value matrix</li> <li>$n$ is the sequence length</li> <li>$d_k$ and $d_v$ are the key and value dimensions</li> <li>$\sqrt{d_k}$ is the scaling factor for numerical stability</li> </ul> <p>Computing the score matrix involves:</p> \[S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}\] <p>This matrix multiplication requires computing the dot product between all pairs of query and key vectors. For each of the $n^2$ pairs $(i,j)$, we compute:</p> \[S_{ij} = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}\] <p>Since each dot product involves $d_k$ multiplications and $d_k - 1$ additions, the total computational cost is:[^1]</p> \[\text{Time Complexity (Step 1)} = O(n^2 d_k)\] <p><strong>Memory Complexity:</strong> The $n \times n$ score matrix $S$ requires $O(n^2)$ space to store.</p> <p>The softmax function is applied row-wise to normalize the attention scores \(\text{softmax}(S)_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^n \exp(S_{ik})}\)</p> <p>For each row $i \in [n]$, this involves:</p> <ul> <li>Computing $n$ exponentials: $O(n)$ operations</li> <li>Computing the row sum: $O(n)$ operations</li> <li>Normalizing each element: $O(n)$ operations</li> </ul> <p>Since we have $n$ rows, the total cost is:</p> \[\text{Time Complexity (Step 2)} = O(n^2)\] <p>Finally, multiply the normalized attention matrix with the value matrix:</p> \[\text{Output} = \text{softmax}(S) \cdot V\] <p>Where $\text{softmax}(S) \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{n \times d_v}$.</p> <p>This matrix multiplication requires computing: \(\text{Output}_{ij} = \sum_{k=1}^n \text{softmax}(S)_{ik} \cdot V_{kj}\)</p> <p>For each of the $n \times d_v$ elements in the output, we perform $n$ operations, yielding:</p> \[\text{Time Complexity (Step 3)} = O(n^2 d_v)\] <p>Combining all three steps:</p> \[\text{Total Time} = O(n^2 d_k) + O(n^2) + O(n^2 d_v) = O(n^2(d_k + d_v))\] <p>In standard Transformer configurations, the embedding dimension $d_{\text{model}}$ is typically divided equally among multiple attention heads. For a single attention head, $d_k \approx d_v \approx \frac{d_{\text{model}}}{h}$ where $h$ is the number of heads. Therefore, the complexity simplifies to:[^2]</p> \[\boxed{\text{Time Complexity} = O(n^2 \cdot d_{\text{model}})}\] <p><strong>Memory Complexity:</strong></p> <p>The dominant memory requirement comes from storing the intermediate attention score matrix</p> \[S \in \mathbb{R}^{n \times n} \quad \Rightarrow \quad \boxed{\text{Memory Complexity} = O(n^2)}\] <p>Additional memory requirements include:</p> <ul> <li>Storing $Q, K, V$ matrices: $O(3 \cdot n \cdot d)$ which is $O(nd)$ overall</li> <li>Temporary buffers for computations: $O(nd)$</li> </ul> <p>However, these are dominated by $O(n^2)$ for large sequence lengths.</p> <p>The quadratic complexity creates severe bottlenecks for processing long sequences:</p> <table> <thead> <tr> <th style="text-align: left">Sequence Length</th> <th style="text-align: left">Relative Cost</th> <th style="text-align: left">Memory (GB at $d=768$)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">512</td> <td style="text-align: left">1×</td> <td style="text-align: left">1.5</td> </tr> <tr> <td style="text-align: left">2,048</td> <td style="text-align: left">16×</td> <td style="text-align: left">24</td> </tr> <tr> <td style="text-align: left">8,192</td> <td style="text-align: left">256×</td> <td style="text-align: left">384</td> </tr> <tr> <td style="text-align: left">65,536</td> <td style="text-align: left">16,384×</td> <td style="text-align: left">$&gt;6$ TB</td> </tr> </tbody> </table> <h3 id="theoretical-lower-bounds">Theoretical Lower Bounds</h3> <p>Research using complexity theory has proven that this quadratic complexity is <strong>fundamental and unavoidable</strong> under the Strong Exponential Time Hypothesis (SETH). The key theorem states:[^3][^1]</p> <p>For the softmax dot-product self-attention with $d_q = \omega(\log n)$, for any $\epsilon &gt; 0$:</p> \[\text{Computing self-attention requires } \Omega(n^{2-\epsilon}) \text{ time}\] <p>This holds even for:</p> <ul> <li> <strong>Exact computation</strong> of attention scores</li> <li> <table> <tbody> <tr> <td> <strong>Approximate computation</strong> with multiplicative error $\mu$: $</td> <td>\hat{Y}<em>{ij} - Y</em>{ij}</td> <td>\leq \mu</td> <td>Y_{ij}</td> <td>$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td> <strong>Approximate computation</strong> with additive error $\mu$: $</td> <td>\hat{Y}<em>{ij} - Y</em>{ij}</td> <td>\leq \mu$</td> </tr> </tbody> </table> </li> </ul> <h3 id="why-self-attention-cannot-escape-quadratic-complexity">Why Self-Attention Cannot Escape Quadratic Complexity</h3> <p>The proof uses reductions from the <strong>Orthogonal Vectors Problem (OVP)</strong>, which is conjectured to require nearly quadratic time. The intuition is:</p> <ol> <li>Computing attention requires evaluating pairwise interactions: $O(n^2)$ pairs</li> <li>Each pair requires a dot product computation: $O(d_k)$ time</li> <li>Even with approximations, one must examine enough pairs to distinguish correct answers from incorrect ones</li> <li>Thus, the quadratic barrier in $n$ is unavoidable[^3]</li> </ol> <h2 id="appendix-b--proving-standard-attentions-memory-bound">Appendix B : Proving Standard Attention’s Memory Bound</h2> <p>The standard Attention Implementation requires $\Theta(Nd + N^2)$ HBM accesses. This can be computed as</p> <ol> <li> <p>Computing $S = QK^T$: Reads $Q$ and $K$, writes $S$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$</p> </li> <li> <p>Computing $P = \text{softmax}(S)$: Reads $S$, writes $P$ to HBM $\rightarrow$ $\Theta(N^2)$.</p> </li> <li> <p>Computing $O = PV$: Reads $P$ and $V$, writes $O$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$.</p> </li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-11-25-flash.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"emharsha1812/emharsha1812.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Harshwardhan Sanjay Fartale. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-harshwardhan",title:"Harshwardhan",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"Projects that I built over the course of my journey",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-stats",title:"stats",description:"",section:"Navigation",handler:()=>{window.location.href="/stats/"}},{id:"nav-cv",title:"cv",description:"My Resume. You can download it from the button right there \ud83d\udc49",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-paper2code",title:"Paper2code",description:"A collection of paper-to-code implementations. This page serves as a testament to my skills in deciphering research papers and translating theoretical concepts into functional code.",section:"Navigation",handler:()=>{window.location.href="/frompapertocode/"}},{id:"nav-teaching",title:"teaching",description:"Highlights of teaching experience and contributions",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-library",title:"Library",description:"Curated reading list with quick filters and links to notes.",section:"Navigation",handler:()=>{window.location.href="/papershelf/"}},{id:"post-can-we-really-identify-llm-generated-text-the-promise-and-limits-of-watermarking",title:"Can we really identify LLM Generated Text? The promise and limits of watermarking...",description:"Exploring the theoretical and practical aspects of watermarking techniques for detecting AI-generated content, including trade-offs, failure modes, and information-theoretic limits",section:"Posts",handler:()=>{window.location.href="/blog/2025/llmgenerated/"}},{id:"post-flashattention-through-the-years-how-io-aware-kernels-reshaped-scalable-transformers",title:"FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers",description:"We present a technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with diagrams and Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms.",section:"Posts",handler:()=>{window.location.href="/blog/2025/flash/"}},{id:"post-r-squared-in-machine-learning",title:"R squared in Machine Learning",description:"Meaning, Explanation & more",section:"Posts",handler:()=>{window.location.href="/blog/2025/rsquared/"}},{id:"post-training-a-simple-bigram-character-level-model-on-tiny-stories",title:"Training a simple bigram character level model on tiny stories",description:"Training a simple bigram character level model on tiny stories",section:"Posts",handler:()=>{window.location.href="/blog/2025/bigram/"}},{id:"post-machine-learning-and-ai-resources",title:"Machine Learning and AI Resources",description:"A collection of links to essential courses on machine learning, deep learning, natural language processing, and artificial intelligence.",section:"Posts",handler:()=>{window.location.href="/blog/2025/nptel-ml/"}},{id:"post-python-notes",title:"Python Notes",description:"A collection of Python notebooks for quick reference",section:"Posts",handler:()=>{window.location.href="/blog/2025/python-notes/"}},{id:"post-kan-kolmogorov-arnold-networks",title:"KAN (Kolmogorov-Arnold Networks)",description:"An Alternative to traditional MLPs",section:"Posts",handler:()=>{window.location.href="/blog/2025/kan/"}},{id:"post-a-visit-to-hungarian-mathematics",title:"A Visit to Hungarian Mathematics",description:"Why Hungarians are so darn good at mathematics ?",section:"Posts",handler:()=>{window.location.href="/blog/2024/hungarian-mathematics/"}},{id:"post-welcome",title:'Welcome! <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.",section:"Posts",handler:()=>{window.open("https://emharsha1812.github.io/tinkerwithml/","_blank")}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"papers2code-attention-is-all-you-need-transformer-implementation",title:"Attention Is All You Need - Transformer Implementation",description:"Complete PyTorch implementation of the Transformer architecture",section:"Papers2code",handler:()=>{window.location.href="/papers2code/attention-is-all-you-need"}},{id:"projects-codeitup",title:"CodeItUp",description:"A versatile online code editor with multi-language support",section:"Projects",handler:()=>{window.location.href="/projects/codeitup/"}},{id:"projects-alfred",title:"Alfred",description:"Your Local AI Coding Butler",section:"Projects",handler:()=>{window.location.href="/projects/llm/"}},{id:"projects-pulse",title:"PULSE",description:"A comprehensive Python library for synthetic sensor data generation",section:"Projects",handler:()=>{window.location.href="/projects/pulse/"}},{id:"projects-tinker-with-machine-learning",title:"Tinker with Machine Learning",description:"A curated place for all my AI learnings. Clearly explained",section:"Projects",handler:()=>{window.location.href="/projects/tinkerwithml/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%61%72%73%68%77%61%72%64%68%61%6E%66%61%72%74%61%6C%65.%6E%69%74%68@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-whatsapp",title:"WhatsApp",section:"Socials",handler:()=>{window.open("https://wa.me/919317439486","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/emharsha1812","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/emharsha1812","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/emharsha1812","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>