<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://emharsha1812.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emharsha1812.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-28T09:35:35+00:00</updated><id>https://emharsha1812.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Python Notes</title><link href="https://emharsha1812.github.io/blog/2025/python-notes/" rel="alternate" type="text/html" title="Python Notes"/><published>2025-01-22T00:12:00+00:00</published><updated>2025-01-22T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/python-notes</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/python-notes/"><![CDATA[<p>Hello everyone,<br/> Below, I am sharing the GitHub repository containing all my Python notebooks, which I created while teaching an AI-ML Ops training program to esteemed DRDO scientists. You can access the GitHub repository here: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">GitHub Link</a>.</p> <p>This repository is a work in progress, and I will continue to update it as I create new notebooks. Here is the current plan for upcoming content:</p> <ol class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Python One-Liners Notebook</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Walrus Operator in Python</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Building a simple project using Python</li> </ol> <p>Feel free to explore and stay tuned for updates!</p> <p>Here’s the direct GitHub link for quick access: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">https://github.com/emharsha1812/Python_Programming_Notebooks</a></p> <p>Don’t forget to ⭐ star the repository to stay updated with new additions!</p> <h3 id="ps---here-is-a-list-of-helpful-links-that-you-can-reference-from-time-to-time">PS - Here is a list of helpful links that you can reference from time to time</h3> <ol> <li> <p><a href="https://realpython.com/python-iterators-iterables/">Python Iterators</a> - A super friendly blog on Python Iterators. I recommend taking this <a href="https://realpython.com/quizzes/python-iterators-iterables/">quiz</a> as well after reading the whole blog.</p> </li> <li> <p><a href="https://realpython.com/python-memory-management/">Memory Management in Python</a> - One of those “You should definitely know this” topics.</p> </li> </ol>]]></content><author><name></name></author><category term="coding,"/><category term="python"/><category term="coding,"/><category term="python"/><summary type="html"><![CDATA[A collection of Python notebooks for quick reference]]></summary></entry><entry><title type="html">Tech Blog Repository - My Learning Journey</title><link href="https://emharsha1812.github.io/blog/2025/blogstack/" rel="alternate" type="text/html" title="Tech Blog Repository - My Learning Journey"/><published>2025-01-16T09:50:00+00:00</published><updated>2025-01-16T09:50:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/blogstack</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/blogstack/"><![CDATA[<blockquote> <p>Knowledge is of no value unless you put it into practice ~ Anton Chekhov</p> </blockquote> <p>As a software engineer and ML enthusiast, I often come across insightful blogs that provide valuable knowledge and practical insights. This post serves as my personal repository of such resources, organized by topics for easy reference.</p> <h3 id="generative-ai">Generative AI</h3> <ol> <li><a href="https://huyenchip.com/2025/01/16/ai-engineering-pitfalls.html"><em>Common pitfalls when building generative AI applications</em></a> by Chip Huyen</li> </ol> <p>This is a living document that I’ll continue to update as I discover more valuable resources. Each blog has been selected based on its depth, practical applicability, and clear explanations.</p> <p>Feel free to suggest any additions or share your thoughts on these resources!</p> <p><em>Last Updated: January 17, 2025</em></p>]]></content><author><name></name></author><category term="learning,resources"/><category term="learning,resources,tech"/><summary type="html"><![CDATA[A curated collection of influential technical blogs that shaped my understanding]]></summary></entry><entry><title type="html">KAN (Kolmogorov-Arnold Networks)</title><link href="https://emharsha1812.github.io/blog/2025/kan/" rel="alternate" type="text/html" title="KAN (Kolmogorov-Arnold Networks)"/><published>2025-01-07T00:12:00+00:00</published><updated>2025-01-07T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/kan</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/kan/"><![CDATA[<h2 id="1-mathematical-foundations--architecture">1. Mathematical Foundations &amp; Architecture</h2> <p><strong>Kolmogorov-Arnold Representation Theorem:</strong> KANs are founded on a classic result by A. N. Kolmogorov and V. Arnold, which states that <em>any continuous multivariate function can be represented as a finite superposition of univariate functions</em>(<a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov%E2%80%99s%20theorem%2C%20proposed%20in%201957%2C,The%20CFL%20condition%2C%20introduced%20by">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a>). In practical terms, this theorem guarantees that for a function \(f(x_1,\dots,x_n)\), there exist some continuous 1D functions \(\{\Phi_q\}\) and \(\{\Psi_{q,p}\}\) such that:</p> \[f(x_1,\ldots,x_n) \;=\; \sum_{q=0}^{2n} \; \Phi_q\!\Big( \sum_{p=1}^n \Psi_{q,p}(x_p) \Big)\,,\] <p>i.e. \(f\) can be decomposed into <strong>inner</strong> univariate functions \(\Psi_{q,p}\) (each depending on a single input variable $x_p$) and <strong>outer</strong> univariate functions \(\Phi_q\) aggregated by addition. This theorem provides a constructive blueprint for function approximation using single-variable building blocks, which is the key inspiration for KANs</p> <p><strong>KAN Architecture</strong> - Instead of the traditional neuron model with linear weighted sums and fixed activations, a KAN implements the above idea by making <strong>each edge</strong> of the network carry a <em>learnable univariate function</em>. In other words, every connection between neurons is parameterized as a nonlinear function (originally chosen as a B-spline) rather than a scalar weight. Each neuron simply sums up the outputs of the incoming edge-functions. Formally, if \(z_i^{(l)}\) denotes the \(i\)-th activation in layer \(l\), then a <strong>KAN layer</strong> computes each output neuron \(j\) as: (<a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview</a>) \(z_j^{(l+1)} \;=\; \sum_{i=1}^{N_l} f_{ij}^{(l)}\!\Big(z_i^{(l)}\Big)\,,\)</p> <p>where \(f_{ij}^{(l)}: \mathbb{R}\to\mathbb{R}\) is a learnable univariate function on the edge from neuron \(i\) (layer \(l\)) to neuron \(j\) (layer \(l+1\)). There are no separate linear weight matrices; the nonlinearity of \(f_{ij}\) itself provides the transformation. In the <em>shallowest</em> case (two-layer KAN), this architecture directly mirrors Kolmogorov’s decomposition: the first layer learns inner functions \(h_{p}(x_p)\) on each input dimension, and the second layer learns outer functions \(g_q(\cdot)\) that combine those results <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Quanta Magazine</a>.</p> <p><strong>Parameterized Functions (B-Splines):</strong> In practice, each learnable edge-function \(f_{ij}\) is parameterized as a spline (often a B-spline) with a set of control points that can be tuned during training. B-splines are piecewise polynomial curves defined by control points, offering a flexible yet smooth basis for approximating arbitrary 1D functions. By adjusting the control points, the shape of the spline changes locally without affecting the entire function. This choice ensures the learned activation functions are <em>smooth</em> addressing potential non-smoothness in Kolmogorov’s original construction and stable to train. Each edge thus has multiple parameters (the spline control values) instead of a single weight. For example, a KAN might initialize each \(f_{ij}\) as a near-linear spline and then let training mold each into the required nonlinear shape. This edge-centric design lets KANs <em>dynamically adapt their activation functions</em> to the data, rather than relying on a fixed function like ReLU or tanh.</p> <p><strong>Illustrative Pseudocode:</strong> The following pseudocode contrasts a single layer of an MLP vs. a KAN:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanmlp.svg-480.webp 480w,/assets/img/kanmlp.svg-800.webp 800w,/assets/img/kanmlp.svg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanmlp.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Comparison of MLP Layer with KAN Layer in Pytorch </div> <p>In the KAN layer, <code class="language-plaintext highlighter-rouge">f_ij</code> is a learned function (e.g. a spline) specific to edge \((i\to j)\), replacing both the weight and the neuron’s activation for that connection. The neuron simply aggregates these contributions (here via summation). Deep KANs can be built by stacking such layers, allowing composition of these univariate transformations across multiple levels.</p> <h2 id="2-comparison-with-mlps">2. Comparison with MLPs</h2> <p><strong>Structural Differences:</strong> Traditional Multi-Layer Perceptrons (MLPs) use <em>linear weights</em> and <em>fixed activation functions at neurons</em>, whereas KANs use <em>no linear weights at all</em> – every “weight” is replaced by a flexible function on the input signal. In effect, MLPs learn parameters for <strong>nodes</strong> (the weight matrix between layers is trained, then a fixed nonlinearity like ReLU is applied), while KANs learn parameters for <strong>edges</strong> (each connection has a trainable nonlinear mapping). This leads to a duality: <em>MLP = fixed nonlinearity + learned linear weights; KAN = fixed linear sum + learned nonlinear functions</em>. The figure below (from Liu et al. 2024) illustrates this difference, highlighting that MLPs apply activations at neurons (circles) whereas KANs apply learned functions on each connecting edge before summing.(<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanvsmlp-480.webp 480w,/assets/img/kanvsmlp-800.webp 800w,/assets/img/kanvsmlp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanvsmlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Source: <a href="https://arxiv.org/pdf/2404.19756" target="_blank">Liu et al. (2024)</a> </div> <p><strong>Learnable Functions vs Fixed Weights:</strong> In an MLP, the transformation from layer to layer is \(\sigma(Wx + b)\), with \(\sigma\) (e.g. ReLU) fixed and \(W,b\) learned. In a KAN, the transformation is \(\sum_i f_i(x_i)\) (plus bias if needed), with each $f_i$ being learned and no separate \(W\). Essentially, KANs “allocate” more flexibility per connection, whereas MLPs rely on combining many fixed nonlinear units to build complexity. This means KANs move the bulk of learnable parameters into the activation functions themselves, often resulting in <em>fewer total connections</em> needed than an equivalent MLP (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a>).</p> <p><strong>Expressive Power (Universal Approximation):</strong> Both MLPs and KANs are universal function approximators, but via different theorems. MLPs leverage the Universal Approximation Theorem (with enough neurons, an MLP can approximate any continuous function on a domain), while KANs directly leverage the Kolmogorov-Arnold (K-A) theorem to construct such approximations. In theory, a single hidden-layer KAN with sufficiently complex edge functions can exactly represent any continuous function (the K-A theorem provides an existence proof), whereas an MLP might require many more neurons or layers to approximate the same function with fixed activations. KANs thus excel at modeling functions with complex or “spiky” behavior in each input dimension, because each edge can carve out a detailed univariate relationship. In practice, KANs implement the K-A decomposition <em>explicitly</em>, using B-spline basis functions to approximate the required univariate mappings. This can translate to <em>greater expressivity per parameter</em>.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>).</p> <p><strong>Parameter Efficiency &amp; Neural Scaling:</strong> A striking reported advantage is that <em>much smaller KANs can achieve accuracy comparable or superior to much larger MLPs</em> on certain tasks. Each KAN edge function (with, say, $k$ control points) can encode a nonlinear relation that an MLP might need multiple neurons and layers to capture. Empirically, Liu <em>et al.</em> (2024) found KANs follow faster <strong>neural scaling laws</strong> – the error decreases more rapidly as model size increases, compared to MLPs. In other words, to reach a given accuracy, a KAN required fewer trainable parameters than an MLP in their tests. The flexibility of splines allows KANs to fit complex patterns without blowing up the network width/depth. One study noted that KANs can <em>match</em> MLP performance at equal parameter counts, and sometimes exceed it, though they require careful tuning (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=KANs%20definitely%20don%27t%20feel%20like,compared%20to%20regular%20neural%20networks">Trying Kolmogorov-Arnold Networks in Practice</a>). The original KAN paper demonstrated that a KAN with significantly fewer nodes could outperform a dense ReLU network on function-fitting benchmarks.</p> <p><strong>Continuous Learning and Locality:</strong> Because each KAN weight is a localized function (with local control points), learning in a KAN can be more localized. This has implications for <strong>continual learning</strong>. In standard nets, fine-tuning on new data often alters weights globally and can erode old capabilities (catastrophic forgetting). In KANs, adding new data primarily adjusts the spline control points <em>in relevant regions of the input space</em>, leaving other regions (and other functions) mostly unchanged. For example, if a KAN-based language model learns a new vocabulary or coding style, only certain edge-functions for those inputs might reshape, while others retain their previously learned shape. This property means KANs can integrate new knowledge without overwriting all weights, potentially enabling more <strong>seamless continual learning</strong>. MLPs, by contrast, have distributed representations where a single weight doesn’t correspond to an isolated input relationship, making targeted updates harder.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,local%20control%20point%20parameters%20change">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>)</p> <p><strong>Interpretability:</strong> A major motivation for KANs is interpretability. In an MLP, each weight by itself is usually not meaningful, and neurons combine many weights making interpretation difficult. In a KAN, each edge’s function $f_{ij}(x)$ can be visualized as a curve, directly showing how the input from neuron $i$ influences neuron $j$ across the range of values. After training, one can <em>extract these learned univariate functions</em> and inspect them.They might correspond to intuitive relations (e.g. an edge function might learn a sinusoidal shape if the output depends sinusoidally on an input).This transparency is especially useful in scientific or engineering tasks where understanding the learned model is as important as its accuracy. MLPs lack this fine-grained interpretability, since their learned mapping is entangled across many parameters. Thus, KANs offer a more human-understandable model: as the saying goes, they turn the <strong>“black box”</strong> into a collection of readable 1D transformations.</p> <p><strong>Summary:</strong> KANs and MLPs both approximate complex functions, but KANs do so by <em>baking learnable math into the connections</em>. This difference yields advantages in function approximation fidelity, parameter efficiency, and interpretability. However, it also comes with computational challenges (will uupdate later). In essence, KANs can be seen as a <strong>new paradigm</strong>: they trade the simple, generic structure of MLPs for a structure with built-in mathematical richness (the Kolmogorov-Arnold basis). This seemingly small change – moving from scalar weights to learned functions – has profound implications on how the network learns and what it can represent (<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>).</p> <p>The original paper can be found <a href="https://arxiv.org/pdf/2404.19756">here</a></p> <p>Last Updated - 25/02/2025</p> <h3 id="references">References</h3> <p>[1] - <a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov,the%20model%E2%80%99s%20flexibility%20and%20interpretability">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a></p> <p>[2] - <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Novel Architecture Makes Neural Networks More Understandable</a></p> <p>[3] - <a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview on KAN: Kolmogorov–Arnold Networks</a></p> <p>[4] - <a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a></p> <p>[5] - <a href="https://www.datacamp.com/tutorial/kolmogorov-arnold-networks#:~:text=simpler%2C%20univariate%20ones,edges%20are%20used%20for%20approximation">Kolmogorov-Arnold Networks (KANs): A Guide With Implementation</a></p> <p>[6] - <a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a></p> <p>[7] - <a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a></p> ]]></content><author><name></name></author><category term="llm,machine-learning,python"/><category term="kan,deep-learning,research"/><summary type="html"><![CDATA[An Alternative to traditional MLPs]]></summary></entry><entry><title type="html">A Visit to Hungarian Mathematics</title><link href="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/" rel="alternate" type="text/html" title="A Visit to Hungarian Mathematics"/><published>2024-09-01T00:12:00+00:00</published><updated>2024-09-01T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/hungarian-mathematics</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/"><![CDATA[<p>Because my domain is machine learning (ML) and artificial intelligence (AI), it’s quite common for me to come across abstract mathematical equations that initially made no sense to me. However, as I dove deep into those arcane-looking symbols, I discovered another interesting thing aside from the meaning: I realized that most of the founders of these equations were from Hungary!</p> <p>I googled, and then, a few clicks later, I stumbled on this very interesting read titled <strong>‘A Visit to Hungarian Mathematics’</strong>. Here’s the <a href="https://gwern.net/doc/math/1993-hersh.pdf">link</a> 🔗 to this pape . It contained exactly what I was looking for; a detailed introspection into Hungarian mathematicss and the mathematicians surrounding them. While reading the paper, I found a very simple but profoundly impactful quote that I would like to share. It says</p> <blockquote> <p>Everyone has ideas, both right ideas and wrong ideas. Scientific work consists merely of seperating them</p> </blockquote> <p>I immediately started voraciously reading the paper from top to bottom, left to right, and backwards too. Even though I am not a mathematician and do not have a mathematics degree (although I have a fairly good amount of mathematical exposure), I love to explore mathematics as a hobby. I sincerely hope that my interest in mathematics is still relevant.</p> <p>One of the key takeaways from this paper will be a quote by Alfred Renyi, a Hungarian mathematician known for his work in probability theory, combinatorics, and other fields. Once, a gifted mathematician told him that his working ability was heavily dependent on external circumstances. Renyi responded,</p> <blockquote> <p><strong>“When I’m unhappy, I use math to find happiness; when I’m content, I use math to maintain my happiness.”</strong></p> </blockquote> <p>Reading about these mathematicians and their passion for pondering, fighting, and finally solving math problems fills me with a deep sense of gratitude towards math.</p>]]></content><author><name></name></author><category term="mathematics,culture"/><category term="mathematics,"/><category term="axioms"/><summary type="html"><![CDATA[Why Hungarians are so darn good at mathematics ?]]></summary></entry><entry><title type="html">Welcome!</title><link href="https://emharsha1812.github.io/blog/2024/welcome/" rel="alternate" type="text/html" title="Welcome!"/><published>2024-08-27T00:00:00+00:00</published><updated>2024-08-27T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/welcome</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/welcome/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.]]></summary></entry></feed>