<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://emharsha1812.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emharsha1812.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-05T14:15:54+00:00</updated><id>https://emharsha1812.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers</title><link href="https://emharsha1812.github.io/blog/2025/flash/" rel="alternate" type="text/html" title="FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers"/><published>2025-11-25T00:00:00+00:00</published><updated>2025-11-25T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/flash</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/flash/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The fundamental concept that underpins the transformer architecture is <strong>Attention</strong>. This was originally developed as an enhancement to RNNs for machine translation <d-cite key="bahdanau2016neuralmachinetranslationjointly"> </d-cite>. However, in 2017, Vaswani et al. <d-cite key="vaswani2023attentionneed"></d-cite> showed that significantly improved performance could be obtained by eliminating the recurrence structure and instead focusing exclusively on the attention mechanism.</p> <p>The importance of this mechanism can be explained with the help of the following example</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_1-480.webp 480w,/assets/img/blog/Figure_1-800.webp 800w,/assets/img/blog/Figure_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/Figure_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Attention weights showing how the model resolves ambiguity in word meaning through context. The arrows indicate strong attention connections between 'bank' and contextually relevant words.</figcaption> </figure> <p>Consider the sentence “I swam across the river to get to the other bank.” The word “bank” has multiple meanings—it could refer to a financial institution or a riverbank. The attention mechanism helps the model understand context by weighing relationships between words. In this case, the model attends strongly to words like “swam,” “across,” and “river,” which provide contextual clues that “bank” refers to a riverbank rather than a financial institution.</p> <p>Therefore, the Attention mechanism has become the single most important mechanism driving the growth of Large Language Models. Over the years, several variants of the attention mechanism have been proposed such as Multi Query Attention (MQA) (cite), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), etc. For instance, here’s a non-exhaustive taxonomy of efficient attention mechanisms</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attentiontaxonomy-480.webp 480w,/assets/img/attentiontaxonomy-800.webp 800w,/assets/img/attentiontaxonomy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attentiontaxonomy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure adapted from <d-cite key="sun2025efficient"></d-cite></figcaption> </figure> <p>However the transformer’s attention mechanism has a fatal flaw: it scales quadratically both in time and memory with sequence length. For example - For a 2,048-token sequence, the attention matrix consumes 16 MB of memory; at 16,384 tokens, it balloons to 1GB per layer. A Rigorous mathematical proof is presented in Appendix A (link here). There have been attempts at sub-quadratic complexity using approximate attention methods such as Linformer <d-cite key="wang2020linformerselfattentionlinearcomplexity"></d-cite>, Performer <d-cite key="choromanski2022rethinkingattentionperformers"></d-cite>, Reformer &lt;d-cite key=efficienttransformer”&gt; &lt;/d-cite&gt;, etc., but these haven’t been adopted in large language models due to their fundamental tradeoffs</p> <p>results in $O(n^2 \cdot d_{\text{model}})$ time complexity and $O(n^2)$ memory complexity, making it prohibitive for processing long sequences beyond 8K-16K tokens without specialized optimizations <d-cite key="keles2022computationalcomplexityselfattention"> </d-cite>.</p> <h2 id="background">Background</h2> <p>One of the hardware-efficient mechanisms that is now widely adopted in different providers is Fast and Memory-Efficient Exact Attention with IO-Awareness, or FlashAttention. The “IO-Awareness” part of the title describes its core technical principle, which involves optimizing data movement.</p> <p>To give a perspective on why it was needed, the standard attention mechanism, which is mathematically defined as For Q (Query), K (Key), V (Value) matrices {belongs to} R^(NxDk) where N is ___ and D is the dimensions of the embedding of each token, has two major flaws.</p> <ol> <li> <p>It is very IO dependent (large number of memory accesses in standard attention).</p> </li> <li> <p>The time and memory complexity of self-attention are quadratic (O(N^2)) in sequence length. (Proof given in Appendix A - Why Self Attention is quadratic)</p> </li> </ol> <p>These two major issues make transformers slow and memory-hungry, especially on long sequences.</p> <p>FlashAttention addresses this by rethinking attention algorithms through the lens of GPU memory heirarchy awarenesss which is minimizing the data movement between high-bandwidth memory (HBM) and on-chip SRM.</p> <p>Problem 1 - The HBM/Latency Problem</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Pyramid-480.webp 480w,/assets/img/Pyramid-800.webp 800w,/assets/img/Pyramid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Pyramid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When we look at the GPU memory hierarchy in detail, we see a sharp difference in both latency and bandwidth across levels. Registers and shared memory sit close to the compute units and respond within a few cycles. HBM sits hundreds of cycles away with higher bandwidth but much higher latency. This separation means that the location of data often dictates runtime. As an example, the H100 GPU has 80GB of high bandwidth memory (HBM3) with bandwidth 3.35TB/s and 256KB of on-chip SRAM per each of 132 streaming multiprocessors with bandwidth estimated around 33TB/s <d-cite key="nvidia2022h100"></d-cite>. This shows that the on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size.</p> <p>The standard Attention Implementation requires $\Theta(Nd + N^2)$ HBM accesses. This can be computed as</p> <ol> <li> <p>Computing $S = QK^T$: Reads $Q$ and $K$, writes $S$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$</p> </li> <li> <p>Computing $P = \text{softmax}(S)$: Reads $S$, writes $P$ to HBM $\rightarrow$ $\Theta(N^2)$.</p> </li> <li> <p>Computing $O = PV$: Reads $P$ and $V$, writes $O$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$.</p> </li> </ol> <h2 id="flashattention---v1">FlashAttention - V1</h2> <p>FlashAttention v1, published at Neural Information Processing Systems 2022, by Tri Dao and collaborators, introducted two key innovations: tiled attention that processes blocks of queries, keys, and values entirely in SRAM, and an online softmax algorithm that computes exact softmax incrementally without materializing the full attention matrix.</p> <h3 id="online-softmax-algorithm-enables-incremental-computation">Online Softmax algorithm enables incremental computation</h3> <p>Standard softmax requires three passes over the data: one pass to find the maximum value for numerical stability. A second pass to compute exponentials and accumulate the normalization sum. A third pass to normalize each element. This can be shown mathematically as follows:</p> <p>Given a vector $(x \in \mathbb{R}^n)$, the stable softmax is computed in three passes:</p> \[\text{Pass 1:} \quad m = \max_{i} x_i\] \[\text{Pass 2:} \quad Z = \sum_{i=1}^{n} e^{\,x_i - m}\] \[\text{Pass 3:} \quad \text{softmax}(x)_i = \frac{e^{\,x_i - m}}{Z}\] <p>This approach gives us the numerically stable softmax. However the dependency forces us to materialize the entire attention matrix in the HBM. Moreover, softmax requires global information (Pass 2). Since the denominator sums over all $N$ elements,seemingly requiring the full row before computing any output.</p> <p>FlashAttention leverages the idea of online softmax; the idea of computing softmax in blocks, while maintaining a running maximum and a running sum of exponentials. It was discovered by Milakov et al <d-cite key="milakov2018online"> and then used by Child et al <d-cite key="child2019generatinglongsequencessparse"> </d-cite></d-cite></p> <p>FlashAttention introduced a new way to use the online softmax inside a tiled attention algorithm. This technique allows the softmax statistics to be computed incrementally</p> <h3 id="algebra-of-online-softmax">Algebra of Online Softmax</h3> <p>Let vector $x$ be split into two blocks $x^{(1)}$ and $x^{(2)}$. We compute local statistics for each block. The block size is chosen such that</p> <p>Local max: $m_1 = \max(x^{(1)})$, $m_2 = \max(x^{(2)})$</p> <p>Thus the local unnormalised sum becomes: $l_1 = \sum e^{x^{(1)} - m_1}$, $l_2 = \sum e^{x^{(2)} - m_2}$</p> <p>To combine these, we define the global max $m_{new} = \max(m_1, m_2)$. The global sum $l_{new}$ can be updated without revisiting the raw data of block 1 which is \(l_{new} = e^{m_1 - m_{new}} l_1 + e^{m_2 - m_{new}} l_2\)</p> <p>Let $O_{old}$ be the current accumulated output scaled by the old normalization factor. The correct update is: \(O_{new} = \text{diag}(l_{new})^{-1} \left( \text{diag}(l_{old}) e^{m_{old} - m_{new}} O_{old} + e^{m_{cur} - m_{new}} P_{cur} V_{cur} \right)\)</p> <p>The quadratic complexity of standard self-attention arises from three fundamental operations that each contribute $O(n^2)$ cost:</p> <ol> <li><strong>Score computation:</strong> $O(n^2 d_k)$ - computing all pairwise similarities</li> <li><strong>Softmax normalization:</strong> $O(n^2)$ - normalizing each of $n^2$ scores</li> <li><strong>Value aggregation:</strong> $O(n^2 d_v)$ - aggregating all $n^2$ weighted values</li> </ol> <p>This</p> <h2 id="looking-through-hardware-aware-lens">Looking through Hardware-aware lens</h2> <p>Standard implementations of attention, exacerbate this algorithmic complexity through inefficient utilization of hardware resources. These implementations are typically memory-bound, meaning their execution speed is limited not by the arithmetic throughput of the GPU’s compute units, but by the bandwidth available to move data between the high-capacity High Bandwidth Memory (HBM) and the high-speed on-chip SRAM. This phenomenon, often referred to as the “memory wall,” dictates that as compute capabilities (FLOPs) outpace memory bandwidth improvements, operations with low arithmetic intensity become increasingly expensive <d-cite key="gholami2024ai"> </d-cite></p> <p>The FlashAttention series (v1, v2, and v3) by Tri Dao represents a rigorous systems-level intervention to address this bottleneck. Rather than approximating the attention mechanism—a strategy that often degrades model quality—FlashAttention redefines the computation through the lens of IO-awareness. By meticulously accounting for the asymmetric memory hierarchy of modern GPUs, FlashAttention minimizes data movement, effectively breaking the memory wall for exact attention computation. Through this blog, we provide an exhaustive technical examination of this evolution, tracing the algorithmic tiling of version 1, the parallelism optimizations of version 2, and the hardware-specific asynchronous pipelining of version 3. We also touch upon FlashAttention-4, as gathered from sources, though no official paper has been released yet.</p> <h2 id="appendix-a">Appendix A</h2> <p>The standard self-attention mechanism used in Transformers is formulated as \(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</p> <p>Where:</p> <ul> <li>$Q \in \mathbb{R}^{n \times d_k}$ is the query matrix</li> <li>$K \in \mathbb{R}^{n \times d_k}$ is the key matrix</li> <li>$V \in \mathbb{R}^{n \times d_v}$ is the value matrix</li> <li>$n$ is the sequence length</li> <li>$d_k$ and $d_v$ are the key and value dimensions</li> <li>$\sqrt{d_k}$ is the scaling factor for numerical stability</li> </ul> <p>Computing the score matrix involves:</p> \[S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}\] <p>This matrix multiplication requires computing the dot product between all pairs of query and key vectors. For each of the $n^2$ pairs $(i,j)$, we compute:</p> \[S_{ij} = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}\] <p>Since each dot product involves $d_k$ multiplications and $d_k - 1$ additions, the total computational cost is:<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> \[\text{Time Complexity (Step 1)} = O(n^2 d_k)\] <p><strong>Memory Complexity:</strong> The $n \times n$ score matrix $S$ requires $O(n^2)$ space to store.</p> <p>The softmax function is applied row-wise to normalize the attention scores \(\text{softmax}(S)_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^n \exp(S_{ik})}\)</p> <p>For each row $i \in [n]$, this involves:</p> <ul> <li>Computing $n$ exponentials: $O(n)$ operations</li> <li>Computing the row sum: $O(n)$ operations</li> <li>Normalizing each element: $O(n)$ operations</li> </ul> <p>Since we have $n$ rows, the total cost is:</p> \[\text{Time Complexity (Step 2)} = O(n^2)\] <p>Finally, multiply the normalized attention matrix with the value matrix:</p> \[\text{Output} = \text{softmax}(S) \cdot V\] <p>Where $\text{softmax}(S) \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{n \times d_v}$.</p> <p>This matrix multiplication requires computing: \(\text{Output}_{ij} = \sum_{k=1}^n \text{softmax}(S)_{ik} \cdot V_{kj}\)</p> <p>For each of the $n \times d_v$ elements in the output, we perform $n$ operations, yielding:</p> \[\text{Time Complexity (Step 3)} = O(n^2 d_v)\] <p>Combining all three steps:</p> \[\text{Total Time} = O(n^2 d_k) + O(n^2) + O(n^2 d_v) = O(n^2(d_k + d_v))\] <p>In standard Transformer configurations, the embedding dimension $d_{\text{model}}$ is typically divided equally among multiple attention heads. For a single attention head, $d_k \approx d_v \approx \frac{d_{\text{model}}}{h}$ where $h$ is the number of heads. Therefore, the complexity simplifies to:<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> \[\boxed{\text{Time Complexity} = O(n^2 \cdot d_{\text{model}})}\] <p><strong>Memory Complexity:</strong></p> <p>The dominant memory requirement comes from storing the intermediate attention score matrix</p> \[S \in \mathbb{R}^{n \times n} \quad \Rightarrow \quad \boxed{\text{Memory Complexity} = O(n^2)}\] <p>Additional memory requirements include:</p> <ul> <li>Storing $Q, K, V$ matrices: $O(3 \cdot n \cdot d)$ which is $O(nd)$ overall</li> <li>Temporary buffers for computations: $O(nd)$</li> </ul> <p>However, these are dominated by $O(n^2)$ for large sequence lengths.</p> <p>The quadratic complexity creates severe bottlenecks for processing long sequences:</p> <table> <thead> <tr> <th style="text-align: left">Sequence Length</th> <th style="text-align: left">Relative Cost</th> <th style="text-align: left">Memory (GB at $d=768$)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">512</td> <td style="text-align: left">1×</td> <td style="text-align: left">1.5</td> </tr> <tr> <td style="text-align: left">2,048</td> <td style="text-align: left">16×</td> <td style="text-align: left">24</td> </tr> <tr> <td style="text-align: left">8,192</td> <td style="text-align: left">256×</td> <td style="text-align: left">384</td> </tr> <tr> <td style="text-align: left">65,536</td> <td style="text-align: left">16,384×</td> <td style="text-align: left">$&gt;6$ TB</td> </tr> </tbody> </table> <h3 id="theoretical-lower-bounds">Theoretical Lower Bounds</h3> <p>Research using complexity theory has proven that this quadratic complexity is <strong>fundamental and unavoidable</strong> under the Strong Exponential Time Hypothesis (SETH). The key theorem states:<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup><sup id="fnref:1:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>For the softmax dot-product self-attention with $d_q = \omega(\log n)$, for any $\epsilon &gt; 0$:</p> \[\text{Computing self-attention requires } \Omega(n^{2-\epsilon}) \text{ time}\] <p>This holds even for:</p> <ul> <li><strong>Exact computation</strong> of attention scores</li> <li> <table> <tbody> <tr> <td><strong>Approximate computation</strong> with multiplicative error $\mu$: $</td> <td>\hat{Y}<em>{ij} - Y</em>{ij}</td> <td>\leq \mu</td> <td>Y_{ij}</td> <td>$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td><strong>Approximate computation</strong> with additive error $\mu$: $</td> <td>\hat{Y}<em>{ij} - Y</em>{ij}</td> <td>\leq \mu$</td> </tr> </tbody> </table> </li> </ul> <h3 id="why-self-attention-cannot-escape-quadratic-complexity">Why Self-Attention Cannot Escape Quadratic Complexity</h3> <p>The proof uses reductions from the <strong>Orthogonal Vectors Problem (OVP)</strong>, which is conjectured to require nearly quadratic time. The intuition is:</p> <ol> <li>Computing attention requires evaluating pairwise interactions: $O(n^2)$ pairs</li> <li>Each pair requires a dot product computation: $O(d_k)$ time</li> <li>Even with approximations, one must examine enough pairs to distinguish correct answers from incorrect ones</li> <li>Thus, the quadratic barrier in $n$ is unavoidable<sup id="fnref:3:1"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></li> </ol> <h2 id="flash-attention">Flash Attention</h2> <h2 id="references">References</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>https://arxiv.org/pdf/2209.04881.pdf <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:2"> <p>https://apxml.com/courses/foundations-transformers-architecture/chapter-6-advanced-architectural-variants-analysis/self-attention-complexity <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>https://proceedings.mlr.press/v201/duman-keles23a/duman-keles23a.pdf <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> </ol> </div>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We present a technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with diagrams and Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms.]]></summary></entry><entry><title type="html">Can we really identify LLM Generated Text? The promise and limits of watermarking</title><link href="https://emharsha1812.github.io/blog/2025/llmgenerated/" rel="alternate" type="text/html" title="Can we really identify LLM Generated Text? The promise and limits of watermarking"/><published>2025-11-25T00:00:00+00:00</published><updated>2025-11-25T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/llmgenerated</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/llmgenerated/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The rise of Large Language Models (LLMs) and Multimodal models has changed the nature of digital content, often making the boundary between human and machine authorship increasingly porous. While this capability offers utility, it also introduces risks, such as the spread of misinformation, academic dishonesty, and a general erosion of trust in digital communication <d-cite key="chen2024llmgeneratedmisinformationdetected,singh2025on"></d-cite>.</p> <div class="l-body" style="border-left: 3px solid #ccc; padding-left: 15px; margin: 20px 0;"> For instance, ask yourself this, how will you confidently determine that this blog post you are reading is written by a human or is purely LLM-Generated? </div> <p>The central question is whether the text generated by an LLM (or in case of Multimodal models any content in the form of text, image or audio) can be reliably distinguished from that written by a human. Several papers have come up with watermarking techniques, notably among them appearing in ICLR presentations and conference <d-cite key="liu2024unforgeablepubliclyverifiablewatermark,liu2024semanticinvariantrobustwatermark,gu2024learnabilitywatermarkslanguagemodels,zhao2023provablerobustwatermarkingaigenerated,hu2023unbiasedwatermarklargelanguage,kirchenbauer2024reliabilitywatermarkslargelanguage,tian2024multiscalepositiveunlabeleddetectionaigenerated,arabi2025hiddennoisetwostagerobust,hu2025videoshieldregulatingdiffusionbasedvideo,lu2025robustwatermarkingusinggenerative"></d-cite>.</p> <p>In the sections that follow, we aim to give the readers a robust theoretical as well as practical understanding of watermarking while emphasizing the trade-offs and failure modes of these techniques. We also aim to tie in information-theoretic limits—how much signal one can embed without degrading text—as well as to what extent detection error bounds are acceptable</p> <p>All of the detection can be roughly bifurcated into two distinct methods; Detection Methods which operate post-hoc on finished text, and watermarking which represents a more proactive approach to establishing provenance. Watermarking is a step which aims to embed an imperceptible statistical signal into text during the generation process, thus creating a verifiable link between an output and its source. This signal is not a secret message itself more a detectable pattern that identifies the text as machine generated.</p> <p>The AI Generated writing has two notable characteristics that makes it seem a little too-perfect (and a little less human). These two characteristics are Perplexity and Burstiness.</p> <h2 id="perplexity">Perplexity</h2> <p>Perplexity - In Language modeling, perplexity quantifies a model’s uncertainty or “confusion” when predicting the next token in the sequence. Mathematically, it is the exponential of the average negative log-likelihood per token.</p> \[\text{Perplexity}(x_{1:T}) = \exp\left(-\frac{1}{T} \sum_{t=1}^{T} \log p(x_t \mid x_{&lt;t})\right)\] <p>A lower perplexity scores indicates that the model is more confident in its predictions, as it is effectively choosing from a smaller set of likely next words.</p> <p>The goal of LLM training is to minimize perplexity on a corpus of human text. Thus they tend to sample high-probability tokens and their generated text often has a lower perplexity score when evaluated by a language model than typical human-written text.</p> <div class="l-body" style="border-left: 3px solid #ccc; padding-left: 15px; margin: 20px 0;"> <strong>Note -</strong> Perplexity can be understood more intuitively through its geometric mean formulation. The geometric mean of a set of numbers is the Tth root of their product (where T is the number of values) and perplexity is the geometric mean of the inverse probabilities: $$ \text{Perplexity}(x_{1:T}) = \left(\prod_{t=1}^{T} \frac{1}{p(x_t \mid x_{&lt;t})}\right)^{1/T} $$ </div> <h2 id="burstiness">Burstiness</h2> <p>Burstiness - While Perplexity measures the average predictability of a text, burstiness measures its variance. it is defined as the change in perplexity over the course of a document. Mathematically,</p> \[B = \frac{\lambda - k}{\lambda + k}\] <p>where \(B\) = Burstiness, \(\lambda\) = Mean inter-arrival time between bursts, \(k\) = Mean burst length</p> <p>Human writing is often characterized by a “bursts” of high perplexity, where a writer uses a creative metaphor, a rare word, or an unconventional sentence structure. In contrast, LLM-generated text tends to maintain a more uniform level of perplexity, resulting in low burstiness.</p> <p>Beyond such raw statistics, it can also be observed that machine text exhibits noticeable stylistic patterns. Studies like <d-cite key="doi:10.1177/0261927X231200201"></d-cite> indicate that AI-generated text often adopts a more analytic and formal tone, may use a higher density of adjectives, and can be less readable than human generated text. Human writing on the other hand, has much more variability with more sophisticated discourse patterns, reflecting a more dynamic and uniform authoring process <d-cite key="kim-etal-2024-threads"></d-cite>.</p>]]></content><author><name>Anonymous</name></author><category term="LLM"/><category term="watermarking"/><category term="AI-detection"/><category term="machine-learning"/><summary type="html"><![CDATA[Exploring the theoretical and practical aspects of watermarking techniques for detecting AI-generated content, including trade-offs, failure modes, and information-theoretic limits]]></summary></entry><entry><title type="html">R squared in Machine Learning</title><link href="https://emharsha1812.github.io/blog/2025/rsquared/" rel="alternate" type="text/html" title="R squared in Machine Learning"/><published>2025-09-01T00:12:00+00:00</published><updated>2025-09-01T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/rsquared</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/rsquared/"><![CDATA[<p>The \(R^2\) metric, often called the coefficient of determination, is one of the most widely used measures for evaluating regression models. At its core, \(R^2\) tells us how well the model explains the variability of the dependent variable relative to a very simple baseline: the mean of that variable. To understand it deeply, let us start from the ground up.</p> <p>Suppose we have a dataset with a target variable \(y\), and our regression model produces predictions \(\hat{y}\). If we did not have any model at all, the best we could do to “predict” \(y\) would be to use its mean, \(\bar{y}\), for every data point. This simple strategy captures no nuance of the data, but it provides a baseline for comparison. The total variability in the data, called the total sum of squares (\(SS_{tot}\)), measures how much the actual values deviate from the mean. Mathematically, it is written as</p> \[SS_{tot} = \sum_i (y_i - \bar{y})^2.\] <p>Now, when we bring in a model, it produces predictions \(\hat{y}_i\). Naturally, those predictions won’t always be perfect, and the deviations of the predictions from the true values are called residuals. The sum of squared residuals (\(SS_{res}\)) captures how much unexplained error remains after using the model:</p> \[SS_{res} = \sum_i (y_i - \hat{y}_i)^2.\] <p>The magic of \(R^2\) lies in how it compares these two quantities. Specifically,</p> \[R^2 = 1 - \frac{SS_{res}}{SS_{tot}}.\] <p>If the model’s predictions are perfect, then the residual sum of squares vanishes to zero, giving \(R^2 = 1\). This indicates that the model explains all the variability of the data. On the other hand, if the model is no better than just predicting the mean, then \(SS_{res} = SS_{tot}\), and \(R^2\) becomes zero. Intriguingly, \(R^2\) can even be negative. This happens when the model is worse than the mean, in the sense that the residual errors are larger than the variability of the data itself. In such cases, the model is actively harmful as an explanatory tool.</p> <p>The intuitive interpretation of \(R^2\) is that it represents the fraction of variance in the dependent variable that is explained by the independent variables. If you think of variance as the “spread” or unpredictability in the data, then a model’s job is to account for as much of that spread as possible by relating it to explanatory features. For instance, if \(R^2 = 0.7\), it suggests that 70% of the variance in the outcome can be explained by the model, while the remaining 30% is still noise or unexplained. This interpretation makes \(R^2\) appealing because it ties the effectiveness of the model directly to the concept of variance explanation.</p> <p>However, one must also be cautious. \(R^2\) only measures variance explained relative to the mean model. It does not tell you whether the model is correct in a predictive sense, nor does it penalize overfitting directly. For example, adding more features to a model will never decrease \(R^2\); it can only stay the same or increase, even if the additional features have no real explanatory power. This is why adjusted \(R^2\) is often introduced, which penalizes the inclusion of unnecessary predictors by taking into account the number of features relative to the number of data points.</p> <p>Another subtle point is that \(R^2\) assumes that variance is the right quantity to explain. This makes sense in linear regression, where the goal is indeed to reduce squared error, but in contexts like nonlinear models or when distributions are highly skewed, the variance explanation picture may not fully align with predictive accuracy. For example, a model might achieve a high \(R^2\) but still perform poorly in predicting new data if it overfits. Similarly, in time series where temporal dependence is critical, variance explanation might be misleading without proper validation.</p> <p>So, in essence, \(R^2\) is a measure of how much better your model is compared to a naive mean predictor in terms of explaining variance in the target variable. It provides a normalized sense of fit: \(R^2 = 1\) means perfect explanation, \(R^2 = 0\) means no better than chance, and negative values indicate worse than chance. Thinking of it through the lens of variance explanation grounds it in the idea of “how much of the spread in the data have we accounted for?”—but it is always best interpreted alongside other metrics and validation strategies.</p> <p>Good. Let us now step into the geometric view, because it enriches our intuition for what \(R^2\) truly measures. Regression can be understood not only as a statistical minimization of squared error but also as a geometric projection in a high-dimensional vector space.</p> <p>Imagine your dataset of responses \(y = (y_1, y_2, \dots, y_n)\) as a vector sitting in an \(n\)-dimensional space. Each coordinate represents one observation. When we perform regression with predictors \(X\), what we are really doing is trying to find another vector \(\hat{y}\) that lies in the subspace spanned by the columns of \(X\). In other words, we are projecting the outcome vector \(y\) onto the space formed by linear combinations of the predictors. The projection gives us the fitted values \(\hat{y}\), while the leftover piece—the residuals \(y - \hat{y}\)—is orthogonal to that subspace.</p> <p>Now, why is this picture powerful for understanding \(R^2\)? Because variance explained corresponds to how much of the “length” (technically, squared norm) of \(y\) is captured in the projection. The total variability of \(y\) around its mean can be written as the squared length of the centered vector \(y - \bar{y}\mathbf{1}\), where \(\mathbf{1}\) is the all-ones vector. That is the total sum of squares, \(SS_{tot}\). The part explained by the regression is the squared length of the projection of this centered vector onto the column space of \(X\). That is called the regression sum of squares, \(SS_{reg}\). The residual sum of squares \(SS_{res}\) is simply the squared length of the orthogonal residual vector. By Pythagoras, these satisfy the neat identity</p> \[SS_{tot} = SS_{reg} + SS_{res}.\] <p>And from this decomposition, you see that</p> \[R^2 = \frac{SS_{reg}}{SS_{tot}},\] <p>which is literally “how much of the squared length is explained by the projection.”</p> <p>This geometry also reveals another interpretation: \(R^2\) is the square of the correlation coefficient between \(y\) and \(\hat{y}\). If you think of correlation as measuring alignment between two vectors, then \(R^2\) measures the degree to which the predicted vector lies in the same direction as the true vector. Perfect alignment gives correlation \(1\) and thus \(R^2 = 1\). A poor model, on the other hand, produces predictions that are only weakly aligned with the actual responses, giving a small \(R^2\). Negative \(R^2\) in this picture corresponds to the situation where the projection chosen by the model actually misaligns the predicted vector in such a way that it increases squared error compared to the mean baseline.</p> <p>So geometrically, variance explainability means this: you start with the cloud of data points in high-dimensional space, you draw the straightest line or hyperplane you can through them (given by the regression model), and you measure how much of the original data’s spread is captured along that line. The closer your data vector \(y\) lies to the subspace spanned by your predictors, the more variance you have explained, and the higher your \(R^2\).</p> <p>This perspective unifies the algebraic and statistical definitions. From one side, \(R^2\) is “1 minus unexplained variance over total variance.” From the other, it is the squared cosine of the angle between the true outcomes and the predictions. Both tell the same story: it quantifies alignment, projection, and variance accounted for by the model.</p> <hr/> <p>let’s now move from \(R^2\) to its refined cousin: <strong>adjusted \(R^2\)</strong>. The motivation for this adjustment emerges from a subtle flaw in plain \(R^2\). Remember that \(R^2\) never decreases as you add more predictors to a model. Even if the new variable has no true relationship with the target, the mere act of giving the model more flexibility allows it to fit the data slightly better, thereby reducing the residual sum of squares. This means that \(R^2\) is biased toward models with more features, and left unchecked, it can reward overfitting.</p> <p>Adjusted \(R^2\) was introduced to correct this. Its guiding idea is simple: yes, adding predictors can reduce error, but unless they genuinely improve explanatory power, they should be penalized for consuming degrees of freedom. The formula makes this precise. If you have \(n\) data points and \(p\) predictors, then adjusted \(R^2\) is defined as</p> \[R^2_{adj} = 1 - \frac{SS_{res}/(n-p-1)}{SS_{tot}/(n-1)}.\] <p>Notice the two denominators: instead of just comparing raw sums of squares, we are now comparing <strong>mean squared residuals per degree of freedom</strong>. The denominator \(n-1\) corresponds to the total variability after estimating a single mean, while the numerator \(n-p-1\) corresponds to the leftover variability after fitting \(p\) predictors. In essence, this formula asks: how much better is the model than the mean predictor, once we take into account the “cost” of the parameters used?</p> <p>The behavior of adjusted \(R^2\) is revealing. If a new predictor improves the model enough that the reduction in residual variance outweighs the penalty of losing a degree of freedom, adjusted \(R^2\) will rise. But if the predictor does not help much, the penalty dominates and adjusted \(R^2\) will actually fall. This makes it a more balanced tool for comparing models of different complexity.</p> <p>Another way to view it is through the lens of variance explanation again. While \(R^2\) asks “what fraction of variance do we explain,” adjusted \(R^2\) sharpens the question to “what fraction of variance do we explain per unit of explanatory effort?” It acknowledges that variance can be explained trivially by throwing in more variables, but meaningful explanation comes only when the gain surpasses the cost.</p> <p>It is important, however, to keep adjusted \(R^2\) in perspective. It is not a panacea. It still assumes that linear regression is the right modeling framework and that squared error is the right measure of fit. It is also influenced by sample size: with a small \(n\), the penalty for adding predictors is heavy, while with very large \(n\), adjusted \(R^2\) behaves more like plain \(R^2\). Nonetheless, within the family of linear regression comparisons, it is often the preferred metric because it guards against the seductive but misleading climb of \(R^2\) as more predictors are introduced.</p> <p>So, if you think of \(R^2\) as a measure of variance explanation in absolute terms, adjusted \(R^2\) is the disciplined version that insists on efficiency—explaining variance, yes, but doing so responsibly, without inflating the sense of achievement by smuggling in unnecessary variables.</p> <hr/>]]></content><author><name></name></author><category term="machine-learning"/><category term="machine-learning"/><summary type="html"><![CDATA[Meaning, Explanation & more]]></summary></entry><entry><title type="html">Training a simple bigram character level model on tiny stories</title><link href="https://emharsha1812.github.io/blog/2025/bigram/" rel="alternate" type="text/html" title="Training a simple bigram character level model on tiny stories"/><published>2025-05-24T00:12:00+00:00</published><updated>2025-05-24T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/bigram</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/bigram/"><![CDATA[<h1 id="building-a-bigram-language-model-a-step-by-step-guide-to-character-level-text-generation">Building a Bigram Language Model: A Step-by-Step Guide to Character-Level Text Generation</h1> <p>I wrote this small snippet as part of my learning process from Andrej’s video (link).</p> <h2 id="what-is-a-bigram-language-model">What is a Bigram Language Model?</h2> <p>A bigram language model predicts the next character in a sequence based solely on the current character. It’s called “bigram” because it considers pairs of characters (bi = two).</p> <p>The model learns a probability distribution over all possible next characters given the current character, essentially building a lookup table that says “when I see character X, what’s the most likely next character?”</p> <h2 id="dataset-preparation-and-text-loading">Dataset Preparation and Text Loading</h2> <p>Our journey begins with loading and examining our text data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">stories.text</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span><span class="o">=</span><span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">text</span><span class="p">[:</span><span class="mi">500</span><span class="p">])</span>
</code></pre></div></div> <p>The Tiny Stories dataset contains simple, child-friendly stories that are perfect for training language models. We load the entire text file into memory as a single string. This approach works well for smaller datasets, though larger datasets would require more sophisticated data loading strategies.</p> <h2 id="character-level-tokenization">Character-Level Tokenization</h2> <p>Unlike word-based models, our character-level approach treats each individual character as a token. This has several advantages:</p> <ul> <li><strong>Simplicity</strong>: No need for complex word segmentation</li> <li><strong>Robustness</strong>: Can handle any text, including typos and rare words</li> <li><strong>Fine-grained control</strong>: Learns spelling patterns and character relationships</li> </ul> <p>Let’s build our character vocabulary:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chars</span><span class="o">=</span><span class="nf">sorted</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">text</span><span class="p">)))</span>
<span class="n">vocab_size</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['\n', ' ', '!', '"', '#', '$', '&amp;', "'", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '&lt;', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¦', '©', '\xad', '±', '´', 'Â', 'Ã', 'â', 'ð', 'œ', 'Š', 'Ÿ', 'Ž', '˜', '“', '”', '‹', '€', '™']
101

 !"#$&amp;'()*+,-./0123456789:;&lt;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz¦©­±´ÂÃâðœŠŸŽ˜“”‹€™
</code></pre></div></div> <p>This code extracts all unique characters from our text and sorts them alphabetically. The vocabulary size tells us how many different characters our model needs to handle. Typically, this includes letters (both cases), numbers, punctuation, and whitespace characters.</p> <h2 id="building-the-tokenizer">Building the Tokenizer</h2> <p>Tokenization is the process of converting text into numerical representations that neural networks can process. We create two essential mappings:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stoi</span><span class="o">=</span><span class="p">{</span><span class="n">ch</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>  <span class="c1"># string to integer
</span><span class="n">itos</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>  <span class="c1"># integer to string
</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">])</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">stoi</code> (string-to-integer) dictionary maps each character to a unique integer ID, while <code class="language-plaintext highlighter-rouge">itos</code> (integer-to-string) provides the reverse mapping. Our encoder and decoder functions handle the conversion between text and numerical sequences.</p> <p>Testing our tokenizer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">exa</span><span class="o">=</span><span class="sh">"</span><span class="s">My name is Harshwardhan</span><span class="sh">"</span>
<span class="n">output</span><span class="o">=</span><span class="nf">encoder</span><span class="p">(</span><span class="n">exa</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">decoder</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[42, 80, 1, 69, 56, 68, 60, 1, 64, 74, 1, 37, 56, 73, 74, 63, 78, 56, 73, 59, 63, 56, 69]
My name is Harshwardhan
</code></pre></div></div> <p>This verification step ensures our encoding and decoding process is lossless - we can convert text to numbers and back to the original text perfectly.</p> <h2 id="converting-to-pytorch-tensors">Converting to PyTorch Tensors</h2> <p>Neural networks work with tensors, so we convert our encoded text into a PyTorch tensor:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nf">encoder</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([19212308])
tensor([48, 71, 70, 75, 14,  1, 48, 71, 70, 75,  1, 74, 56, 78,  1, 75, 63, 60,
         1, 74])
</code></pre></div></div> <p>The resulting tensor contains integer indices representing each character in our text. The shape tells us the total length of our dataset, while examining the first 100 elements helps us verify the conversion worked correctly.</p> <h2 id="dataset-splitting">Dataset Splitting</h2> <p>Machine learning requires separate training and validation sets to properly evaluate model performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">train</span><span class="o">=</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
<span class="n">validate</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span>
</code></pre></div></div> <p>We use a 90-10 split, dedicating 90% of our data to training and 10% to validation. The validation set helps us monitor whether our model is learning genuine patterns or simply memorizing the training data (overfitting).</p> <h2 id="understanding-context-windows">Understanding Context Windows</h2> <p>Language models don’t process entire texts at once. Instead, they work with fixed-size context windows.</p> <p>To give a context of what I am trying to say, here’s a snippet you can run to get an idea</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">block_size</span><span class="o">=</span><span class="mi">8</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">train</span><span class="p">[:</span><span class="n">block_size</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">block_size</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">when input is </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s"> the target: </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>when input is tensor([48]) the target: 71
when input is tensor([48, 71]) the target: 70
when input is tensor([48, 71, 70]) the target: 75
when input is tensor([48, 71, 70, 75]) the target: 14
when input is tensor([48, 71, 70, 75, 14]) the target: 1
when input is tensor([48, 71, 70, 75, 14,  1]) the target: 48
when input is tensor([48, 71, 70, 75, 14,  1, 48]) the target: 71
when input is tensor([48, 71, 70, 75, 14,  1, 48, 71]) the target: 70
</code></pre></div></div> <p>This code demonstrates a crucial concept: from a single sequence of length 8, we can create 8 different training examples. Each example uses a progressively longer context to predict the next character:</p> <ul> <li>Given just the first character, predict the second</li> <li>Given the first two characters, predict the third</li> <li>And so on…</li> </ul> <p>This approach maximizes the learning opportunities from our data and teaches the model to work with contexts of varying lengths.</p> <h2 id="batch-processing-for-efficient-training">Batch Processing for Efficient Training</h2> <p>Neural networks train more efficiently when processing multiple examples simultaneously. Our batch generation function creates random samples:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span>
<span class="n">block_size</span><span class="o">=</span><span class="mi">8</span>

<span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
    <span class="n">data</span><span class="o">=</span><span class="n">train</span> <span class="k">if</span> <span class="n">split</span><span class="o">==</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span> <span class="k">else</span> <span class="n">validate</span>
    <span class="n">ix</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="n">block_size</span><span class="p">,(</span><span class="n">batch_size</span><span class="p">,))</span>
    <span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span>
</code></pre></div></div> <p>This function randomly selects starting positions in our dataset and extracts sequences of length <code class="language-plaintext highlighter-rouge">block_size</code>. The result is two tensors:</p> <ul> <li><code class="language-plaintext highlighter-rouge">x</code>: Input sequences (what the model sees)</li> <li><code class="language-plaintext highlighter-rouge">y</code>: Target sequences (what the model should predict)</li> </ul> <p>The random sampling ensures our model sees different parts of the text in each batch, promoting better generalization.</p> <h2 id="the-bigram-language-model-architecture">The Bigram Language Model Architecture</h2> <p>Now we build our neural network. Despite its simplicity, this model embodies key language modeling concepts:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</code></pre></div></div> <p>The core of our model is an embedding table - essentially a learned lookup table where each character is associated with a vector of probabilities for the next character. The embedding dimension equals our vocabulary size, creating a direct mapping from current character to next character probabilities.</p> <h2 id="forward-pass-and-loss-calculation">Forward Pass and Loss Calculation</h2> <p>The forward pass transforms input sequences into predictions and calculates the training loss:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1"># (B,T,C)
</span>    
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <p>The embedding table produces “logits” - raw prediction scores for each possible next character. When we have targets (during training), we calculate cross-entropy loss, which measures how well our predictions match the actual next characters.</p> <p>The reshaping operations (<code class="language-plaintext highlighter-rouge">view</code>) are necessary because PyTorch’s cross-entropy function expects 2D inputs, but our model produces 3D tensors (batch, time, characters).</p> <h2 id="text-generation">Text Generation</h2> <p>The generation function is where our trained model becomes useful:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># focus on last time step
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># convert to probabilities
</span>        <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># sample
</span>        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># append
</span>    <span class="k">return</span> <span class="n">idx</span>
</code></pre></div></div> <p>This function implements autoregressive generation:</p> <ol> <li>Get predictions for the current sequence</li> <li>Focus only on the last position (most recent character)</li> <li>Convert logits to probabilities using softmax</li> <li>Sample a character based on these probabilities</li> <li>Add the sampled character to our sequence</li> <li>Repeat</li> </ol> <p>The sampling step is crucial - rather than always picking the most likely character (which would be deterministic and repetitive), we sample according to the probability distribution, introducing controlled randomness that makes the generated text more interesting and varied.</p> <h2 id="training-loop">Training Loop</h2> <p>Training a neural network involves repeatedly showing it examples and adjusting its parameters to reduce prediction errors:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="k">for</span> <span class="n">steps</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">(</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">m</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p>Each training step follows a standard pattern:</p> <ol> <li><strong>Forward pass</strong>: Feed data through the model to get predictions</li> <li><strong>Loss calculation</strong>: Compare predictions to actual targets</li> <li><strong>Backward pass</strong>: Calculate gradients showing how to improve</li> <li><strong>Parameter update</strong>: Adjust model weights to reduce loss</li> </ol> <p>We use the AdamW optimizer, which adapts the learning rate for each parameter individually, leading to more stable and efficient training than basic gradient descent.</p> <h2 id="monitoring-progress">Monitoring Progress</h2> <p>Before training, our model generates mostly gibberish:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">decoder</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sha inth ge jonin out, peroamy aveppedan s lld het
</code></pre></div></div> <p>After 10,000 training steps, the same generation call produces much more coherent text. The loss value also decreases significantly, indicating that our model is learning the character patterns in our dataset.</p> <h2 id="key-insights-and-limitations">Key Insights and Limitations</h2> <p>Our bigram model, while simple, demonstrates several important concepts:</p> <p><strong>Strengths:</strong></p> <ul> <li><strong>Simplicity</strong>: Easy to understand and implement</li> <li><strong>Speed</strong>: Fast training and inference</li> <li><strong>Foundational</strong>: Introduces core language modeling concepts</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li><strong>Limited context</strong>: Only considers the immediately previous character</li> <li><strong>No long-range dependencies</strong>: Cannot capture relationships between distant characters</li> <li><strong>Basic patterns</strong>: Learns simple character transitions but misses complex linguistic structures</li> </ul> <p>Here is the link to the Google Colab Notebook -</p>]]></content><author><name></name></author><category term="coding,"/><category term="python"/><category term="coding,"/><category term="python"/><summary type="html"><![CDATA[Training a simple bigram character level model on tiny stories]]></summary></entry><entry><title type="html">Machine Learning and AI Resources</title><link href="https://emharsha1812.github.io/blog/2025/nptel-ml/" rel="alternate" type="text/html" title="Machine Learning and AI Resources"/><published>2025-02-13T00:00:00+00:00</published><updated>2025-02-13T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/nptel-ml</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/nptel-ml/"><![CDATA[<blockquote> <p>“The goal is to turn data into information, and information into insight.”<br/> ~ Carly Fiorina</p> </blockquote> <p>Machine learning and artificial intelligence have revolutionized the way we approach problem-solving in many fields, from healthcare to robotics to natural language processing. If you’re looking to deepen your understanding of these technologies, here are some of the best online resources and courses available.</p> <h2 id="traditional-machine-learning">Traditional Machine Learning</h2> <ol> <li><a href="https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f/">Micro, Macro &amp; Weighted Averages of F1 Score, Clearly Explained</a></li> </ol> <h2 id="computer-vision">Computer Vision</h2> <ol> <li><a href="https://medium.com/@RobuRishabh/convolution-in-cnns-65e1655b5901">Convolution in CNNs</a></li> </ol> <h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2> <ol> <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs - Chris Olah</a></li> <li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable effectiveness of Recurrent Neural Networks</a></li> <li><a href="https://mmuratarat.github.io/2019-02-07/bptt-of-rnn">Backpropogation through time - Mathematical Derivation</a></li> <li><a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html">Why LSTMs Stop Your Gradients From Vanishing: A View from the Backwards Pass</a></li> </ol> <h2 id="learning-methods">Learning Methods</h2> <ol> <li><a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">Contrastive Representation Learning</a></li> </ol> <h2 id="famous-papers">Famous Papers</h2> <ol> <li><a href="https://medium.com/one-minute-machine-learning/clip-paper-explained-easily-in-3-levels-of-detail-61959814ad13">OpenAI’s CLIP Paper - Explanation</a></li> </ol> <h3 id="machine-learning--deep-learning-courses">Machine Learning &amp; Deep Learning Courses</h3> <ol> <li> <p><strong><a href="https://nptel.ac.in/courses/106106213">Practical Machine Learning with TensorFlow</a></strong><br/> Learn to build machine learning models using TensorFlow.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111105489">Mathematics for Machine Learning</a></strong><br/> A deep dive into the mathematical concepts that underpin machine learning algorithms.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111108066">Advanced Matrix Theory and Linear Algebra for Engineers</a></strong><br/> Understand matrix theory and linear algebra with an emphasis on engineering applications.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111108157">Matrix Theory</a></strong><br/> Learn the foundations of matrix theory, crucial for deep learning and machine learning algorithms.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111107137">Essential Mathematics for Machine Learning</a></strong><br/> Build a strong mathematical foundation for machine learning and AI.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/108103192">Machine Learning and Deep Learning Fundamentals</a></strong><br/> This course provides a comprehensive introduction to machine learning and deep learning concepts.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106139">Machine Learning</a></strong><br/> A fundamental course to kickstart your journey in machine learning.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106198">Machine Learning for Engineering and Science Applications</a></strong><br/> Learn how machine learning is applied in engineering and scientific research.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/108103192">Machine Learning And Deep Learning – Fundamentals and Applications</a></strong><br/> A blend of theory and practical applications in machine learning and deep learning.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106184">Deep Learning - Part 1</a></strong><br/> Introduction to deep learning fundamentals, including neural networks and optimization techniques.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106201">Deep Learning - Part 2</a></strong><br/> Dive deeper into advanced deep learning concepts, architectures, and frameworks.</p> </li> </ol> <h3 id="natural-language-processing-nlp">Natural Language Processing (NLP)</h3> <ol> <li> <p><strong><a href="https://nptel.ac.in/courses/106105158">Natural Language Processing</a></strong><br/> Learn the fundamentals of NLP, including text processing and feature extraction.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106101007">Natural Language Processing</a></strong><br/> A comprehensive NLP course exploring algorithms and applications.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106211">Applied Natural Language Processing</a></strong><br/> Learn how to apply NLP techniques in real-world projects.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106224">Deep Learning for Computer Vision</a></strong><br/> Explore how deep learning models are applied to computer vision problems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/108105103">Deep Learning for Visual Computing</a></strong><br/> Understand the intersection of deep learning and visual computing.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106102576">Introduction to Large Language Models - Tanmoy Chakraborty</a></strong><br/> A course dedicated to large language models and their applications in NLP.</p> </li> <li> <p><strong><a href="https://www.youtube.com/playlist?list=PLZ2ps__7DhBbaMNZoyW2Hizl8DG6ikkjo">Introduction to Large Language Models - Mitesh Khapra</a></strong><br/> Learn about large language models from an industry expert.</p> </li> </ol> <h3 id="reinforcement-learning--ai">Reinforcement Learning &amp; AI</h3> <ol> <li> <p><strong><a href="https://nptel.ac.in/courses/106101466">Distributed Optimization and Machine Learning</a></strong><br/> Explore the optimization techniques used in distributed machine learning systems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/110101145">Bandit Algorithm</a></strong><br/> Learn the fundamentals of multi-armed bandit algorithms, useful in reinforcement learning.</p> </li> <li> <p><strong><a href="https://www.youtube.com/playlist?list=PLL1s8qiaGy0LwIajdxKZr_FRL7KZeQK9r">Deep Generative Models</a></strong><br/> Delve into the theory and applications of generative models like GANs and VAEs.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106143">Reinforcement Learning</a></strong><br/> An introduction to reinforcement learning, where agents learn by interacting with the environment.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106140">Artificial Intelligence: Knowledge Representation and Reasoning</a></strong><br/> Learn how knowledge can be represented and reasoned within AI systems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106226">Artificial Intelligence Search Methods For Problem Solving</a></strong><br/> Study search algorithms, essential for AI problem-solving.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106238">Applied Accelerated Artificial Intelligence</a></strong><br/> Learn how to speed up and apply AI techniques in various industries.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106105077">Artificial Intelligence</a></strong><br/> A comprehensive introduction to the field of artificial intelligence.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106105078">Artificial Intelligence</a></strong><br/> Learn AI concepts and techniques applicable in real-world problems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/117108048">Pattern Recognition</a></strong><br/> Understand pattern recognition and its applications across diverse fields.</p> </li> </ol> <hr/> <p>Credits - <a href="https://www.linkedin.com/in/bastyajayshenoy/">Ajay Shenoy</a></p> <h2 id="large-language-models">Large Language Models</h2> <h3 id="llm-reasoning-papers">LLM Reasoning Papers</h3> <ul> <li><strong>LM Post-Training: A Deep Dive into Reasoning</strong> <ul> <li><a href="https://arxiv.org/pdf/2502.21321">https://arxiv.org/pdf/2502.21321</a></li> </ul> </li> <li><strong>A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1</strong> <ul> <li><a href="https://arxiv.org/html/2502.10867v1">https://arxiv.org/html/2502.10867v1</a></li> </ul> </li> <li><strong>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</strong> <ul> <li><a href="https://arxiv.org/html/2502.14768v1">https://arxiv.org/html/2502.14768v1</a></li> </ul> </li> </ul> <h3 id="llm-research-blogs">LLM Research Blogs</h3> <ul> <li><strong>LLM Research Newsletter</strong> <ul> <li><a href="https://www.llmsresearch.com/">https://www.llmsresearch.com/</a></li> </ul> </li> <li><strong>Transformer Circuit thread</strong> <ul> <li><a href="https://transformer-circuits.pub/">https://transformer-circuits.pub/</a></li> </ul> </li> </ul> <h3 id="internals">Internals</h3> <ul> <li><strong>Pytorch Internals</strong> <ul> <li><a href="https://blog.ezyang.com/2019/05/pytorch-internals/">https://blog.ezyang.com/2019/05/pytorch-internals/</a></li> </ul> </li> <li><strong>Transformer Internals</strong> <ul> <li><a href="https://goyalpramod.github.io/blogs/Transformers_laid_out/#coding-the-transformer/">https://goyalpramod.github.io/blogs/Transformers_laid_out/#coding-the-transformer</a></li> </ul> </li> </ul> <h3 id="iclr-blog-posts">ICLR BLog Posts</h3> <ul> <li><strong>A New Alchemy: Language Model Development as a Subfield?</strong> <ul> <li><a href="https://iclr-blogposts.github.io/2024/blog/language-model-development-as-a-new-subfield/">https://iclr-blogposts.github.io/2024/blog/language-model-development-as-a-new-subfield/</a></li> </ul> </li> <li><strong>Fairness in AI: two philosophies or just one?</strong> <ul> <li><a href="https://iclr-blogposts.github.io/2024/blog/fairness-ai-two-phil-or-just-one/">https://iclr-blogposts.github.io/2024/blog/fairness-ai-two-phil-or-just-one/</a></li> </ul> </li> </ul> <h2 id="generative-ai">Generative AI</h2> <ol> <li> <p><a href="https://huyenchip.com/2025/01/16/ai-engineering-pitfalls.html"><em>Common pitfalls when building generative AI applications</em></a> by Chip Huyen</p> </li> <li> <p><a href="https://blog.ml.cmu.edu/#"><em>ML CMU Blog</em></a></p> </li> </ol> <h2 id="mechanistic-interpretibility-mi">Mechanistic Interpretibility (MI)</h2> <p>Mechanistic interpretability aims to reverse-engineer a neural network into human-understandable mechanisms. MI focuses on transformers (specifically LLMs) but is not limited to these neural network architectures</p> <h3 id="people">People</h3> <ol> <li><a href="https://www.neelnanda.io/mechanistic-interpretability">Neel Nanda</a></li> <li><a href="https://www.alignmentforum.org/">Alignment Forum</a></li> <li></li> </ol> <h3 id="primer-on-llms">Primer on LLMs</h3> <ol> <li><a href="https://www.understandingai.org/p/large-language-models-explained-with">Large language models, explained with a minimum of math and jargon</a></li> <li></li> </ol> <h3 id="transformers">Transformers</h3> <ol> <li><a href="https://arena-chapter1-transformer-interp.streamlit.app/">Transformers Interpretibility</a></li> <li><a href="https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability">200 Concrete Open Problems in MI</a></li> <li></li> </ol> <h3 id="quick-guides-to-mi">Quick Guides to MI</h3> <ol> <li><a href="https://mindfulmodeler.substack.com/p/what-is-mechanistic-interpretability">What is Mechanistic Interpretability and where did it come from?</a></li> <li><a href="https://bluedot.org/blog/introduction-to-mechanistic-interpretability">Introduction to Mechanistic Interpretability</a></li> <li><a href="https://seantrott.substack.com/p/mechanistic-interpretability-for">“Mechanistic interpretability” for LLMs, explained</a></li> </ol> <h4 id="how-to-get-started-with-mi-">How to get started with MI ?</h4> <ol> <li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Concrete Steps to Get Started in Transformer Mechanistic Interpretability</a></li> </ol> <h3 id="relevant-papers">Relevant Papers</h3> <ol> <li><a href="https://arxiv.org/abs/2310.14491">Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models</a></li> <li><a href="https://arxiv.org/html/2407.02646v1">A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models</a></li> <li><a href="https://arxiv.org/pdf/2404.14082">Mechanistic Interpretability for AI Safety : A Review</a></li> </ol> <h3 id="straight-from-anthropic">Straight from Anthropic</h3> <ol> <li><a href="https://www.anthropic.com/research/mapping-mind-language-model">Mapping the mind of a Large Language model</a></li> <li><a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html">Interpretibility Dreams</a></li> <li><a href="https://www.anthropic.com/news/golden-gate-claude">Golden Gate Claude</a></li> <li><a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</a></li> <li><a href="https://transformer-circuits.pub/">Transformer Circuits Thread</a></li> </ol> <h3 id="blogs">Blogs</h3> <ol> <li><a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability">Neel Nanda’s case on why we need interpretibility research</a></li> <li><a href="https://hkamath.me/blog/2024/rqae/">A Microscope into the Dark Matter of Interpretability</a></li> </ol> <h3 id="libraries">Libraries</h3> <ol> <li><a href="https://transformerlensorg.github.io/TransformerLens/content/getting_started_mech_interp.html">Transfomer Lens</a></li> <li><a href="https://www.neuronpedia.org/">Neuronpedia</a></li> <li><a href="https://nnsight.net/">Interpretable Neural Networks</a></li> </ol> <h2 id="why-we-need-mi-research-">Why we need MI Research ?</h2> <p>Neel Nanda makes a couple of strong arguments <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability">here</a> (15 in fact!) on why interpretibility research is needed and how it will help us resolve x-issues</p>]]></content><author><name></name></author><category term="machine-learning,"/><category term="ai,"/><category term="deep-learning,"/><category term="nlp"/><category term="machine-learning,"/><category term="deep-learning,"/><category term="ai,"/><category term="nlp"/><summary type="html"><![CDATA[A collection of links to essential courses on machine learning, deep learning, natural language processing, and artificial intelligence.]]></summary></entry><entry><title type="html">Python Notes</title><link href="https://emharsha1812.github.io/blog/2025/python-notes/" rel="alternate" type="text/html" title="Python Notes"/><published>2025-01-22T00:12:00+00:00</published><updated>2025-01-22T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/python-notes</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/python-notes/"><![CDATA[<p>Hello everyone,<br/> Below, I am sharing the GitHub repository containing all my Python notebooks, which I created while teaching an AI-ML Ops training program to esteemed DRDO scientists. You can access the GitHub repository here: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">GitHub Link</a>.</p> <p>This repository is a work in progress, and I will continue to update it as I create new notebooks. Here is the current plan for upcoming content:</p> <ol class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Python One-Liners Notebook</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Walrus Operator in Python</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Building a simple project using Python</li> </ol> <p>Feel free to explore and stay tuned for updates!</p> <p>Here’s the direct GitHub link for quick access: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">https://github.com/emharsha1812/Python_Programming_Notebooks</a></p> <p>Don’t forget to ⭐ star the repository to stay updated with new additions!</p> <h3 id="ps---here-is-a-list-of-helpful-links-that-you-can-reference-from-time-to-time">PS - Here is a list of helpful links that you can reference from time to time</h3> <ol> <li> <p><a href="https://realpython.com/python-iterators-iterables/">Python Iterators</a> - A super friendly blog on Python Iterators. I recommend taking this <a href="https://realpython.com/quizzes/python-iterators-iterables/">quiz</a> as well after reading the whole blog.</p> </li> <li> <p><a href="https://realpython.com/python-memory-management/">Memory Management in Python</a> - One of those “You should definitely know this” topics.</p> </li> </ol>]]></content><author><name></name></author><category term="coding,"/><category term="python"/><category term="coding,"/><category term="python"/><summary type="html"><![CDATA[A collection of Python notebooks for quick reference]]></summary></entry><entry><title type="html">KAN (Kolmogorov-Arnold Networks)</title><link href="https://emharsha1812.github.io/blog/2025/kan/" rel="alternate" type="text/html" title="KAN (Kolmogorov-Arnold Networks)"/><published>2025-01-07T00:12:00+00:00</published><updated>2025-01-07T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/kan</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/kan/"><![CDATA[<h2 id="1-mathematical-foundations--architecture">1. Mathematical Foundations &amp; Architecture</h2> <p><strong>Kolmogorov-Arnold Representation Theorem:</strong> KANs are founded on a classic result by A. N. Kolmogorov and V. Arnold, which states that <em>any continuous multivariate function can be represented as a finite superposition of univariate functions</em>(<a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov%E2%80%99s%20theorem%2C%20proposed%20in%201957%2C,The%20CFL%20condition%2C%20introduced%20by">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a>). In practical terms, this theorem guarantees that for a function \(f(x_1,\dots,x_n)\), there exist some continuous 1D functions \(\{\Phi_q\}\) and \(\{\Psi_{q,p}\}\) such that:</p> \[f(x_1,\ldots,x_n) \;=\; \sum_{q=0}^{2n} \; \Phi_q\!\Big( \sum_{p=1}^n \Psi_{q,p}(x_p) \Big)\,,\] <p>i.e. \(f\) can be decomposed into <strong>inner</strong> univariate functions \(\Psi_{q,p}\) (each depending on a single input variable $x_p$) and <strong>outer</strong> univariate functions \(\Phi_q\) aggregated by addition. This theorem provides a constructive blueprint for function approximation using single-variable building blocks, which is the key inspiration for KANs</p> <p><strong>KAN Architecture</strong> - Instead of the traditional neuron model with linear weighted sums and fixed activations, a KAN implements the above idea by making <strong>each edge</strong> of the network carry a <em>learnable univariate function</em>. In other words, every connection between neurons is parameterized as a nonlinear function (originally chosen as a B-spline) rather than a scalar weight. Each neuron simply sums up the outputs of the incoming edge-functions. Formally, if \(z_i^{(l)}\) denotes the \(i\)-th activation in layer \(l\), then a <strong>KAN layer</strong> computes each output neuron \(j\) as: (<a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview</a>) \(z_j^{(l+1)} \;=\; \sum_{i=1}^{N_l} f_{ij}^{(l)}\!\Big(z_i^{(l)}\Big)\,,\)</p> <p>where \(f_{ij}^{(l)}: \mathbb{R}\to\mathbb{R}\) is a learnable univariate function on the edge from neuron \(i\) (layer \(l\)) to neuron \(j\) (layer \(l+1\)). There are no separate linear weight matrices; the nonlinearity of \(f_{ij}\) itself provides the transformation. In the <em>shallowest</em> case (two-layer KAN), this architecture directly mirrors Kolmogorov’s decomposition: the first layer learns inner functions \(h_{p}(x_p)\) on each input dimension, and the second layer learns outer functions \(g_q(\cdot)\) that combine those results <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Quanta Magazine</a>.</p> <p><strong>Parameterized Functions (B-Splines):</strong> In practice, each learnable edge-function \(f_{ij}\) is parameterized as a spline (often a B-spline) with a set of control points that can be tuned during training. B-splines are piecewise polynomial curves defined by control points, offering a flexible yet smooth basis for approximating arbitrary 1D functions. By adjusting the control points, the shape of the spline changes locally without affecting the entire function. This choice ensures the learned activation functions are <em>smooth</em> addressing potential non-smoothness in Kolmogorov’s original construction and stable to train. Each edge thus has multiple parameters (the spline control values) instead of a single weight. For example, a KAN might initialize each \(f_{ij}\) as a near-linear spline and then let training mold each into the required nonlinear shape. This edge-centric design lets KANs <em>dynamically adapt their activation functions</em> to the data, rather than relying on a fixed function like ReLU or tanh.</p> <p><strong>Illustrative Pseudocode:</strong> The following pseudocode contrasts a single layer of an MLP vs. a KAN:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanmlp.svg-480.webp 480w,/assets/img/kanmlp.svg-800.webp 800w,/assets/img/kanmlp.svg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanmlp.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Comparison of MLP Layer with KAN Layer in Pytorch </div> <p>In the KAN layer, <code class="language-plaintext highlighter-rouge">f_ij</code> is a learned function (e.g. a spline) specific to edge \((i\to j)\), replacing both the weight and the neuron’s activation for that connection. The neuron simply aggregates these contributions (here via summation). Deep KANs can be built by stacking such layers, allowing composition of these univariate transformations across multiple levels.</p> <h2 id="2-comparison-with-mlps">2. Comparison with MLPs</h2> <p><strong>Structural Differences:</strong> Traditional Multi-Layer Perceptrons (MLPs) use <em>linear weights</em> and <em>fixed activation functions at neurons</em>, whereas KANs use <em>no linear weights at all</em> – every “weight” is replaced by a flexible function on the input signal. In effect, MLPs learn parameters for <strong>nodes</strong> (the weight matrix between layers is trained, then a fixed nonlinearity like ReLU is applied), while KANs learn parameters for <strong>edges</strong> (each connection has a trainable nonlinear mapping). This leads to a duality: <em>MLP = fixed nonlinearity + learned linear weights; KAN = fixed linear sum + learned nonlinear functions</em>. The figure below (from Liu et al. 2024) illustrates this difference, highlighting that MLPs apply activations at neurons (circles) whereas KANs apply learned functions on each connecting edge before summing.(<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanvsmlp-480.webp 480w,/assets/img/kanvsmlp-800.webp 800w,/assets/img/kanvsmlp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanvsmlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Source: <a href="https://arxiv.org/pdf/2404.19756" target="_blank">Liu et al. (2024)</a> </div> <p><strong>Learnable Functions vs Fixed Weights:</strong> In an MLP, the transformation from layer to layer is \(\sigma(Wx + b)\), with \(\sigma\) (e.g. ReLU) fixed and \(W,b\) learned. In a KAN, the transformation is \(\sum_i f_i(x_i)\) (plus bias if needed), with each $f_i$ being learned and no separate \(W\). Essentially, KANs “allocate” more flexibility per connection, whereas MLPs rely on combining many fixed nonlinear units to build complexity. This means KANs move the bulk of learnable parameters into the activation functions themselves, often resulting in <em>fewer total connections</em> needed than an equivalent MLP (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a>).</p> <p><strong>Expressive Power (Universal Approximation):</strong> Both MLPs and KANs are universal function approximators, but via different theorems. MLPs leverage the Universal Approximation Theorem (with enough neurons, an MLP can approximate any continuous function on a domain), while KANs directly leverage the Kolmogorov-Arnold (K-A) theorem to construct such approximations. In theory, a single hidden-layer KAN with sufficiently complex edge functions can exactly represent any continuous function (the K-A theorem provides an existence proof), whereas an MLP might require many more neurons or layers to approximate the same function with fixed activations. KANs thus excel at modeling functions with complex or “spiky” behavior in each input dimension, because each edge can carve out a detailed univariate relationship. In practice, KANs implement the K-A decomposition <em>explicitly</em>, using B-spline basis functions to approximate the required univariate mappings. This can translate to <em>greater expressivity per parameter</em>.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>).</p> <p><strong>Parameter Efficiency &amp; Neural Scaling:</strong> A striking reported advantage is that <em>much smaller KANs can achieve accuracy comparable or superior to much larger MLPs</em> on certain tasks. Each KAN edge function (with, say, $k$ control points) can encode a nonlinear relation that an MLP might need multiple neurons and layers to capture. Empirically, Liu <em>et al.</em> (2024) found KANs follow faster <strong>neural scaling laws</strong> – the error decreases more rapidly as model size increases, compared to MLPs. In other words, to reach a given accuracy, a KAN required fewer trainable parameters than an MLP in their tests. The flexibility of splines allows KANs to fit complex patterns without blowing up the network width/depth. One study noted that KANs can <em>match</em> MLP performance at equal parameter counts, and sometimes exceed it, though they require careful tuning (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=KANs%20definitely%20don%27t%20feel%20like,compared%20to%20regular%20neural%20networks">Trying Kolmogorov-Arnold Networks in Practice</a>). The original KAN paper demonstrated that a KAN with significantly fewer nodes could outperform a dense ReLU network on function-fitting benchmarks.</p> <p><strong>Continuous Learning and Locality:</strong> Because each KAN weight is a localized function (with local control points), learning in a KAN can be more localized. This has implications for <strong>continual learning</strong>. In standard nets, fine-tuning on new data often alters weights globally and can erode old capabilities (catastrophic forgetting). In KANs, adding new data primarily adjusts the spline control points <em>in relevant regions of the input space</em>, leaving other regions (and other functions) mostly unchanged. For example, if a KAN-based language model learns a new vocabulary or coding style, only certain edge-functions for those inputs might reshape, while others retain their previously learned shape. This property means KANs can integrate new knowledge without overwriting all weights, potentially enabling more <strong>seamless continual learning</strong>. MLPs, by contrast, have distributed representations where a single weight doesn’t correspond to an isolated input relationship, making targeted updates harder.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,local%20control%20point%20parameters%20change">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>)</p> <p><strong>Interpretability:</strong> A major motivation for KANs is interpretability. In an MLP, each weight by itself is usually not meaningful, and neurons combine many weights making interpretation difficult. In a KAN, each edge’s function $f_{ij}(x)$ can be visualized as a curve, directly showing how the input from neuron $i$ influences neuron $j$ across the range of values. After training, one can <em>extract these learned univariate functions</em> and inspect them.They might correspond to intuitive relations (e.g. an edge function might learn a sinusoidal shape if the output depends sinusoidally on an input).This transparency is especially useful in scientific or engineering tasks where understanding the learned model is as important as its accuracy. MLPs lack this fine-grained interpretability, since their learned mapping is entangled across many parameters. Thus, KANs offer a more human-understandable model: as the saying goes, they turn the <strong>“black box”</strong> into a collection of readable 1D transformations.</p> <p><strong>Summary:</strong> KANs and MLPs both approximate complex functions, but KANs do so by <em>baking learnable math into the connections</em>. This difference yields advantages in function approximation fidelity, parameter efficiency, and interpretability. However, it also comes with computational challenges (will uupdate later). In essence, KANs can be seen as a <strong>new paradigm</strong>: they trade the simple, generic structure of MLPs for a structure with built-in mathematical richness (the Kolmogorov-Arnold basis). This seemingly small change – moving from scalar weights to learned functions – has profound implications on how the network learns and what it can represent (<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>).</p> <p>The original paper can be found <a href="https://arxiv.org/pdf/2404.19756">here</a></p> <p>Last Updated - 25/02/2025</p> <h3 id="references">References</h3> <p>[1] - <a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov,the%20model%E2%80%99s%20flexibility%20and%20interpretability">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a></p> <p>[2] - <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Novel Architecture Makes Neural Networks More Understandable</a></p> <p>[3] - <a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview on KAN: Kolmogorov–Arnold Networks</a></p> <p>[4] - <a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a></p> <p>[5] - <a href="https://www.datacamp.com/tutorial/kolmogorov-arnold-networks#:~:text=simpler%2C%20univariate%20ones,edges%20are%20used%20for%20approximation">Kolmogorov-Arnold Networks (KANs): A Guide With Implementation</a></p> <p>[6] - <a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a></p> <p>[7] - <a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a></p> ]]></content><author><name></name></author><category term="llm,machine-learning,python"/><category term="kan,deep-learning,research"/><summary type="html"><![CDATA[An Alternative to traditional MLPs]]></summary></entry><entry><title type="html">A Visit to Hungarian Mathematics</title><link href="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/" rel="alternate" type="text/html" title="A Visit to Hungarian Mathematics"/><published>2024-09-01T00:12:00+00:00</published><updated>2024-09-01T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/hungarian-mathematics</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/"><![CDATA[<p>Because my domain is machine learning (ML) and artificial intelligence (AI), it’s quite common for me to come across abstract mathematical equations that initially made no sense to me. However, as I dove deep into those arcane-looking symbols, I discovered another interesting thing aside from the meaning: I realized that most of the founders of these equations were from Hungary!</p> <p>I googled, and then, a few clicks later, I stumbled on this very interesting read titled <strong>‘A Visit to Hungarian Mathematics’</strong>. Here’s the <a href="https://gwern.net/doc/math/1993-hersh.pdf">link</a> 🔗 to this pape . It contained exactly what I was looking for; a detailed introspection into Hungarian mathematicss and the mathematicians surrounding them. While reading the paper, I found a very simple but profoundly impactful quote that I would like to share. It says</p> <blockquote> <p>Everyone has ideas, both right ideas and wrong ideas. Scientific work consists merely of seperating them</p> </blockquote> <p>I immediately started voraciously reading the paper from top to bottom, left to right, and backwards too. Even though I am not a mathematician and do not have a mathematics degree (although I have a fairly good amount of mathematical exposure), I love to explore mathematics as a hobby. I sincerely hope that my interest in mathematics is still relevant.</p> <p>One of the key takeaways from this paper will be a quote by Alfred Renyi, a Hungarian mathematician known for his work in probability theory, combinatorics, and other fields. Once, a gifted mathematician told him that his working ability was heavily dependent on external circumstances. Renyi responded,</p> <blockquote> <p><strong>“When I’m unhappy, I use math to find happiness; when I’m content, I use math to maintain my happiness.”</strong></p> </blockquote> <p>Reading about these mathematicians and their passion for pondering, fighting, and finally solving math problems fills me with a deep sense of gratitude towards math.</p>]]></content><author><name></name></author><category term="mathematics,culture"/><category term="mathematics,"/><category term="axioms"/><summary type="html"><![CDATA[Why Hungarians are so darn good at mathematics ?]]></summary></entry><entry><title type="html">Welcome!</title><link href="https://emharsha1812.github.io/blog/2024/welcome/" rel="alternate" type="text/html" title="Welcome!"/><published>2024-08-27T00:00:00+00:00</published><updated>2024-08-27T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/welcome</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/welcome/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.]]></summary></entry></feed>