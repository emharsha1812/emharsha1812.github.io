<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://emharsha1812.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emharsha1812.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-21T16:20:47+00:00</updated><id>https://emharsha1812.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">KL Divergence</title><link href="https://emharsha1812.github.io/blog/2025/kldivergence/" rel="alternate" type="text/html" title="KL Divergence"/><published>2025-12-20T00:12:00+00:00</published><updated>2025-12-20T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/kldivergence</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/kldivergence/"><![CDATA[<p>We first define the relative entropy (KL-divergence), cross entropy, and entropy in the context of discrete probability distributions. We then provide a definition of the KL-divergence for continuous random variables.</p> <h2 id="the-kl-divergence-for-discrete-distributions">The KL-divergence for Discrete Distributions</h2> <p>Assume two probability distributions $p(\cdot)$ and $q(\cdot)$ over elements in some discrete sets $\mathcal{X}_p$ and $\mathcal{X}_q$ respectively. That is, $p(x)$ or $q(x)$ denote the respective probabilities, which are strictly positive unless $x \not\in \mathcal{X}_p$ for which $p(x) = 0$ (or similarly $x \not\in \mathcal{X}_q$ for which $q(x) = 0$).</p> <p>A key measure for the proximity between the distributions $p(\cdot)$ and $q(\cdot)$ is the <em>Kullback‚ÄìLeibler divergence</em>, also shortened as <em>KL-divergence</em>, and also known as the <em>relative entropy</em>. It is denoted $D_{\mathrm{KL}}(p\parallel q)$ and as long as $\mathcal{X}_p \subseteq \mathcal{X}_q$ it is the expected value of $\log p(X)/q(X)$ where $X$ is a random variable following the probability law $p(\cdot)$. Namely,</p> \[D_{\mathrm{KL}}(p\parallel q) = \sum_{x\in\mathcal{X}_p} p(x) \log\frac{p(x)}{q(x)}. \qquad (B.1)\] <p>Further if $\mathcal{X}_p \not\subseteq \mathcal{X}_q$, that is if there is some element in $\mathcal{X}_p$ that is not in $\mathcal{X}_q$, then by definition $D_{\mathrm{KL}}(p\parallel q)=+\infty$. This definition as infinity is natural since we would otherwise divide by $0$ for some $q(x)$.</p> <p>Observe that the expression for $D_{\mathrm{KL}}(p\parallel q)$ from (B.1) can be decomposed into the difference of $H(p)$ from $H(p,q)$ via,</p> \[D_{\mathrm{KL}}(p\parallel q) = \sum_{x\in\mathcal{X}} p(x)\log\frac{1}{q(x)} - \sum_{x\in\mathcal{X}} p(x)\log\frac{1}{p(x)} .\] <p>where the first term is $H(p,q)$ and the second term is $H(p)$.</p> <p>Here,</p> \[H(p,q) = -\sum_{x\in\mathcal{X}} p(x)\log q(x) \qquad (B.2)\] <p>is called the <em>cross entropy</em> of $p$ and $q$ and</p> \[H(p) = -\sum_{x\in\mathcal{X}} p(x)\log p(x) \qquad (B.3)\] <p>is called the <em>entropy</em> of $p$. Hence in words, the KL-divergence or relative entropy of $p$ and $q$ is the cross entropy of $p$ and $q$ with the entropy of $p$ subtracted. Note that in case where there are only two values in $\mathcal{X}$, say $0$ and $1$, where we denote $p(1)=p_1$ and $q(1)=q_1$, we have</p> \[H(p) = -\bigl(p_1\log p_1 + (1-p_1)\log(1-p_1)\bigr), \qquad (B.4)\] \[H(p,q) = -\bigl(p_1\log q_1 + (1-p_1)\log(1-q_1)\bigr). \qquad (B.5)\] <p>Some observations are in order. First observe that $D_{\mathrm{KL}}(p\parallel q) \ge 0$. Further note that in general $D_{\mathrm{KL}}(p\parallel q) \ne D_{\mathrm{KL}}(q\parallel p)$ and similarly $H(p,q) \ne H(q,p)$. Hence as a ‚Äúdistance measure‚Äù the KL-divergence is not a true metric since it is not symmetric over its arguments. Nevertheless, when $p=q$ the KL-divergence is 0 and similarly the cross entropy equals the entropy. In addition, it can be shown that $D_{\mathrm{KL}}(p\parallel q) = 0$ only when $p = q$. Hence the KL-divergence may play a role similar to a distance metric in certain applications. In fact, one may consider a sequence $q^{(1)},q^{(2)},\dots$ which has decreasing $D_{\mathrm{KL}}(p\parallel q^{(t)})$ approaching $0$ as $t\to\infty$. For such a sequence the probability distributions $q^{(t)}$ approach$^1$ the target distribution $p$ since the KL-divergence convergences to $0$.</p> <h2 id="the-kl-divergence-for-continuous-distributions">The KL-divergence for Continuous Distributions</h2> <p>The KL-divergence in (B.1) naturally extends to arbitrary probability distributions that are not necessarily discrete. In our case let us consider continuous multi-dimensional distributions. In this case $p(\cdot)$ and $q(\cdot)$ are probability densities, and the sets $\mathcal{X}_p$ and $\mathcal{X}_q$ are their respective supports. Now very similarly to (B.1), as long as $\mathcal{X}_p \subseteq \mathcal{X}_q$ we define,</p> \[D_{\mathrm{KL}}(p\parallel q) = \int_{x\in\mathcal{X}_p} p(x) \log\frac{p(x)}{q(x)} dx. \qquad (B.6)\]]]></content><author><name></name></author><category term="mathematics,"/><category term="ml"/><category term="mathematics,"/><category term="machine"/><category term="learning"/><summary type="html"><![CDATA[Mathematics of KL Divergence]]></summary></entry><entry><title type="html">A Visit to Hungarian Mathematics</title><link href="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/" rel="alternate" type="text/html" title="A Visit to Hungarian Mathematics"/><published>2024-09-01T00:12:00+00:00</published><updated>2024-09-01T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/hungarian-mathematics</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/"><![CDATA[<p>Because my domain is machine learning (ML) and artificial intelligence (AI), it‚Äôs quite common for me to come across abstract mathematical equations that initially made no sense to me. However, as I dove deep into those arcane-looking symbols, I discovered another interesting thing aside from the meaning: I realized that most of the founders of these equations were from Hungary!</p> <p>I googled, and then, a few clicks later, I stumbled on this very interesting read titled <strong>‚ÄòA Visit to Hungarian Mathematics‚Äô</strong>. Here‚Äôs the <a href="https://gwern.net/doc/math/1993-hersh.pdf">link</a> üîó to this pape . It contained exactly what I was looking for; a detailed introspection into Hungarian mathematicss and the mathematicians surrounding them. While reading the paper, I found a very simple but profoundly impactful quote that I would like to share. It says</p> <blockquote> <p>Everyone has ideas, both right ideas and wrong ideas. Scientific work consists merely of seperating them</p> </blockquote> <p>I immediately started voraciously reading the paper from top to bottom, left to right, and backwards too. Even though I am not a mathematician and do not have a mathematics degree (although I have a fairly good amount of mathematical exposure), I love to explore mathematics as a hobby. I sincerely hope that my interest in mathematics is still relevant.</p> <p>One of the key takeaways from this paper will be a quote by Alfred Renyi, a Hungarian mathematician known for his work in probability theory, combinatorics, and other fields. Once, a gifted mathematician told him that his working ability was heavily dependent on external circumstances. Renyi responded,</p> <blockquote> <p><strong>‚ÄúWhen I‚Äôm unhappy, I use math to find happiness; when I‚Äôm content, I use math to maintain my happiness.‚Äù</strong></p> </blockquote> <p>Reading about these mathematicians and their passion for pondering, fighting, and finally solving math problems fills me with a deep sense of gratitude towards math.</p>]]></content><author><name></name></author><category term="mathematics,culture"/><category term="mathematics,"/><category term="axioms"/><summary type="html"><![CDATA[Why Hungarians are so darn good at mathematics ?]]></summary></entry><entry><title type="html">Welcome!</title><link href="https://emharsha1812.github.io/blog/2024/welcome/" rel="alternate" type="text/html" title="Welcome!"/><published>2024-08-27T00:00:00+00:00</published><updated>2024-08-27T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/welcome</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/welcome/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.]]></summary></entry></feed>