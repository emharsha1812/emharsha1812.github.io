<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://emharsha1812.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emharsha1812.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-03T10:20:17+00:00</updated><id>https://emharsha1812.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">R squared in Machine Learning</title><link href="https://emharsha1812.github.io/blog/2025/rsquared/" rel="alternate" type="text/html" title="R squared in Machine Learning"/><published>2025-09-01T00:12:00+00:00</published><updated>2025-09-01T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/rsquared</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/rsquared/"><![CDATA[<p>The \(R^2\) metric, often called the coefficient of determination, is one of the most widely used measures for evaluating regression models. At its core, \(R^2\) tells us how well the model explains the variability of the dependent variable relative to a very simple baseline: the mean of that variable. To understand it deeply, let us start from the ground up.</p> <p>Suppose we have a dataset with a target variable \(y\), and our regression model produces predictions \(\hat{y}\). If we did not have any model at all, the best we could do to “predict” \(y\) would be to use its mean, \(\bar{y}\), for every data point. This simple strategy captures no nuance of the data, but it provides a baseline for comparison. The total variability in the data, called the total sum of squares (\(SS_{tot}\)), measures how much the actual values deviate from the mean. Mathematically, it is written as</p> \[SS_{tot} = \sum_i (y_i - \bar{y})^2.\] <p>Now, when we bring in a model, it produces predictions \(\hat{y}_i\). Naturally, those predictions won’t always be perfect, and the deviations of the predictions from the true values are called residuals. The sum of squared residuals (\(SS_{res}\)) captures how much unexplained error remains after using the model:</p> \[SS_{res} = \sum_i (y_i - \hat{y}_i)^2.\] <p>The magic of \(R^2\) lies in how it compares these two quantities. Specifically,</p> \[R^2 = 1 - \frac{SS_{res}}{SS_{tot}}.\] <p>If the model’s predictions are perfect, then the residual sum of squares vanishes to zero, giving \(R^2 = 1\). This indicates that the model explains all the variability of the data. On the other hand, if the model is no better than just predicting the mean, then \(SS_{res} = SS_{tot}\), and \(R^2\) becomes zero. Intriguingly, \(R^2\) can even be negative. This happens when the model is worse than the mean, in the sense that the residual errors are larger than the variability of the data itself. In such cases, the model is actively harmful as an explanatory tool.</p> <p>The intuitive interpretation of \(R^2\) is that it represents the fraction of variance in the dependent variable that is explained by the independent variables. If you think of variance as the “spread” or unpredictability in the data, then a model’s job is to account for as much of that spread as possible by relating it to explanatory features. For instance, if \(R^2 = 0.7\), it suggests that 70% of the variance in the outcome can be explained by the model, while the remaining 30% is still noise or unexplained. This interpretation makes \(R^2\) appealing because it ties the effectiveness of the model directly to the concept of variance explanation.</p> <p>However, one must also be cautious. \(R^2\) only measures variance explained relative to the mean model. It does not tell you whether the model is correct in a predictive sense, nor does it penalize overfitting directly. For example, adding more features to a model will never decrease \(R^2\); it can only stay the same or increase, even if the additional features have no real explanatory power. This is why adjusted \(R^2\) is often introduced, which penalizes the inclusion of unnecessary predictors by taking into account the number of features relative to the number of data points.</p> <p>Another subtle point is that \(R^2\) assumes that variance is the right quantity to explain. This makes sense in linear regression, where the goal is indeed to reduce squared error, but in contexts like nonlinear models or when distributions are highly skewed, the variance explanation picture may not fully align with predictive accuracy. For example, a model might achieve a high \(R^2\) but still perform poorly in predicting new data if it overfits. Similarly, in time series where temporal dependence is critical, variance explanation might be misleading without proper validation.</p> <p>So, in essence, \(R^2\) is a measure of how much better your model is compared to a naive mean predictor in terms of explaining variance in the target variable. It provides a normalized sense of fit: \(R^2 = 1\) means perfect explanation, \(R^2 = 0\) means no better than chance, and negative values indicate worse than chance. Thinking of it through the lens of variance explanation grounds it in the idea of “how much of the spread in the data have we accounted for?”—but it is always best interpreted alongside other metrics and validation strategies.</p> <p>Good. Let us now step into the geometric view, because it enriches our intuition for what \(R^2\) truly measures. Regression can be understood not only as a statistical minimization of squared error but also as a geometric projection in a high-dimensional vector space.</p> <p>Imagine your dataset of responses \(y = (y_1, y_2, \dots, y_n)\) as a vector sitting in an \(n\)-dimensional space. Each coordinate represents one observation. When we perform regression with predictors \(X\), what we are really doing is trying to find another vector \(\hat{y}\) that lies in the subspace spanned by the columns of \(X\). In other words, we are projecting the outcome vector \(y\) onto the space formed by linear combinations of the predictors. The projection gives us the fitted values \(\hat{y}\), while the leftover piece—the residuals \(y - \hat{y}\)—is orthogonal to that subspace.</p> <p>Now, why is this picture powerful for understanding \(R^2\)? Because variance explained corresponds to how much of the “length” (technically, squared norm) of \(y\) is captured in the projection. The total variability of \(y\) around its mean can be written as the squared length of the centered vector \(y - \bar{y}\mathbf{1}\), where \(\mathbf{1}\) is the all-ones vector. That is the total sum of squares, \(SS_{tot}\). The part explained by the regression is the squared length of the projection of this centered vector onto the column space of \(X\). That is called the regression sum of squares, \(SS_{reg}\). The residual sum of squares \(SS_{res}\) is simply the squared length of the orthogonal residual vector. By Pythagoras, these satisfy the neat identity</p> \[SS_{tot} = SS_{reg} + SS_{res}.\] <p>And from this decomposition, you see that</p> \[R^2 = \frac{SS_{reg}}{SS_{tot}},\] <p>which is literally “how much of the squared length is explained by the projection.”</p> <p>This geometry also reveals another interpretation: \(R^2\) is the square of the correlation coefficient between \(y\) and \(\hat{y}\). If you think of correlation as measuring alignment between two vectors, then \(R^2\) measures the degree to which the predicted vector lies in the same direction as the true vector. Perfect alignment gives correlation \(1\) and thus \(R^2 = 1\). A poor model, on the other hand, produces predictions that are only weakly aligned with the actual responses, giving a small \(R^2\). Negative \(R^2\) in this picture corresponds to the situation where the projection chosen by the model actually misaligns the predicted vector in such a way that it increases squared error compared to the mean baseline.</p> <p>So geometrically, variance explainability means this: you start with the cloud of data points in high-dimensional space, you draw the straightest line or hyperplane you can through them (given by the regression model), and you measure how much of the original data’s spread is captured along that line. The closer your data vector \(y\) lies to the subspace spanned by your predictors, the more variance you have explained, and the higher your \(R^2\).</p> <p>This perspective unifies the algebraic and statistical definitions. From one side, \(R^2\) is “1 minus unexplained variance over total variance.” From the other, it is the squared cosine of the angle between the true outcomes and the predictions. Both tell the same story: it quantifies alignment, projection, and variance accounted for by the model.</p> <hr/> <p>let’s now move from \(R^2\) to its refined cousin: <strong>adjusted \(R^2\)</strong>. The motivation for this adjustment emerges from a subtle flaw in plain \(R^2\). Remember that \(R^2\) never decreases as you add more predictors to a model. Even if the new variable has no true relationship with the target, the mere act of giving the model more flexibility allows it to fit the data slightly better, thereby reducing the residual sum of squares. This means that \(R^2\) is biased toward models with more features, and left unchecked, it can reward overfitting.</p> <p>Adjusted \(R^2\) was introduced to correct this. Its guiding idea is simple: yes, adding predictors can reduce error, but unless they genuinely improve explanatory power, they should be penalized for consuming degrees of freedom. The formula makes this precise. If you have \(n\) data points and \(p\) predictors, then adjusted \(R^2\) is defined as</p> \[R^2_{adj} = 1 - \frac{SS_{res}/(n-p-1)}{SS_{tot}/(n-1)}.\] <p>Notice the two denominators: instead of just comparing raw sums of squares, we are now comparing <strong>mean squared residuals per degree of freedom</strong>. The denominator \(n-1\) corresponds to the total variability after estimating a single mean, while the numerator \(n-p-1\) corresponds to the leftover variability after fitting \(p\) predictors. In essence, this formula asks: how much better is the model than the mean predictor, once we take into account the “cost” of the parameters used?</p> <p>The behavior of adjusted \(R^2\) is revealing. If a new predictor improves the model enough that the reduction in residual variance outweighs the penalty of losing a degree of freedom, adjusted \(R^2\) will rise. But if the predictor does not help much, the penalty dominates and adjusted \(R^2\) will actually fall. This makes it a more balanced tool for comparing models of different complexity.</p> <p>Another way to view it is through the lens of variance explanation again. While \(R^2\) asks “what fraction of variance do we explain,” adjusted \(R^2\) sharpens the question to “what fraction of variance do we explain per unit of explanatory effort?” It acknowledges that variance can be explained trivially by throwing in more variables, but meaningful explanation comes only when the gain surpasses the cost.</p> <p>It is important, however, to keep adjusted \(R^2\) in perspective. It is not a panacea. It still assumes that linear regression is the right modeling framework and that squared error is the right measure of fit. It is also influenced by sample size: with a small \(n\), the penalty for adding predictors is heavy, while with very large \(n\), adjusted \(R^2\) behaves more like plain \(R^2\). Nonetheless, within the family of linear regression comparisons, it is often the preferred metric because it guards against the seductive but misleading climb of \(R^2\) as more predictors are introduced.</p> <p>So, if you think of \(R^2\) as a measure of variance explanation in absolute terms, adjusted \(R^2\) is the disciplined version that insists on efficiency—explaining variance, yes, but doing so responsibly, without inflating the sense of achievement by smuggling in unnecessary variables.</p> <hr/>]]></content><author><name></name></author><category term="machine-learning"/><category term="machine-learning"/><summary type="html"><![CDATA[Meaning, Explanation & more]]></summary></entry><entry><title type="html">Training a simple bigram character level model on tiny stories</title><link href="https://emharsha1812.github.io/blog/2025/bigram/" rel="alternate" type="text/html" title="Training a simple bigram character level model on tiny stories"/><published>2025-05-24T00:12:00+00:00</published><updated>2025-05-24T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/bigram</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/bigram/"><![CDATA[<h1 id="building-a-bigram-language-model-a-step-by-step-guide-to-character-level-text-generation">Building a Bigram Language Model: A Step-by-Step Guide to Character-Level Text Generation</h1> <p>I wrote this small snippet as part of my learning process from Andrej’s video (link).</p> <h2 id="what-is-a-bigram-language-model">What is a Bigram Language Model?</h2> <p>A bigram language model predicts the next character in a sequence based solely on the current character. It’s called “bigram” because it considers pairs of characters (bi = two).</p> <p>The model learns a probability distribution over all possible next characters given the current character, essentially building a lookup table that says “when I see character X, what’s the most likely next character?”</p> <h2 id="dataset-preparation-and-text-loading">Dataset Preparation and Text Loading</h2> <p>Our journey begins with loading and examining our text data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">stories.text</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span><span class="o">=</span><span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">text</span><span class="p">[:</span><span class="mi">500</span><span class="p">])</span>
</code></pre></div></div> <p>The Tiny Stories dataset contains simple, child-friendly stories that are perfect for training language models. We load the entire text file into memory as a single string. This approach works well for smaller datasets, though larger datasets would require more sophisticated data loading strategies.</p> <h2 id="character-level-tokenization">Character-Level Tokenization</h2> <p>Unlike word-based models, our character-level approach treats each individual character as a token. This has several advantages:</p> <ul> <li><strong>Simplicity</strong>: No need for complex word segmentation</li> <li><strong>Robustness</strong>: Can handle any text, including typos and rare words</li> <li><strong>Fine-grained control</strong>: Learns spelling patterns and character relationships</li> </ul> <p>Let’s build our character vocabulary:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chars</span><span class="o">=</span><span class="nf">sorted</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">text</span><span class="p">)))</span>
<span class="n">vocab_size</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['\n', ' ', '!', '"', '#', '$', '&amp;', "'", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '&lt;', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¦', '©', '\xad', '±', '´', 'Â', 'Ã', 'â', 'ð', 'œ', 'Š', 'Ÿ', 'Ž', '˜', '“', '”', '‹', '€', '™']
101

 !"#$&amp;'()*+,-./0123456789:;&lt;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz¦©­±´ÂÃâðœŠŸŽ˜“”‹€™
</code></pre></div></div> <p>This code extracts all unique characters from our text and sorts them alphabetically. The vocabulary size tells us how many different characters our model needs to handle. Typically, this includes letters (both cases), numbers, punctuation, and whitespace characters.</p> <h2 id="building-the-tokenizer">Building the Tokenizer</h2> <p>Tokenization is the process of converting text into numerical representations that neural networks can process. We create two essential mappings:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stoi</span><span class="o">=</span><span class="p">{</span><span class="n">ch</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>  <span class="c1"># string to integer
</span><span class="n">itos</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>  <span class="c1"># integer to string
</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">])</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">stoi</code> (string-to-integer) dictionary maps each character to a unique integer ID, while <code class="language-plaintext highlighter-rouge">itos</code> (integer-to-string) provides the reverse mapping. Our encoder and decoder functions handle the conversion between text and numerical sequences.</p> <p>Testing our tokenizer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">exa</span><span class="o">=</span><span class="sh">"</span><span class="s">My name is Harshwardhan</span><span class="sh">"</span>
<span class="n">output</span><span class="o">=</span><span class="nf">encoder</span><span class="p">(</span><span class="n">exa</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">decoder</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[42, 80, 1, 69, 56, 68, 60, 1, 64, 74, 1, 37, 56, 73, 74, 63, 78, 56, 73, 59, 63, 56, 69]
My name is Harshwardhan
</code></pre></div></div> <p>This verification step ensures our encoding and decoding process is lossless - we can convert text to numbers and back to the original text perfectly.</p> <h2 id="converting-to-pytorch-tensors">Converting to PyTorch Tensors</h2> <p>Neural networks work with tensors, so we convert our encoded text into a PyTorch tensor:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nf">encoder</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([19212308])
tensor([48, 71, 70, 75, 14,  1, 48, 71, 70, 75,  1, 74, 56, 78,  1, 75, 63, 60,
         1, 74])
</code></pre></div></div> <p>The resulting tensor contains integer indices representing each character in our text. The shape tells us the total length of our dataset, while examining the first 100 elements helps us verify the conversion worked correctly.</p> <h2 id="dataset-splitting">Dataset Splitting</h2> <p>Machine learning requires separate training and validation sets to properly evaluate model performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">train</span><span class="o">=</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
<span class="n">validate</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span>
</code></pre></div></div> <p>We use a 90-10 split, dedicating 90% of our data to training and 10% to validation. The validation set helps us monitor whether our model is learning genuine patterns or simply memorizing the training data (overfitting).</p> <h2 id="understanding-context-windows">Understanding Context Windows</h2> <p>Language models don’t process entire texts at once. Instead, they work with fixed-size context windows.</p> <p>To give a context of what I am trying to say, here’s a snippet you can run to get an idea</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">block_size</span><span class="o">=</span><span class="mi">8</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">train</span><span class="p">[:</span><span class="n">block_size</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">block_size</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">when input is </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s"> the target: </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>when input is tensor([48]) the target: 71
when input is tensor([48, 71]) the target: 70
when input is tensor([48, 71, 70]) the target: 75
when input is tensor([48, 71, 70, 75]) the target: 14
when input is tensor([48, 71, 70, 75, 14]) the target: 1
when input is tensor([48, 71, 70, 75, 14,  1]) the target: 48
when input is tensor([48, 71, 70, 75, 14,  1, 48]) the target: 71
when input is tensor([48, 71, 70, 75, 14,  1, 48, 71]) the target: 70
</code></pre></div></div> <p>This code demonstrates a crucial concept: from a single sequence of length 8, we can create 8 different training examples. Each example uses a progressively longer context to predict the next character:</p> <ul> <li>Given just the first character, predict the second</li> <li>Given the first two characters, predict the third</li> <li>And so on…</li> </ul> <p>This approach maximizes the learning opportunities from our data and teaches the model to work with contexts of varying lengths.</p> <h2 id="batch-processing-for-efficient-training">Batch Processing for Efficient Training</h2> <p>Neural networks train more efficiently when processing multiple examples simultaneously. Our batch generation function creates random samples:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span>
<span class="n">block_size</span><span class="o">=</span><span class="mi">8</span>

<span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
    <span class="n">data</span><span class="o">=</span><span class="n">train</span> <span class="k">if</span> <span class="n">split</span><span class="o">==</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span> <span class="k">else</span> <span class="n">validate</span>
    <span class="n">ix</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="n">block_size</span><span class="p">,(</span><span class="n">batch_size</span><span class="p">,))</span>
    <span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span>
</code></pre></div></div> <p>This function randomly selects starting positions in our dataset and extracts sequences of length <code class="language-plaintext highlighter-rouge">block_size</code>. The result is two tensors:</p> <ul> <li><code class="language-plaintext highlighter-rouge">x</code>: Input sequences (what the model sees)</li> <li><code class="language-plaintext highlighter-rouge">y</code>: Target sequences (what the model should predict)</li> </ul> <p>The random sampling ensures our model sees different parts of the text in each batch, promoting better generalization.</p> <h2 id="the-bigram-language-model-architecture">The Bigram Language Model Architecture</h2> <p>Now we build our neural network. Despite its simplicity, this model embodies key language modeling concepts:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</code></pre></div></div> <p>The core of our model is an embedding table - essentially a learned lookup table where each character is associated with a vector of probabilities for the next character. The embedding dimension equals our vocabulary size, creating a direct mapping from current character to next character probabilities.</p> <h2 id="forward-pass-and-loss-calculation">Forward Pass and Loss Calculation</h2> <p>The forward pass transforms input sequences into predictions and calculates the training loss:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1"># (B,T,C)
</span>    
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <p>The embedding table produces “logits” - raw prediction scores for each possible next character. When we have targets (during training), we calculate cross-entropy loss, which measures how well our predictions match the actual next characters.</p> <p>The reshaping operations (<code class="language-plaintext highlighter-rouge">view</code>) are necessary because PyTorch’s cross-entropy function expects 2D inputs, but our model produces 3D tensors (batch, time, characters).</p> <h2 id="text-generation">Text Generation</h2> <p>The generation function is where our trained model becomes useful:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># focus on last time step
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># convert to probabilities
</span>        <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># sample
</span>        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># append
</span>    <span class="k">return</span> <span class="n">idx</span>
</code></pre></div></div> <p>This function implements autoregressive generation:</p> <ol> <li>Get predictions for the current sequence</li> <li>Focus only on the last position (most recent character)</li> <li>Convert logits to probabilities using softmax</li> <li>Sample a character based on these probabilities</li> <li>Add the sampled character to our sequence</li> <li>Repeat</li> </ol> <p>The sampling step is crucial - rather than always picking the most likely character (which would be deterministic and repetitive), we sample according to the probability distribution, introducing controlled randomness that makes the generated text more interesting and varied.</p> <h2 id="training-loop">Training Loop</h2> <p>Training a neural network involves repeatedly showing it examples and adjusting its parameters to reduce prediction errors:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="k">for</span> <span class="n">steps</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">(</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">m</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p>Each training step follows a standard pattern:</p> <ol> <li><strong>Forward pass</strong>: Feed data through the model to get predictions</li> <li><strong>Loss calculation</strong>: Compare predictions to actual targets</li> <li><strong>Backward pass</strong>: Calculate gradients showing how to improve</li> <li><strong>Parameter update</strong>: Adjust model weights to reduce loss</li> </ol> <p>We use the AdamW optimizer, which adapts the learning rate for each parameter individually, leading to more stable and efficient training than basic gradient descent.</p> <h2 id="monitoring-progress">Monitoring Progress</h2> <p>Before training, our model generates mostly gibberish:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">decoder</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sha inth ge jonin out, peroamy aveppedan s lld het
</code></pre></div></div> <p>After 10,000 training steps, the same generation call produces much more coherent text. The loss value also decreases significantly, indicating that our model is learning the character patterns in our dataset.</p> <h2 id="key-insights-and-limitations">Key Insights and Limitations</h2> <p>Our bigram model, while simple, demonstrates several important concepts:</p> <p><strong>Strengths:</strong></p> <ul> <li><strong>Simplicity</strong>: Easy to understand and implement</li> <li><strong>Speed</strong>: Fast training and inference</li> <li><strong>Foundational</strong>: Introduces core language modeling concepts</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li><strong>Limited context</strong>: Only considers the immediately previous character</li> <li><strong>No long-range dependencies</strong>: Cannot capture relationships between distant characters</li> <li><strong>Basic patterns</strong>: Learns simple character transitions but misses complex linguistic structures</li> </ul> <p>Here is the link to the Google Colab Notebook -</p>]]></content><author><name></name></author><category term="coding,"/><category term="python"/><category term="coding,"/><category term="python"/><summary type="html"><![CDATA[Training a simple bigram character level model on tiny stories]]></summary></entry><entry><title type="html">Machine Learning and AI Resources</title><link href="https://emharsha1812.github.io/blog/2025/nptel-ml/" rel="alternate" type="text/html" title="Machine Learning and AI Resources"/><published>2025-02-13T00:00:00+00:00</published><updated>2025-02-13T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/nptel-ml</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/nptel-ml/"><![CDATA[<blockquote> <p>“The goal is to turn data into information, and information into insight.”<br/> ~ Carly Fiorina</p> </blockquote> <p>Machine learning and artificial intelligence have revolutionized the way we approach problem-solving in many fields, from healthcare to robotics to natural language processing. If you’re looking to deepen your understanding of these technologies, here are some of the best online resources and courses available.</p> <h2 id="traditional-machine-learning">Traditional Machine Learning</h2> <ol> <li><a href="https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f/">Micro, Macro &amp; Weighted Averages of F1 Score, Clearly Explained</a></li> </ol> <h2 id="computer-vision">Computer Vision</h2> <ol> <li><a href="https://medium.com/@RobuRishabh/convolution-in-cnns-65e1655b5901">Convolution in CNNs</a></li> </ol> <h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2> <ol> <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs - Chris Olah</a></li> <li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable effectiveness of Recurrent Neural Networks</a></li> <li><a href="https://mmuratarat.github.io/2019-02-07/bptt-of-rnn">Backpropogation through time - Mathematical Derivation</a></li> <li><a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html">Why LSTMs Stop Your Gradients From Vanishing: A View from the Backwards Pass</a></li> </ol> <h2 id="learning-methods">Learning Methods</h2> <ol> <li><a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">Contrastive Representation Learning</a></li> </ol> <h2 id="famous-papers">Famous Papers</h2> <ol> <li><a href="https://medium.com/one-minute-machine-learning/clip-paper-explained-easily-in-3-levels-of-detail-61959814ad13">OpenAI’s CLIP Paper - Explanation</a></li> </ol> <h3 id="machine-learning--deep-learning-courses">Machine Learning &amp; Deep Learning Courses</h3> <ol> <li> <p><strong><a href="https://nptel.ac.in/courses/106106213">Practical Machine Learning with TensorFlow</a></strong><br/> Learn to build machine learning models using TensorFlow.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111105489">Mathematics for Machine Learning</a></strong><br/> A deep dive into the mathematical concepts that underpin machine learning algorithms.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111108066">Advanced Matrix Theory and Linear Algebra for Engineers</a></strong><br/> Understand matrix theory and linear algebra with an emphasis on engineering applications.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111108157">Matrix Theory</a></strong><br/> Learn the foundations of matrix theory, crucial for deep learning and machine learning algorithms.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111107137">Essential Mathematics for Machine Learning</a></strong><br/> Build a strong mathematical foundation for machine learning and AI.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/108103192">Machine Learning and Deep Learning Fundamentals</a></strong><br/> This course provides a comprehensive introduction to machine learning and deep learning concepts.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106139">Machine Learning</a></strong><br/> A fundamental course to kickstart your journey in machine learning.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106198">Machine Learning for Engineering and Science Applications</a></strong><br/> Learn how machine learning is applied in engineering and scientific research.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/108103192">Machine Learning And Deep Learning – Fundamentals and Applications</a></strong><br/> A blend of theory and practical applications in machine learning and deep learning.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106184">Deep Learning - Part 1</a></strong><br/> Introduction to deep learning fundamentals, including neural networks and optimization techniques.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106201">Deep Learning - Part 2</a></strong><br/> Dive deeper into advanced deep learning concepts, architectures, and frameworks.</p> </li> </ol> <h3 id="natural-language-processing-nlp">Natural Language Processing (NLP)</h3> <ol> <li> <p><strong><a href="https://nptel.ac.in/courses/106105158">Natural Language Processing</a></strong><br/> Learn the fundamentals of NLP, including text processing and feature extraction.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106101007">Natural Language Processing</a></strong><br/> A comprehensive NLP course exploring algorithms and applications.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106211">Applied Natural Language Processing</a></strong><br/> Learn how to apply NLP techniques in real-world projects.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106224">Deep Learning for Computer Vision</a></strong><br/> Explore how deep learning models are applied to computer vision problems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/108105103">Deep Learning for Visual Computing</a></strong><br/> Understand the intersection of deep learning and visual computing.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106102576">Introduction to Large Language Models - Tanmoy Chakraborty</a></strong><br/> A course dedicated to large language models and their applications in NLP.</p> </li> <li> <p><strong><a href="https://www.youtube.com/playlist?list=PLZ2ps__7DhBbaMNZoyW2Hizl8DG6ikkjo">Introduction to Large Language Models - Mitesh Khapra</a></strong><br/> Learn about large language models from an industry expert.</p> </li> </ol> <h3 id="reinforcement-learning--ai">Reinforcement Learning &amp; AI</h3> <ol> <li> <p><strong><a href="https://nptel.ac.in/courses/106101466">Distributed Optimization and Machine Learning</a></strong><br/> Explore the optimization techniques used in distributed machine learning systems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/110101145">Bandit Algorithm</a></strong><br/> Learn the fundamentals of multi-armed bandit algorithms, useful in reinforcement learning.</p> </li> <li> <p><strong><a href="https://www.youtube.com/playlist?list=PLL1s8qiaGy0LwIajdxKZr_FRL7KZeQK9r">Deep Generative Models</a></strong><br/> Delve into the theory and applications of generative models like GANs and VAEs.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106143">Reinforcement Learning</a></strong><br/> An introduction to reinforcement learning, where agents learn by interacting with the environment.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106140">Artificial Intelligence: Knowledge Representation and Reasoning</a></strong><br/> Learn how knowledge can be represented and reasoned within AI systems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106226">Artificial Intelligence Search Methods For Problem Solving</a></strong><br/> Study search algorithms, essential for AI problem-solving.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106238">Applied Accelerated Artificial Intelligence</a></strong><br/> Learn how to speed up and apply AI techniques in various industries.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106105077">Artificial Intelligence</a></strong><br/> A comprehensive introduction to the field of artificial intelligence.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106105078">Artificial Intelligence</a></strong><br/> Learn AI concepts and techniques applicable in real-world problems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/117108048">Pattern Recognition</a></strong><br/> Understand pattern recognition and its applications across diverse fields.</p> </li> </ol> <hr/> <p>Credits - <a href="https://www.linkedin.com/in/bastyajayshenoy/">Ajay Shenoy</a></p> <h2 id="large-language-models">Large Language Models</h2> <h3 id="llm-reasoning-papers">LLM Reasoning Papers</h3> <ul> <li><strong>LM Post-Training: A Deep Dive into Reasoning</strong> <ul> <li><a href="https://arxiv.org/pdf/2502.21321">https://arxiv.org/pdf/2502.21321</a></li> </ul> </li> <li><strong>A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1</strong> <ul> <li><a href="https://arxiv.org/html/2502.10867v1">https://arxiv.org/html/2502.10867v1</a></li> </ul> </li> <li><strong>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</strong> <ul> <li><a href="https://arxiv.org/html/2502.14768v1">https://arxiv.org/html/2502.14768v1</a></li> </ul> </li> </ul> <h3 id="llm-research-blogs">LLM Research Blogs</h3> <ul> <li><strong>LLM Research Newsletter</strong> <ul> <li><a href="https://www.llmsresearch.com/">https://www.llmsresearch.com/</a></li> </ul> </li> <li><strong>Transformer Circuit thread</strong> <ul> <li><a href="https://transformer-circuits.pub/">https://transformer-circuits.pub/</a></li> </ul> </li> </ul> <h3 id="internals">Internals</h3> <ul> <li><strong>Pytorch Internals</strong> <ul> <li><a href="https://blog.ezyang.com/2019/05/pytorch-internals/">https://blog.ezyang.com/2019/05/pytorch-internals/</a></li> </ul> </li> <li><strong>Transformer Internals</strong> <ul> <li><a href="https://goyalpramod.github.io/blogs/Transformers_laid_out/#coding-the-transformer/">https://goyalpramod.github.io/blogs/Transformers_laid_out/#coding-the-transformer</a></li> </ul> </li> </ul> <h3 id="iclr-blog-posts">ICLR BLog Posts</h3> <ul> <li><strong>A New Alchemy: Language Model Development as a Subfield?</strong> <ul> <li><a href="https://iclr-blogposts.github.io/2024/blog/language-model-development-as-a-new-subfield/">https://iclr-blogposts.github.io/2024/blog/language-model-development-as-a-new-subfield/</a></li> </ul> </li> <li><strong>Fairness in AI: two philosophies or just one?</strong> <ul> <li><a href="https://iclr-blogposts.github.io/2024/blog/fairness-ai-two-phil-or-just-one/">https://iclr-blogposts.github.io/2024/blog/fairness-ai-two-phil-or-just-one/</a></li> </ul> </li> </ul> <h2 id="generative-ai">Generative AI</h2> <ol> <li> <p><a href="https://huyenchip.com/2025/01/16/ai-engineering-pitfalls.html"><em>Common pitfalls when building generative AI applications</em></a> by Chip Huyen</p> </li> <li> <p><a href="https://blog.ml.cmu.edu/#"><em>ML CMU Blog</em></a></p> </li> </ol> <h2 id="mechanistic-interpretibility-mi">Mechanistic Interpretibility (MI)</h2> <p>Mechanistic interpretability aims to reverse-engineer a neural network into human-understandable mechanisms. MI focuses on transformers (specifically LLMs) but is not limited to these neural network architectures</p> <h3 id="people">People</h3> <ol> <li><a href="https://www.neelnanda.io/mechanistic-interpretability">Neel Nanda</a></li> <li><a href="https://www.alignmentforum.org/">Alignment Forum</a></li> <li></li> </ol> <h3 id="primer-on-llms">Primer on LLMs</h3> <ol> <li><a href="https://www.understandingai.org/p/large-language-models-explained-with">Large language models, explained with a minimum of math and jargon</a></li> <li></li> </ol> <h3 id="transformers">Transformers</h3> <ol> <li><a href="https://arena-chapter1-transformer-interp.streamlit.app/">Transformers Interpretibility</a></li> <li><a href="https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability">200 Concrete Open Problems in MI</a></li> <li></li> </ol> <h3 id="quick-guides-to-mi">Quick Guides to MI</h3> <ol> <li><a href="https://mindfulmodeler.substack.com/p/what-is-mechanistic-interpretability">What is Mechanistic Interpretability and where did it come from?</a></li> <li><a href="https://bluedot.org/blog/introduction-to-mechanistic-interpretability">Introduction to Mechanistic Interpretability</a></li> <li><a href="https://seantrott.substack.com/p/mechanistic-interpretability-for">“Mechanistic interpretability” for LLMs, explained</a></li> </ol> <h4 id="how-to-get-started-with-mi-">How to get started with MI ?</h4> <ol> <li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Concrete Steps to Get Started in Transformer Mechanistic Interpretability</a></li> </ol> <h3 id="relevant-papers">Relevant Papers</h3> <ol> <li><a href="https://arxiv.org/abs/2310.14491">Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models</a></li> <li><a href="https://arxiv.org/html/2407.02646v1">A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models</a></li> <li><a href="https://arxiv.org/pdf/2404.14082">Mechanistic Interpretability for AI Safety : A Review</a></li> </ol> <h3 id="straight-from-anthropic">Straight from Anthropic</h3> <ol> <li><a href="https://www.anthropic.com/research/mapping-mind-language-model">Mapping the mind of a Large Language model</a></li> <li><a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html">Interpretibility Dreams</a></li> <li><a href="https://www.anthropic.com/news/golden-gate-claude">Golden Gate Claude</a></li> <li><a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</a></li> <li><a href="https://transformer-circuits.pub/">Transformer Circuits Thread</a></li> </ol> <h3 id="blogs">Blogs</h3> <ol> <li><a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability">Neel Nanda’s case on why we need interpretibility research</a></li> <li><a href="https://hkamath.me/blog/2024/rqae/">A Microscope into the Dark Matter of Interpretability</a></li> </ol> <h3 id="libraries">Libraries</h3> <ol> <li><a href="https://transformerlensorg.github.io/TransformerLens/content/getting_started_mech_interp.html">Transfomer Lens</a></li> <li><a href="https://www.neuronpedia.org/">Neuronpedia</a></li> <li><a href="https://nnsight.net/">Interpretable Neural Networks</a></li> </ol> <h2 id="why-we-need-mi-research-">Why we need MI Research ?</h2> <p>Neel Nanda makes a couple of strong arguments <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability">here</a> (15 in fact!) on why interpretibility research is needed and how it will help us resolve x-issues</p>]]></content><author><name></name></author><category term="machine-learning,"/><category term="ai,"/><category term="deep-learning,"/><category term="nlp"/><category term="machine-learning,"/><category term="deep-learning,"/><category term="ai,"/><category term="nlp"/><summary type="html"><![CDATA[A collection of links to essential courses on machine learning, deep learning, natural language processing, and artificial intelligence.]]></summary></entry><entry><title type="html">Python Notes</title><link href="https://emharsha1812.github.io/blog/2025/python-notes/" rel="alternate" type="text/html" title="Python Notes"/><published>2025-01-22T00:12:00+00:00</published><updated>2025-01-22T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/python-notes</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/python-notes/"><![CDATA[<p>Hello everyone,<br/> Below, I am sharing the GitHub repository containing all my Python notebooks, which I created while teaching an AI-ML Ops training program to esteemed DRDO scientists. You can access the GitHub repository here: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">GitHub Link</a>.</p> <p>This repository is a work in progress, and I will continue to update it as I create new notebooks. Here is the current plan for upcoming content:</p> <ol class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Python One-Liners Notebook</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Walrus Operator in Python</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Building a simple project using Python</li> </ol> <p>Feel free to explore and stay tuned for updates!</p> <p>Here’s the direct GitHub link for quick access: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">https://github.com/emharsha1812/Python_Programming_Notebooks</a></p> <p>Don’t forget to ⭐ star the repository to stay updated with new additions!</p> <h3 id="ps---here-is-a-list-of-helpful-links-that-you-can-reference-from-time-to-time">PS - Here is a list of helpful links that you can reference from time to time</h3> <ol> <li> <p><a href="https://realpython.com/python-iterators-iterables/">Python Iterators</a> - A super friendly blog on Python Iterators. I recommend taking this <a href="https://realpython.com/quizzes/python-iterators-iterables/">quiz</a> as well after reading the whole blog.</p> </li> <li> <p><a href="https://realpython.com/python-memory-management/">Memory Management in Python</a> - One of those “You should definitely know this” topics.</p> </li> </ol>]]></content><author><name></name></author><category term="coding,"/><category term="python"/><category term="coding,"/><category term="python"/><summary type="html"><![CDATA[A collection of Python notebooks for quick reference]]></summary></entry><entry><title type="html">KAN (Kolmogorov-Arnold Networks)</title><link href="https://emharsha1812.github.io/blog/2025/kan/" rel="alternate" type="text/html" title="KAN (Kolmogorov-Arnold Networks)"/><published>2025-01-07T00:12:00+00:00</published><updated>2025-01-07T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/kan</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/kan/"><![CDATA[<h2 id="1-mathematical-foundations--architecture">1. Mathematical Foundations &amp; Architecture</h2> <p><strong>Kolmogorov-Arnold Representation Theorem:</strong> KANs are founded on a classic result by A. N. Kolmogorov and V. Arnold, which states that <em>any continuous multivariate function can be represented as a finite superposition of univariate functions</em>(<a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov%E2%80%99s%20theorem%2C%20proposed%20in%201957%2C,The%20CFL%20condition%2C%20introduced%20by">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a>). In practical terms, this theorem guarantees that for a function \(f(x_1,\dots,x_n)\), there exist some continuous 1D functions \(\{\Phi_q\}\) and \(\{\Psi_{q,p}\}\) such that:</p> \[f(x_1,\ldots,x_n) \;=\; \sum_{q=0}^{2n} \; \Phi_q\!\Big( \sum_{p=1}^n \Psi_{q,p}(x_p) \Big)\,,\] <p>i.e. \(f\) can be decomposed into <strong>inner</strong> univariate functions \(\Psi_{q,p}\) (each depending on a single input variable $x_p$) and <strong>outer</strong> univariate functions \(\Phi_q\) aggregated by addition. This theorem provides a constructive blueprint for function approximation using single-variable building blocks, which is the key inspiration for KANs</p> <p><strong>KAN Architecture</strong> - Instead of the traditional neuron model with linear weighted sums and fixed activations, a KAN implements the above idea by making <strong>each edge</strong> of the network carry a <em>learnable univariate function</em>. In other words, every connection between neurons is parameterized as a nonlinear function (originally chosen as a B-spline) rather than a scalar weight. Each neuron simply sums up the outputs of the incoming edge-functions. Formally, if \(z_i^{(l)}\) denotes the \(i\)-th activation in layer \(l\), then a <strong>KAN layer</strong> computes each output neuron \(j\) as: (<a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview</a>) \(z_j^{(l+1)} \;=\; \sum_{i=1}^{N_l} f_{ij}^{(l)}\!\Big(z_i^{(l)}\Big)\,,\)</p> <p>where \(f_{ij}^{(l)}: \mathbb{R}\to\mathbb{R}\) is a learnable univariate function on the edge from neuron \(i\) (layer \(l\)) to neuron \(j\) (layer \(l+1\)). There are no separate linear weight matrices; the nonlinearity of \(f_{ij}\) itself provides the transformation. In the <em>shallowest</em> case (two-layer KAN), this architecture directly mirrors Kolmogorov’s decomposition: the first layer learns inner functions \(h_{p}(x_p)\) on each input dimension, and the second layer learns outer functions \(g_q(\cdot)\) that combine those results <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Quanta Magazine</a>.</p> <p><strong>Parameterized Functions (B-Splines):</strong> In practice, each learnable edge-function \(f_{ij}\) is parameterized as a spline (often a B-spline) with a set of control points that can be tuned during training. B-splines are piecewise polynomial curves defined by control points, offering a flexible yet smooth basis for approximating arbitrary 1D functions. By adjusting the control points, the shape of the spline changes locally without affecting the entire function. This choice ensures the learned activation functions are <em>smooth</em> addressing potential non-smoothness in Kolmogorov’s original construction and stable to train. Each edge thus has multiple parameters (the spline control values) instead of a single weight. For example, a KAN might initialize each \(f_{ij}\) as a near-linear spline and then let training mold each into the required nonlinear shape. This edge-centric design lets KANs <em>dynamically adapt their activation functions</em> to the data, rather than relying on a fixed function like ReLU or tanh.</p> <p><strong>Illustrative Pseudocode:</strong> The following pseudocode contrasts a single layer of an MLP vs. a KAN:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanmlp.svg-480.webp 480w,/assets/img/kanmlp.svg-800.webp 800w,/assets/img/kanmlp.svg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanmlp.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Comparison of MLP Layer with KAN Layer in Pytorch </div> <p>In the KAN layer, <code class="language-plaintext highlighter-rouge">f_ij</code> is a learned function (e.g. a spline) specific to edge \((i\to j)\), replacing both the weight and the neuron’s activation for that connection. The neuron simply aggregates these contributions (here via summation). Deep KANs can be built by stacking such layers, allowing composition of these univariate transformations across multiple levels.</p> <h2 id="2-comparison-with-mlps">2. Comparison with MLPs</h2> <p><strong>Structural Differences:</strong> Traditional Multi-Layer Perceptrons (MLPs) use <em>linear weights</em> and <em>fixed activation functions at neurons</em>, whereas KANs use <em>no linear weights at all</em> – every “weight” is replaced by a flexible function on the input signal. In effect, MLPs learn parameters for <strong>nodes</strong> (the weight matrix between layers is trained, then a fixed nonlinearity like ReLU is applied), while KANs learn parameters for <strong>edges</strong> (each connection has a trainable nonlinear mapping). This leads to a duality: <em>MLP = fixed nonlinearity + learned linear weights; KAN = fixed linear sum + learned nonlinear functions</em>. The figure below (from Liu et al. 2024) illustrates this difference, highlighting that MLPs apply activations at neurons (circles) whereas KANs apply learned functions on each connecting edge before summing.(<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanvsmlp-480.webp 480w,/assets/img/kanvsmlp-800.webp 800w,/assets/img/kanvsmlp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanvsmlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Source: <a href="https://arxiv.org/pdf/2404.19756" target="_blank">Liu et al. (2024)</a> </div> <p><strong>Learnable Functions vs Fixed Weights:</strong> In an MLP, the transformation from layer to layer is \(\sigma(Wx + b)\), with \(\sigma\) (e.g. ReLU) fixed and \(W,b\) learned. In a KAN, the transformation is \(\sum_i f_i(x_i)\) (plus bias if needed), with each $f_i$ being learned and no separate \(W\). Essentially, KANs “allocate” more flexibility per connection, whereas MLPs rely on combining many fixed nonlinear units to build complexity. This means KANs move the bulk of learnable parameters into the activation functions themselves, often resulting in <em>fewer total connections</em> needed than an equivalent MLP (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a>).</p> <p><strong>Expressive Power (Universal Approximation):</strong> Both MLPs and KANs are universal function approximators, but via different theorems. MLPs leverage the Universal Approximation Theorem (with enough neurons, an MLP can approximate any continuous function on a domain), while KANs directly leverage the Kolmogorov-Arnold (K-A) theorem to construct such approximations. In theory, a single hidden-layer KAN with sufficiently complex edge functions can exactly represent any continuous function (the K-A theorem provides an existence proof), whereas an MLP might require many more neurons or layers to approximate the same function with fixed activations. KANs thus excel at modeling functions with complex or “spiky” behavior in each input dimension, because each edge can carve out a detailed univariate relationship. In practice, KANs implement the K-A decomposition <em>explicitly</em>, using B-spline basis functions to approximate the required univariate mappings. This can translate to <em>greater expressivity per parameter</em>.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>).</p> <p><strong>Parameter Efficiency &amp; Neural Scaling:</strong> A striking reported advantage is that <em>much smaller KANs can achieve accuracy comparable or superior to much larger MLPs</em> on certain tasks. Each KAN edge function (with, say, $k$ control points) can encode a nonlinear relation that an MLP might need multiple neurons and layers to capture. Empirically, Liu <em>et al.</em> (2024) found KANs follow faster <strong>neural scaling laws</strong> – the error decreases more rapidly as model size increases, compared to MLPs. In other words, to reach a given accuracy, a KAN required fewer trainable parameters than an MLP in their tests. The flexibility of splines allows KANs to fit complex patterns without blowing up the network width/depth. One study noted that KANs can <em>match</em> MLP performance at equal parameter counts, and sometimes exceed it, though they require careful tuning (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=KANs%20definitely%20don%27t%20feel%20like,compared%20to%20regular%20neural%20networks">Trying Kolmogorov-Arnold Networks in Practice</a>). The original KAN paper demonstrated that a KAN with significantly fewer nodes could outperform a dense ReLU network on function-fitting benchmarks.</p> <p><strong>Continuous Learning and Locality:</strong> Because each KAN weight is a localized function (with local control points), learning in a KAN can be more localized. This has implications for <strong>continual learning</strong>. In standard nets, fine-tuning on new data often alters weights globally and can erode old capabilities (catastrophic forgetting). In KANs, adding new data primarily adjusts the spline control points <em>in relevant regions of the input space</em>, leaving other regions (and other functions) mostly unchanged. For example, if a KAN-based language model learns a new vocabulary or coding style, only certain edge-functions for those inputs might reshape, while others retain their previously learned shape. This property means KANs can integrate new knowledge without overwriting all weights, potentially enabling more <strong>seamless continual learning</strong>. MLPs, by contrast, have distributed representations where a single weight doesn’t correspond to an isolated input relationship, making targeted updates harder.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,local%20control%20point%20parameters%20change">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>)</p> <p><strong>Interpretability:</strong> A major motivation for KANs is interpretability. In an MLP, each weight by itself is usually not meaningful, and neurons combine many weights making interpretation difficult. In a KAN, each edge’s function $f_{ij}(x)$ can be visualized as a curve, directly showing how the input from neuron $i$ influences neuron $j$ across the range of values. After training, one can <em>extract these learned univariate functions</em> and inspect them.They might correspond to intuitive relations (e.g. an edge function might learn a sinusoidal shape if the output depends sinusoidally on an input).This transparency is especially useful in scientific or engineering tasks where understanding the learned model is as important as its accuracy. MLPs lack this fine-grained interpretability, since their learned mapping is entangled across many parameters. Thus, KANs offer a more human-understandable model: as the saying goes, they turn the <strong>“black box”</strong> into a collection of readable 1D transformations.</p> <p><strong>Summary:</strong> KANs and MLPs both approximate complex functions, but KANs do so by <em>baking learnable math into the connections</em>. This difference yields advantages in function approximation fidelity, parameter efficiency, and interpretability. However, it also comes with computational challenges (will uupdate later). In essence, KANs can be seen as a <strong>new paradigm</strong>: they trade the simple, generic structure of MLPs for a structure with built-in mathematical richness (the Kolmogorov-Arnold basis). This seemingly small change – moving from scalar weights to learned functions – has profound implications on how the network learns and what it can represent (<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>).</p> <p>The original paper can be found <a href="https://arxiv.org/pdf/2404.19756">here</a></p> <p>Last Updated - 25/02/2025</p> <h3 id="references">References</h3> <p>[1] - <a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov,the%20model%E2%80%99s%20flexibility%20and%20interpretability">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a></p> <p>[2] - <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Novel Architecture Makes Neural Networks More Understandable</a></p> <p>[3] - <a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview on KAN: Kolmogorov–Arnold Networks</a></p> <p>[4] - <a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a></p> <p>[5] - <a href="https://www.datacamp.com/tutorial/kolmogorov-arnold-networks#:~:text=simpler%2C%20univariate%20ones,edges%20are%20used%20for%20approximation">Kolmogorov-Arnold Networks (KANs): A Guide With Implementation</a></p> <p>[6] - <a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a></p> <p>[7] - <a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a></p> ]]></content><author><name></name></author><category term="llm,machine-learning,python"/><category term="kan,deep-learning,research"/><summary type="html"><![CDATA[An Alternative to traditional MLPs]]></summary></entry><entry><title type="html">A Visit to Hungarian Mathematics</title><link href="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/" rel="alternate" type="text/html" title="A Visit to Hungarian Mathematics"/><published>2024-09-01T00:12:00+00:00</published><updated>2024-09-01T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/hungarian-mathematics</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/"><![CDATA[<p>Because my domain is machine learning (ML) and artificial intelligence (AI), it’s quite common for me to come across abstract mathematical equations that initially made no sense to me. However, as I dove deep into those arcane-looking symbols, I discovered another interesting thing aside from the meaning: I realized that most of the founders of these equations were from Hungary!</p> <p>I googled, and then, a few clicks later, I stumbled on this very interesting read titled <strong>‘A Visit to Hungarian Mathematics’</strong>. Here’s the <a href="https://gwern.net/doc/math/1993-hersh.pdf">link</a> 🔗 to this pape . It contained exactly what I was looking for; a detailed introspection into Hungarian mathematicss and the mathematicians surrounding them. While reading the paper, I found a very simple but profoundly impactful quote that I would like to share. It says</p> <blockquote> <p>Everyone has ideas, both right ideas and wrong ideas. Scientific work consists merely of seperating them</p> </blockquote> <p>I immediately started voraciously reading the paper from top to bottom, left to right, and backwards too. Even though I am not a mathematician and do not have a mathematics degree (although I have a fairly good amount of mathematical exposure), I love to explore mathematics as a hobby. I sincerely hope that my interest in mathematics is still relevant.</p> <p>One of the key takeaways from this paper will be a quote by Alfred Renyi, a Hungarian mathematician known for his work in probability theory, combinatorics, and other fields. Once, a gifted mathematician told him that his working ability was heavily dependent on external circumstances. Renyi responded,</p> <blockquote> <p><strong>“When I’m unhappy, I use math to find happiness; when I’m content, I use math to maintain my happiness.”</strong></p> </blockquote> <p>Reading about these mathematicians and their passion for pondering, fighting, and finally solving math problems fills me with a deep sense of gratitude towards math.</p>]]></content><author><name></name></author><category term="mathematics,culture"/><category term="mathematics,"/><category term="axioms"/><summary type="html"><![CDATA[Why Hungarians are so darn good at mathematics ?]]></summary></entry><entry><title type="html">Welcome!</title><link href="https://emharsha1812.github.io/blog/2024/welcome/" rel="alternate" type="text/html" title="Welcome!"/><published>2024-08-27T00:00:00+00:00</published><updated>2024-08-27T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/welcome</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/welcome/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.]]></summary></entry></feed>