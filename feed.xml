<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://emharsha1812.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emharsha1812.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-06T17:39:25+00:00</updated><id>https://emharsha1812.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers</title><link href="https://emharsha1812.github.io/blog/2025/flash/" rel="alternate" type="text/html" title="FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers"/><published>2025-11-25T00:00:00+00:00</published><updated>2025-11-25T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/flash</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/flash/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The fundamental concept that underpins the transformer architecture is <strong>Attention</strong>. This was originally developed as an enhancement to RNNs for machine translation <d-cite key="bahdanau2016neuralmachinetranslationjointly"> </d-cite>. However, in 2017, Vaswani et al. <d-cite key="vaswani2023attentionneed"></d-cite> showed that significantly improved performance could be obtained by eliminating the recurrence structure and instead focusing exclusively on the attention mechanism.</p> <p>The importance of this mechanism can be explained with the help of the following example</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_1-480.webp 480w,/assets/img/blog/Figure_1-800.webp 800w,/assets/img/blog/Figure_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/Figure_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Attention weights showing how the model resolves ambiguity in word meaning through context. The arrows indicate strong attention connections between 'bank' and contextually relevant words.</figcaption> </figure> <p>Consider the sentence <strong>“I swam across the river to get to the other bank.”</strong> The word “<strong>bank</strong>” has multiple meanings—it could refer to a financial institution or a riverbank. The attention mechanism helps the model understand context by weighing relationships between words. In this case, the model attends strongly to words like “swam,” “across,” and “river,” which provide contextual clues that “bank” refers to a riverbank rather than a financial institution.</p> <p>Therefore, the Attention mechanism has become the single most important mechanism driving the growth of Large Language Models. Over the years, several variants of the attention mechanism have been proposed such as Multi Query Attention (MQA) (cite), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), etc. For instance, here’s a non-exhaustive taxonomy of efficient attention mechanisms</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attentiontaxonomy-480.webp 480w,/assets/img/attentiontaxonomy-800.webp 800w,/assets/img/attentiontaxonomy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/attentiontaxonomy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure adapted from <d-cite key="sun2025efficient"></d-cite></figcaption> </figure> <p>However, the transformer’s attention mechanism has a fatal flaw: it scales quadratically both in time and memory with sequence length, resulting in $O(n^2 \cdot d_{\text{model}})$ time complexity and $O(n^2)$ memory complexity. For example, a 2,048-token sequence requires 16 MB of memory for the attention matrix; at 16,384 tokens, this balloons to 1GB per layer. A rigorous mathematical proof is presented in Appendix A. This quadratic scaling makes it prohibitive for processing long sequences beyond 8K-16K tokens without specialized optimizations <d-cite key="keles2022computationalcomplexityselfattention"></d-cite>. While many works aim for sub-quadratic attention using approximations, including Linformer <d-cite key="wang2020linformerselfattentionlinearcomplexity"></d-cite>, Performer <d-cite key="choromanski2022rethinkingattentionperformers"></d-cite>, and Reformer <d-cite key="kitaev2020reformerefficienttransformer"></d-cite>, these methods have seen limited use in large language models. These are approximate attention methods that reduce cost through low-rank projections, kernel approximations, or sparse routing. These assumptions improve asymptotic complexity, but they introduce accuracy and hardware-efficiency tradeoffs, so large-scale models still rely on exact attention.</p> <p>The FlashAttention series by Tri Dao <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention2"></d-cite><d-cite key="shah2024flashattention3"></d-cite> looks at the attention bottleneck from a systems angle. Instead of approximating attention and hurting model quality, the idea is to rethink how attention moves data through the GPU. Modern GPUs have a very uneven memory hierarchy, so the cost of moving data often dominates the cost of doing the math. FlashAttention reduces this movement and gets closer to the limits of the hardware. In this blog, we walk through how this idea has evolved. FlashAttention v1 introduced tiled exact attention. FlashAttention v2 improved how work is split across the GPU. FlashAttention v3 added warp specialization, asynchrony, and low precision on Hopper to push utilization even higher. We also refer to what is known about FlashAttention 4, though an official paper is not public yet.</p> <h2 id="background">Background</h2> <h3 id="gpu-memory-hierarchy">GPU Memory Hierarchy</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_2-480.webp 480w,/assets/img/blog/Figure_2-800.webp 800w,/assets/img/blog/Figure_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/Figure_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Memory Hierarchy with Bandwidth &amp; Memory Size</figcaption> </figure> <p>Every modern processor faces the same fundamental challenge: fast storage is expense and small, while large storage is slow and cheap. Modern GPUs organise memory into a hierarchal form which has five distinct levels, each with different characteristics, different access patterns, and different implications for your code. Starting from the fastest and smallest and working towards the slowest and largest, these levels are: registers, shared memory, L1 cache, L2 cache, and global memory. (Figure 2). At the stop of the memory hierarchy sit registers, the fstest storage available on a GPU. Each thread runninng on the GPU has access to a private set of registers - typically up to 255 registers per thread on modern NVIDIA architectures. These registers feed directly into the computatinal units. When a thread performs an arithmatic operation, the operands come from registers and the result goes back to registers. There is no seperate “register access” operation visible to the programmer; registers are simply where the active data lives. The register file on a single Streaming Multiprocessor contains 65,536 registers with each register holding 32 bits. This gives 256 kilobytes of register storage per SM, and these registers are dynamically shared among all threads running on that SM</p> <p>Unlike registers, which are private to each individual threads, shared memory (SRAM) is shared among all threads in a thread block. It is the primary mechanism for threads to cooperate and community and is <strong>explicitly managed by the programmer rather than automatically managed by the compiler</strong>. Shared memory provides a staging area for data that multiple threads need to access. Rather than having each thread read the same value from slow global memory, one thread can read it once into shared memory, synchronize with the other threads, and then all threads can read from fast shared memory. Secondly, it enables algorithms that require threads to exchange data. This is used by FlashAttention explicitly</p> <p>At the bottom of the hierarchy sits global memory - the large DRAM pool that provides the bulk of a GPU’s storage capacity. An H100 for example, has 80GBs of HBM memory, operating at around 3,000 gigabytes per second of bandwidth. THis is where your input data starts, where your output data goes, and where any persistent state lives. It is also by far the slowest level of heirarchy, with individual access latencies reaching 400 to 800 clock cycles depending on architecture and access pattern.</p> <table> <thead> <tr> <th style="text-align: left">Memory Level</th> <th style="text-align: left">Capacity</th> <th style="text-align: left">Bandwidth</th> <th style="text-align: left">Latency</th> <th style="text-align: left">Programmer Control</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Registers</strong></td> <td style="text-align: left">256 KB/SM</td> <td style="text-align: left">~100 TB/s</td> <td style="text-align: left">0 cycles</td> <td style="text-align: left">Automatic</td> </tr> <tr> <td style="text-align: left"><strong>SRAM</strong></td> <td style="text-align: left">192 KB/SM</td> <td style="text-align: left">19 TB/s</td> <td style="text-align: left">~20 cycles</td> <td style="text-align: left">Explicit</td> </tr> <tr> <td style="text-align: left"><strong>L2 Cache</strong></td> <td style="text-align: left">40-50 MB</td> <td style="text-align: left">12 TB/s</td> <td style="text-align: left">~200 cycles</td> <td style="text-align: left">Transparent</td> </tr> <tr> <td style="text-align: left"><strong>HBM</strong></td> <td style="text-align: left">40-80 GB</td> <td style="text-align: left">1.5-2 TB/s</td> <td style="text-align: left">~500 cycles</td> <td style="text-align: left">Explicit</td> </tr> </tbody> </table> <p>When we look at the GPU memory hierarchy in detail, we see a sharp difference in both latency and bandwidth across levels. Registers and shared memory sit close to the compute units and respond within a few cycles. HBM sits hundreds of cycles away with higher bandwidth but much higher latency. This separation means that the location of data often dictates runtime. As an example, the A100 GPU has 40GB of high bandwidth memory (HBM2e) with bandwidth 1.6TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s <d-cite key="nvidia2020a100"></d-cite>. This shows that the on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As L2 Cache is not directly controllable by the programmer we focus on the HBM and SRAM</p> <h3 id="attention-is-memory-bound-despite-on2-compute">Attention is memory bound despite $O(N^{2})$ compute</h3> <p>The efficiency of a kernel is governed by its arithmatic intensity, defined as the number of floating-point operations (FLOPs) performed per byte of memory access. Arithmatic Intensity is commonly used to measure whether operations can be classified as either compute-bound or memory-bound. A process is memory-bound when the execution speed is limited by how fast data can be moved between memory (HBM/RAM) and the processor cores, rather than by how fast the cores can compute. Typical examples include elementwise operations such as activation, dropout and reduction operations such as sum, softmax, batch norm, layer norm. A process is compute-bound when the execution speed is limited by the raw processing power (FLOPS) of the cores such as Matrix Multiplication, Convolution</p> \[\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}}\] <p>Standard Attention involves three primary stages</p> <ol> <li>Matrix Multiplication: $ S = Q K^T $</li> <li>Softmax: $ P = \text{softmax}(S) $</li> <li>Matrix Multiplication: $ O = P V $</li> </ol> <p>The matrix multiplications ($QK^T$ and $PV$) are compute-bound operations with high arithmetic intensity ($O(N^2 d)$ FLOPs vs $O(N^2)$ IO). However, the intermediate Softmax operation is memory-bound. It requires reading the entire $N \times N$ matrix $S$ from HBM, performing reduction operations, and writing the resulting $P$ matrix back to HBM. This $O(N^2)$ memory traffic saturates the HBM bandwidth, leaving the powerful Tensor Cores idle. (Detailed Proof provided in Appendix B)</p> <h2 id="flashattention---v1">FlashAttention - V1</h2> <p>One of the hardware-efficient mechanisms now widely adopted across different providers is Fast and Memory-Efficient Exact Attention with IO-Awareness, or FlashAttention. The “IO-Awareness” part of the title describes its core technical principle: optimizing data movement between GPU memory hierarchies.</p> <p>FlashAttention addresses the dual challenges of speed and memory consumption in transformers, especially on long sequences, by rethinking attention algorithms through the lens of GPU memory hierarchy awareness. The key insight is minimizing data movement between high-bandwidth memory (HBM) and on-chip SRAM.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_3-480.webp 480w,/assets/img/blog/Figure_3-800.webp 800w,/assets/img/blog/Figure_3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/Figure_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Standard execution loads data from HBM for every step. Kernel fusion and tiling keep data in fast memory longer, reducing memory traffic and improving throughput.</figcaption> </figure> <p>FlashAttention v1, published at Neural Information Processing Systems 2022 by Tri Dao and collaborators, introduced two key innovations: <strong>tiled</strong> attention that processes blocks of queries, keys, and values entirely in SRAM, and an <strong>online softmax algorithm</strong> that computes exact softmax incrementally without materializing the full attention matrix.</p> <h3 id="online-softmax-algorithm-enables-incremental-computation">Online Softmax Algorithm Enables Incremental Computation</h3> <p>Standard softmax computation requires three sequential passes over the data, making it inherently memory-intensive. The first pass finds the maximum value for numerical stability. The second pass computes exponentials and accumulates the normalization sum. The third pass normalizes each element. This three-pass structure can be expressed mathematically as follows:</p> <p>Given a vector $x \in \mathbb{R}^n$, the numerically stable softmax is computed in three passes:</p> \[\text{Pass 1:} \quad m = \max_{i} x_i\] \[\text{Pass 2:} \quad d = \sum_{i=1}^{n} e^{\,x_i - m}\] \[\text{Pass 3:} \quad \text{softmax}(x)_i = \frac{e^{\,x_i - m}}{d}\] <p>This approach ensures numerical stability by preventing overflow in the exponential computation. However, this three-pass dependency creates a critical bottleneck: we must materialize the entire attention matrix in HBM before proceeding. The softmax operation requires global information—specifically, the denominator in Pass 2 must sum over all $N$ elements. This seemingly requires loading the full row into memory before computing any output, making the process extremely memory I/O intensive and defeating attempts to tile the computation efficiently.</p> <p>The breakthrough came from <strong>online softmax</strong>, an algorithmic technique that computes softmax incrementally in blocks while maintaining running statistics for the maximum and the sum of exponentials. This method was originally discovered by Milakov and Gimelshein <d-cite key="milakov2018online"></d-cite> and later applied to sparse attention patterns by Child et al. <d-cite key="child2019generatinglongsequencessparse"></d-cite>. FlashAttention’s key innovation was adapting online softmax to work within a tiled attention algorithm, enabling exact softmax computation without materializing the full attention matrix. Instead of waiting for the global maximum $m_N$ across all elements before computing the normalization sum, the algorithm maintains running statistics that are updated as each new block of data is processed. This incremental approach transforms softmax into a streaming computation that can be fused with the surrounding matrix operations.</p> <h3 id="algebra-of-online-softmax">Algebra of Online Softmax</h3> <p>The elegance of online softmax lies in its ability to combine partial results without revisiting the raw data. In a streaming (block-wise) setting, we process the sequence in chunks. Let’s assume we have processed the first block of keys and have a local maximum $m_{old}$ and a local unnormalized sum $\ell_{old}$ (where $\ell = \sum e^{x_j - m}$). Now, we load a new block of keys and compute their raw scores. From this new block, we find a local maximum $m_{block}$ and a local sum $\ell_{block}$.</p> <p>For each block, we compute local statistics independently. The block size is chosen such that each block fits comfortably in SRAM, allowing us to process it entirely on-chip.</p> <p><strong>Running maximum</strong> (after processing new block):</p> \[m_{new} = \max(m_{old}, m_{block})\] <p>with initial condition: $m_{old} = -\infty$ for the first block</p> <p><strong>Running sum</strong> (relative to current maximum):</p> \[\ell_{new} = \sum_{j} e^{x_j - m_{new}}\] <p>with initial condition: $\ell_{old} = 0$ for the first block</p> <h4 id="recurrence-relation-for-the-sum">Recurrence Relation for the Sum</h4> <p>The critical mathematical insight is the <strong>rescaling formula</strong> for $\ell_{new}$. To combine the old and new blocks into a valid global state without accessing the old data, we need to express the new sum in terms of the old statistics.</p> <p>Starting from the definition:</p> \[\ell_{new} = \sum_{j \in \text{old}} e^{x_j - m_{new}} + \sum_{j \in \text{block}} e^{x_j - m_{new}}\] <p><strong>Key transformation:</strong> Express terms using the old maximum $m_{old}$:</p> \[e^{x_j - m_{new}} = e^{x_j - m_{old}} \cdot e^{m_{old} - m_{new}}\] <p>Therefore, for the old block terms:</p> \[\sum_{j \in \text{old}} e^{x_j - m_{new}} = e^{m_{old} - m_{new}} \sum_{j \in \text{old}} e^{x_j - m_{old}} = e^{m_{old} - m_{new}} \cdot \ell_{old}\] <p>Similarly, for the new block terms:</p> \[\sum_{j \in \text{block}} e^{x_j - m_{new}} = e^{m_{block} - m_{new}} \sum_{j \in \text{block}} e^{x_j - m_{block}} = e^{m_{block} - m_{new}} \cdot \ell_{block}\] <p>Combining both:</p> \[\boxed{\ell_{new} = e^{m_{old} - m_{new}} \cdot \ell_{old} + e^{m_{block} - m_{new}} \cdot \ell_{block}}\] <p>We can define the correction factor $\alpha = e^{m_{old} - m_{new}}$. This is the <strong>fundamental rescaling equation</strong> for online softmax. It allows us to update the running sum using only the previous statistics ($m_{old}$ and $\ell_{old}$) and the new block statistics, without ever loading the earlier blocks from HBM again. The exponential correction terms ensure numerical stability throughout the incremental computation by properly rescaling all exponentials relative to the current global maximum. This reduces the operation from 3 passes to 2 passes <d-cite key="dukhan2020two"> </d-cite></p> <h3 id="flashattention--forward-pass">FlashAttention : Forward Pass</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_4-480.webp 480w,/assets/img/blog/Figure_4-800.webp 800w,/assets/img/blog/Figure_4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/Figure_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Algorithm for FlashAttention Forward Pass. The algorithm partitions inputs into blocks that fit in SRAM, computes attention incrementally using online softmax, and updates running statistics to avoid materializing the full attention matrix in HBM.</figcaption> </figure> <p>For each attention head, FlashAttention reduces memory reads and writes by tiling. It loads small blocks of queries, keys, and values from GPU HBM into fast on-chip SRAM, computes attention for that block, and updates the output before moving on to the next block. This limits how often data moves between slow and fast memory, which is the main bottleneck on modern GPUs. Cutting this movement often gives a 2–4× speedup in practice.</p> <h4 id="mathematical-derivation">Mathematical Derivation</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_5-480.webp 480w,/assets/img/blog/Figure_5-800.webp 800w,/assets/img/blog/Figure_5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/Figure_5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We define the block sizes based on the available SRAM size $M$. Let $B_c$ be the block size for columns (dimension along $N$ for $K, V$), and let $B_r$ be the block size for rows (dimension along $N$ for $Q, O$). The key constraint is $4 B_c d \le M$ to ensure that $K, V$ blocks and various buffers fit in SRAM. Usually, we set $B_c \approx \lceil \frac{M}{4d} \rceil$ and $B_r \approx \min(\lceil \frac{M}{4d} \rceil, d)$.</p> <p>The matrices are divided into blocks as follows:</p> <ul> <li>$Q$ is divided into $T_r = \lceil N/B_r \rceil$ blocks: $Q_1, \dots, Q_{T_r}$</li> <li>$K$ and $V$ are divided into $T_c = \lceil N/B_c \rceil$ blocks: $K_1, \dots, K_{T_c}$ and $V_1, \dots, V_{T_c}$</li> <li>$O$ is divided into $T_r$ blocks: $O_1, \dots, O_{T_r}$</li> </ul> <p>Ideally, to minimize HBM writes of the output $O$, we want to load a block of $Q$, iterate over all blocks of $K, V$, accumulating the result, and then write $O$ once. However, FlashAttention V1 actually uses an outer loop over $K, V$ and an inner loop over $Q$ to better utilize the SRAM for the larger $K, V$ blocks, though conceptually the accumulation happens per query row.</p> <p><strong>Initialization:</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_6-480.webp 480w,/assets/img/blog/Figure_6-800.webp 800w,/assets/img/blog/Figure_6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/Figure_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Q, K, and V blocks are stored in HBM (blue). Each K and V block is streamed into SRAM, where partial score matrices S(t) and exponentials A(t) are computed (orange). Softmax normalization is accumulated across blocks, and partial outputs O(t) are merged to form the final output.</figcaption> </figure> <p>We initialize the output matrix $O = 0 \in \mathbb{R}^{N \times d}$, the running sum vector $\ell = 0 \in \mathbb{R}^N$, and the running maximum vector $m = -\infty \in \mathbb{R}^N$. These statistics will be updated incrementally as we process each block.</p> <p><strong>Outer Loop</strong> (iterating over $K, V$ blocks, $j = 1, \dots, T_c$):</p> <ol> <li> <p><strong>Load Key-Value blocks:</strong> Load $K_j, V_j \in \mathbb{R}^{B_c \times d}$ from HBM to on-chip SRAM. These blocks remain in SRAM throughout the inner loop.</p> <p><strong>Inner Loop</strong> (iterating over $Q$ blocks, $i = 1, \dots, T_r$):</p> <ol> <li> <p><strong>Load Query block and statistics:</strong> Load $Q_i \in \mathbb{R}^{B_r \times d}$, $O_i \in \mathbb{R}^{B_r \times d}$, $\ell_i \in \mathbb{R}^{B_r}$, $m_i \in \mathbb{R}^{B_r}$ from HBM to SRAM.</p> </li> <li> <p><strong>Compute attention scores:</strong> Compute $S_{ij} = Q_i K_j^\top \in \mathbb{R}^{B_r \times B_c}$. This matrix multiplication is performed entirely in SRAM, computing the raw attention scores between the current query block and key block.</p> </li> <li> <p><strong>Compute local statistics:</strong> For the current block, we compute:</p> \[\tilde{m}_{ij} = \text{rowmax}(S_{ij}) \in \mathbb{R}^{B_r}\] <p>(maximum score in each row)</p> \[\tilde{P}_{ij} = \exp(S_{ij} - \tilde{m}_{ij}) \in \mathbb{R}^{B_r \times B_c}\] <p>(pointwise exponential with local normalization)</p> \[\tilde{\ell}_{ij} = \text{rowsum}(\tilde{P}_{ij}) \in \mathbb{R}^{B_r}\] <p>(sum of exponentials in each row)</p> </li> <li> <p><strong>Update running statistics:</strong> Using the online softmax rescaling formula:</p> \[m_i^{\text{new}} = \max(m_i, \tilde{m}_{ij})\] <p>(update global maximum)</p> \[\ell_i^{\text{new}} = e^{m_i - m_i^{\text{new}}} \ell_i + e^{\tilde{m}_{ij} - m_i^{\text{new}}} \tilde{\ell}_{ij}\] <p>(rescale and accumulate sum)</p> </li> <li> <p><strong>Update output:</strong> We incrementally update the attention output. First, compute partial output contribution:</p> \[\tilde{V}_{ij} = \tilde{P}_{ij} V_j\] <p>(weighted sum of values for current block)</p> <p>Then combine with running output using the rescaling formula:</p> \[O_i^{\text{new}} = \text{diag}(\ell_i^{\text{new}})^{-1} \left( \text{diag}(\ell_i) e^{m_i - m_i^{\text{new}}} O_i + e^{\tilde{m}_{ij} - m_i^{\text{new}}} \tilde{V}_{ij} \right)\] <p>This rescales the old output and adds the new contribution, then normalizes by the updated sum.</p> </li> <li> <p><strong>Write back to HBM:</strong> Write $O_i^{\text{new}}, \ell_i^{\text{new}}, m_i^{\text{new}}$ back to HBM, updating the global state for this query block.</p> </li> </ol> </li> </ol> <p>In practice, to avoid numerical instability from dividing by $\ell_i$ at every step, the algorithm often stores the unnormalized output (let’s call it $U_i = O_i \cdot \ell_i$) and only divides by $\ell_i$ at the very end of the computation or maintains the invariant correctly. The formulation above effectively rescales the previous running average to match the new magnitude determined by the new maximum.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_7-480.webp 480w,/assets/img/blog/Figure_7-800.webp 800w,/assets/img/blog/Figure_7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/Figure_7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Blockwise computation of attention using online softmax. Q remains in HBM while K and V are streamed in blocks. For each block, partial scores S(t) and exponentials A(t) are computed in SRAM. The running softmax denominator is updated across blocks, and partial outputs O(t) are rescaled and accumulated to form the final output.</figcaption> </figure> <h3 id="complexity-analysis">Complexity Analysis</h3> <p>The efficiency of FlashAttention is theoretically grounded in its IO complexity. We analyze the number of HBM accesses required by comparing standard attention with FlashAttention.</p> <h4 id="standard-attention-io-complexity">Standard Attention IO Complexity</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Figure_8-480.webp 480w,/assets/img/blog/Figure_8-800.webp 800w,/assets/img/blog/Figure_8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/Figure_8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>For standard attention, the HBM accesses are:</p> <ul> <li>Read $Q, K, V$: $O(Nd)$</li> <li>Write $S = QK^T$: $O(N^2)$</li> <li>Read $S$: $O(N^2)$</li> <li>Write $P = \text{softmax}(S)$: $O(N^2)$</li> <li>Read $P$ and $V$: $O(N^2) + O(Nd)$</li> <li>Write $O = PV$: $O(Nd)$</li> </ul> <p><strong>Total:</strong> $\Theta(N^2)$ HBM accesses (assuming $N \gg d$)</p> <h4 id="flashattention-io-complexity">FlashAttention IO Complexity</h4> <p>FlashAttention’s tiled algorithm significantly reduces memory traffic:</p> <p><strong>Outer loop</strong> (over $K, V$ blocks, $j = 1, \dots, T_c$): Each iteration loads blocks $K_j, V_j$ into SRAM. Since the outer loop iterates over all $K, V$ blocks, these matrices are loaded once in total: $O(Nd)$.</p> <p><strong>Inner loop</strong> (over $Q$ blocks, $i = 1, \dots, T_r$): For each outer iteration, the inner loop loads $Q_i$, $O_i$, $\ell_i$, $m_i$ from HBM.</p> <ul> <li>Size of $Q_i$: $B_r \times d$</li> <li>Total inner loop iterations: $T_c \times T_r$</li> <li>Total loading of $Q$: $T_c \times (T_r \times B_r d) = T_c \times Nd$</li> </ul> <p>Since $T_c = \lceil N/B_c \rceil$ and $B_c = \Theta(M/d)$:</p> \[\text{Total } Q \text{ loads} = \frac{N}{B_c} \times Nd = \frac{N}{M/d} \times Nd = \frac{N^2 d^2}{M}\] <p><strong>Theorem:</strong> For sequence length $N$, head dimension $d$, and SRAM size $M$, FlashAttention requires $O(N^2 d^2 M^{-1})$ HBM accesses.</p> <p>Since $d^2$ is typically much smaller than $M$ (e.g., $d=64 \implies d^2=4096$, while $M \approx 10^5$ bytes), the ratio $d^2/M \ll 1$. Thus, FlashAttention provides a significant reduction in HBM accesses compared to standard attention’s $O(N^2)$ complexity.</p> <h4 id="lower-bound-and-optimality">Lower Bound and Optimality</h4> <p>Dao et al. prove that this complexity is <strong>asymptotically optimal</strong>. <d-cite key="dao2022flashattentionfastmemoryefficientexact"></d-cite> The proof relies on the “Red-Blue Pebble Game,” a standard model for analyzing memory hierarchy complexity <d-cite key="demaine2018red"></d-cite>. The attention computation graph involves computing $N^2$ pairwise interactions. To compute these interactions with a limited cache of size $M$, any algorithm must re-stream inputs multiple times. It has been proven that any algorithm computing exact attention must incur $\Omega(N^2 d^2 M^{-1})$ memory accesses. This confirms that FlashAttention is not just an improvement but <strong>IO-optimal</strong> for exact attention computation.</p> <h4 id="complexity-comparison-with-standard-attention">Complexity Comparison with Standard Attention</h4> <table> <thead> <tr> <th style="text-align: left">Metric</th> <th style="text-align: left">Standard Attention</th> <th style="text-align: left">FlashAttention</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Time Complexity (FLOPs)</strong></td> <td style="text-align: left">$O(N^2 d)$</td> <td style="text-align: left">$O(N^2 d)$</td> </tr> <tr> <td style="text-align: left"><strong>Space Complexity (Memory)</strong></td> <td style="text-align: left">$O(N^2)$ (stores $S, P$)</td> <td style="text-align: left">$O(N)$ (stores $O, \ell, m$)</td> </tr> <tr> <td style="text-align: left"><strong>IO Complexity (HBM Access)</strong></td> <td style="text-align: left">$O(N^2)$</td> <td style="text-align: left">$O(N^2 d^2 M^{-1})$</td> </tr> </tbody> </table> <h3 id="flashattention-backward-pass">FlashAttention: Backward Pass</h3> <p>Training deep models requires a backward pass to compute gradients. Standard backpropagation requires the stored attention probability matrix $P$ (size $N \times N$) to compute gradients with respect to $Q$ and $K$. Storing $P$ for long sequences is prohibitively expensive ($O(N^2)$ memory).</p> <p>FlashAttention solves this through <strong>recomputation</strong>. Instead of saving $P$, it saves only the final output $O$ and the normalization statistics $(\ell, m)$ from the forward pass—both of size $N \times 1$, plus the random seed for dropout. During the backward pass, the kernel reloads $Q$, $K$, $V$ from HBM and uses $m$ and $\ell$ to regenerate the attention scores $S$ and probabilities $P$ block-by-block in SRAM, exactly as they were computed in the forward pass.</p> <p>While this recomputation increases FLOPs by repeating the forward matrix multiplications, the reduction in HBM reads—avoiding $O(N^2)$ reads of $P$—results in a net speedup because the operation is memory-bound. The additional compute cost is more than offset by the savings in memory bandwidth.</p> <h4 id="backward-pass-derivation">Backward Pass Derivation</h4> <p>The backward pass must compute gradients $d\mathbf{Q}$, $d\mathbf{K}$, $d\mathbf{V}$ given $d\mathbf{O}$ from the loss. Standard backprop saves the $N \times N$ attention matrix $\mathbf{P}$ from the forward pass. FlashAttention instead saves only $\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O} \in \mathbb{R}^{N \times d}$ and the log-sum-exp statistics $\mathbf{L} \in \mathbb{R}^N$ where $L_i = m_i + \log(\ell_i)$. The $O(N^2)$ matrices $\mathbf{S} = \mathbf{QK}^T$ and $\mathbf{P} = \text{softmax}(\mathbf{S})$ are recomputed on-the-fly in SRAM during backprop.</p> <p><strong>Gradient computation.</strong> Recall the forward equations:</p> \[\mathbf{S} = \mathbf{QK}^T, \quad \mathbf{P} = \text{softmax}(\mathbf{S}), \quad \mathbf{O} = \mathbf{PV}\] <p>Backpropagating through this chain via $\mathcal{L} \xleftarrow{\text{grad}} \mathbf{O} \xleftarrow{\mathbf{P,V}} \mathbf{P} \xleftarrow{\text{softmax}} \mathbf{S} \xleftarrow{\mathbf{Q,K}} \mathbf{Q}, \mathbf{K}$ gives:</p> <p><strong>Gradient w.r.t. $\mathbf{V}$:</strong> From $\mathbf{O} = \mathbf{PV}$,</p> \[d\mathbf{V} = \mathbf{P}^T d\mathbf{O}\] <p><strong>Gradient w.r.t. $\mathbf{P}$:</strong></p> \[d\mathbf{P} = d\mathbf{O} \mathbf{V}^T \in \mathbb{R}^{N \times N}\] <p><strong>Gradient through softmax.</strong> For row-wise softmax, the Jacobian is $\frac{\partial P_{ij}}{\partial S_{ik}} = P_{ij}(\delta_{jk} - P_{ik})$ where $\delta$ is the Kronecker delta. Applying the chain rule, for each row $i$:</p> \[d\mathbf{S}_i = \mathbf{P}_i \odot d\mathbf{P}_i - \mathbf{P}_i (d\mathbf{P}_i^\top \mathbf{P}_i)\] <p>where $\odot$ denotes element-wise multiplication.</p> <p>Define $D_i = d\mathbf{P}_{i:} \cdot \mathbf{P}_{i:} = \sum_j d\mathbf{P}_{ij} \mathbf{P}_{ij}$. Since $d\mathbf{P}_{ij} = (d\mathbf{O}_i)^\top \mathbf{V}_j$ and $\sum_j \mathbf{P}_{ij} \mathbf{V}_j = \mathbf{O}_i$:</p> \[D_i = \sum_j (d\mathbf{O}_i^\top \mathbf{V}_j) \mathbf{P}_{ij} = d\mathbf{O}_i^\top \mathbf{O}_i\] <p>Thus the gradient through softmax becomes:</p> \[d\mathbf{S} = \mathbf{P} \odot d\mathbf{P} - \mathbf{P} \odot (d\mathbf{O} \odot \mathbf{O})\] <p><strong>Gradients w.r.t. $\mathbf{Q}$ and $\mathbf{K}$.</strong> Since $\mathbf{S} = \mathbf{QK}^T$:</p> \[d\mathbf{Q} = d\mathbf{S} \mathbf{K} \in \mathbb{R}^{N \times d}\] \[d\mathbf{K} = d\mathbf{S}^T \mathbf{Q} \in \mathbb{R}^{N \times d}\] <p><strong>Tiled backward pass with recomputation.</strong> FlashAttention avoids storing $O(N^2)$ attention matrices $\mathbf{P}$ and $\mathbf{S}$ by recomputing them block-by-block in SRAM during the backward pass. This strategy exchanges additional compute for reduced memory requirements. The approach is practical because attention computation is memory-bound—the memory bandwidth savings outweigh the recomputation cost.</p> <h4 id="backward-pass-setup">Backward Pass Setup</h4> <p><strong>Saved state from forward pass:</strong></p> <p>From the forward pass, we save only the minimal information required:</p> <ul> <li>$\mathbf{Q}, \mathbf{K}, \mathbf{V}$ (all in HBM, size $N \times d$ each)</li> <li>$\mathbf{O}$ (output, size $N \times d$)</li> <li>Log-sum-exp statistics: $\mathbf{L} \in \mathbb{R}^N$ where $L_i = m_i + \log(\ell_i)$</li> <li>Dropout random seed (if used)</li> </ul> <p><strong>Preprocessing step:</strong></p> <p>Before the main loop, we compute an auxiliary vector $\mathbf{D} \in \mathbb{R}^N$:</p> \[D_i = \sum_{j=1}^d dO_{ij} \cdot O_{ij}\] <p>This vector encodes the diagonal of $(dO \odot O)$ and is reused throughout the backward pass to efficiently compute gradients through softmax for each block. Computing $\mathbf{D}$ requires one pass over $dO$ and $O$, costing $O(Nd)$ HBM accesses.</p> <h4 id="backward-algorithm-flashattention-v1">Backward Algorithm (FlashAttention V1)</h4> <p>The backward pass uses the same tiling structure as the forward pass. We iterate over $K, V$ blocks in the outer loop and $Q$ blocks in the inner loop, accumulating gradients into each block as we process them.</p> <p><strong>Outer Loop</strong> (over $K, V$ blocks, $j = 1, \dots, T_c$):</p> <ol> <li> <p><strong>Load key-value blocks:</strong> Load $\mathbf{K}_j, \mathbf{V}_j \in \mathbb{R}^{B_c \times d}$ from HBM to SRAM.</p> </li> <li> <p><strong>Initialize gradient accumulators:</strong> Set $d\mathbf{K}_j = 0$ and $d\mathbf{V}_j = 0$ in SRAM. These accumulators will receive contributions from all query blocks in the inner loop.</p> <p><strong>Inner Loop</strong> (over $Q$ blocks, $i = 1, \dots, T_r$):</p> <ol> <li><strong>Load query block and statistics:</strong> Load from HBM into SRAM: <ul> <li>$\mathbf{Q}_i \in \mathbb{R}^{B_r \times d}$</li> <li>$d\mathbf{Q}_i \in \mathbb{R}^{B_r \times d}$ (partial gradient, accumulated across $j$ iterations)</li> <li>$d\mathbf{O}_i \in \mathbb{R}^{B_r \times d}$ (upstream gradient from backprop)</li> <li>$\mathbf{O}_i \in \mathbb{R}^{B_r \times d}$ (output from forward pass)</li> <li>Statistics: $\mathbf{L}_i, m_i, D_i$ (all size $B_r$)</li> </ul> </li> <li> <p><strong>Recompute attention scores and probabilities:</strong></p> \[\mathbf{S}_{ij} = \mathbf{Q}_i \mathbf{K}_j^\top \in \mathbb{R}^{B_r \times B_c}\] <p>Recover the attention probabilities using the saved log-sum-exp statistics:</p> \[\mathbf{P}_{ij} = \exp(\mathbf{S}_{ij} - m_i) / \exp(\mathbf{L}_i - m_i)\] <p>where subtraction and division are applied row-wise with broadcasting. If dropout was applied during the forward pass, regenerate the dropout mask using the saved random seed and apply it to $\mathbf{P}_{ij}$.</p> </li> <li> <p><strong>Compute value gradients:</strong> Accumulate into $d\mathbf{V}_j$:</p> \[d\mathbf{V}_j \leftarrow d\mathbf{V}_j + \mathbf{P}_{ij}^\top d\mathbf{O}_i\] <p>This is a $(B_c \times B_r) \times (B_r \times d) \to (B_c \times d)$ matrix multiplication.</p> </li> <li> <p><strong>Compute score gradients through softmax:</strong> The gradient flowing backward through softmax is:</p> \[d\mathbf{S}_{ij} = \mathbf{P}_{ij} \odot \left( d\mathbf{O}_i \mathbf{V}_j^\top - \mathbf{P}_{ij}^\top (d\mathbf{O}_i \odot \mathbf{O}_i) \right)\] <p>Computing this requires first forming the intermediate matrix:</p> \[\mathbf{A}_{ij} = d\mathbf{O}_i \mathbf{V}_j^\top\] <p>Then applying the softmax gradient formula:</p> \[d\mathbf{S}_{ij} = \mathbf{P}_{ij} \odot \left( \mathbf{A}_{ij} - D_i \right)\] <p>where $D_i$ is broadcast across the column dimension.</p> </li> <li> <p><strong>Compute query gradients:</strong> Accumulate into $d\mathbf{Q}_i$:</p> \[d\mathbf{Q}_i \leftarrow d\mathbf{Q}_i + d\mathbf{S}_{ij} \mathbf{K}_j\] <p>This is $(B_r \times B_c) \times (B_c \times d) \to (B_r \times d)$.</p> </li> <li> <p><strong>Compute key gradients:</strong> Accumulate into $d\mathbf{K}_j$:</p> \[d\mathbf{K}_j \leftarrow d\mathbf{K}_j + d\mathbf{S}_{ij}^\top \mathbf{Q}_i\] <p>This is $(B_c \times B_r) \times (B_r \times d) \to (B_c \times d)$.</p> </li> <li><strong>Write query gradients back:</strong> Write updated $d\mathbf{Q}_i$ to HBM. When the outer loop is parallelized across GPU threads or blocks, this requires atomic operations to safely accumulate contributions from multiple $j$ values into the same $d\mathbf{Q}_i$. In sequential execution, this is a standard write operation.</li> </ol> <p><strong>End Inner Loop</strong></p> </li> <li> <p><strong>Write key-value gradients:</strong> After processing all query blocks, write the accumulated gradients to HBM:</p> \[\text{Write } d\mathbf{K}_j, d\mathbf{V}_j \text{ to HBM}\] </li> </ol> <p><strong>End Outer Loop</strong></p> <h4 id="backward-pass-complexity-analysis">Backward Pass Complexity Analysis</h4> <p><strong>Floating-point operations:</strong></p> <p>The backward pass requires recomputing the attention matrices and computing all gradient operations. The FLOP count is:</p> <ul> <li>Recomputing $\mathbf{S}_{ij} = \mathbf{Q}_i \mathbf{K}_j^\top$: $T_c \times T_r \times 2 B_r B_c d = 2 N^2 d$ FLOPs</li> <li>Gradient computations (matrix multiplies for $dV, dQ, dK$): $2 N^2 d$ FLOPs</li> <li><strong>Total backward FLOPs:</strong> $\Theta(N^2 d)$, equivalent to the forward pass</li> </ul> <p><strong>HBM access (IO complexity):</strong></p> <ul> <li>Load $Q, K, V$ for recomputation: $O(N^2 d^2 / M)$ (same scaling as forward pass)</li> <li>Load $dO, O$ (single pass): $O(Nd)$</li> <li>Load and write statistics $L, m, D$: $O(N)$</li> <li>Write $dQ, dK, dV$: $O(Nd)$</li> </ul> <p><strong>Total backward IO:</strong> $\Theta(N^2 d^2 / M)$ HBM accesses</p> <p>This matches the forward pass complexity. Critically, we avoid storing and reading the $O(N^2)$ attention matrix, which is the dominant memory cost in standard backpropagation.</p> <h4 id="comparison-standard-backpropagation-vs-flashattention">Comparison: Standard Backpropagation vs. FlashAttention</h4> <table> <thead> <tr> <th style="text-align: left">Aspect</th> <th style="text-align: left">Standard Backprop</th> <th style="text-align: left">FlashAttention</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Saved state</strong></td> <td style="text-align: left">$Q, K, V, P, S$</td> <td style="text-align: left">$Q, K, V, O, L, m$</td> </tr> <tr> <td style="text-align: left"><strong>Memory footprint</strong></td> <td style="text-align: left">$O(Nd + 2N^2)$</td> <td style="text-align: left">$O(Nd)$</td> </tr> <tr> <td style="text-align: left"><strong>Backward recomputation</strong></td> <td style="text-align: left">None</td> <td style="text-align: left">$S, P$ blocks on-the-fly</td> </tr> <tr> <td style="text-align: left"><strong>HBM reads in backward</strong></td> <td style="text-align: left">$\Theta(N^2)$ from $P$</td> <td style="text-align: left">$\Theta(N^2 d^2 / M)$</td> </tr> <tr> <td style="text-align: left"><strong>Backward FLOPs</strong></td> <td style="text-align: left">$\Theta(N^2 d)$</td> <td style="text-align: left">$\Theta(N^2 d)$</td> </tr> </tbody> </table> <p>The memory savings of $2N^2$ elements directly offset the recomputation cost of $\Theta(N^2 d)$ FLOPs. On memory-bound hardware where the memory bus saturates before compute cores reach full utilization, this trade-off consistently improves overall throughput.</p> <p><strong>Preprocessing step:</strong></p> <p>Before the main loop, compute a single auxiliary vector $\mathbf{D} \in \mathbb{R}^N$:</p> \[D_i = \sum_{j=1}^d dO_{ij} \cdot O_{ij}\] <p>This vector encodes the diagonal of $(dO \odot O)$ and is reused to efficiently compute gradients through softmax for each block. Computing $\mathbf{D}$ requires one HBM pass over $dO$ and $O$, costing $O(Nd)$ HBM accesses.</p> <h4 id="backward-algorithm-flashattention-v1-1">Backward Algorithm (FlashAttention V1)</h4> <p>The backward pass mirrors the forward tiling structure but with reversed loop order. We iterate over $K,V$ blocks in the outer loop and $Q$ blocks in the inner loop, accumulating gradients into each block.</p> <p><strong>Outer Loop</strong> (over $K, V$ blocks, $j = 1, \dots, T_c$):</p> <ol> <li> <p><strong>Load key-value blocks:</strong> Load $\mathbf{K}_j, \mathbf{V}_j \in \mathbb{R}^{B_c \times d}$ from HBM to SRAM.</p> </li> <li> <p><strong>Initialize gradient accumulators:</strong> Set $d\mathbf{K}_j = 0 \in \mathbb{R}^{B_c \times d}$ and $d\mathbf{V}_j = 0 \in \mathbb{R}^{B_c \times d}$ in SRAM.</p> <p><strong>Inner Loop</strong> (over $Q$ blocks, $i = 1, \dots, T_r$):</p> <ol> <li><strong>Load query blocks and gradients:</strong> Load the following from HBM into SRAM: <ul> <li>$\mathbf{Q}_i \in \mathbb{R}^{B_r \times d}$</li> <li>$d\mathbf{Q}_i \in \mathbb{R}^{B_r \times d}$ (partial gradient, accumulated across $j$ iterations)</li> <li>$d\mathbf{O}_i \in \mathbb{R}^{B_r \times d}$ (gradient from next layer)</li> <li>$\mathbf{O}_i \in \mathbb{R}^{B_r \times d}$ (output from forward pass)</li> <li>Statistics: $\mathbf{L}_i, m_i, D_i$ (all size $B_r$)</li> </ul> </li> <li> <p><strong>Recompute attention scores and probabilities:</strong></p> \[\mathbf{S}_{ij} = \mathbf{Q}_i \mathbf{K}_j^\top \in \mathbb{R}^{B_r \times B_c}\] <p>Compute local exponents using the saved maximum and log-sum-exp:</p> \[\mathbf{P}_{ij} = \exp(\mathbf{S}_{ij} - m_i) / \exp(\mathbf{L}_i - m_i)\] <p>where division and subtraction are row-wise (broadcasting $m_i$ and $L_i$ across columns). If dropout was applied during the forward pass, re-generate the dropout mask using the same random seed and apply it to $\mathbf{P}_{ij}$.</p> </li> <li> <p><strong>Compute value gradients:</strong> Accumulate into $d\mathbf{V}_j$:</p> \[d\mathbf{V}_j \leftarrow d\mathbf{V}_j + \mathbf{P}_{ij}^\top d\mathbf{O}_i\] <p>This is a $(B_c \times B_r) \times (B_r \times d) \to (B_c \times d)$ matrix multiplication.</p> </li> <li> <p><strong>Compute score gradients via softmax chain rule:</strong> The gradient through the softmax operation is:</p> \[d\mathbf{S}_{ij} = \mathbf{P}_{ij} \odot \left( d\mathbf{O}_i \mathbf{V}_j^\top - \mathbf{P}_{ij}^\top (d\mathbf{O}_i \odot \mathbf{O}_i) \right)\] <p>Expanding this: first compute the intermediate $(B_r \times B_c)$ matrix:</p> \[\mathbf{A}_{ij} = d\mathbf{O}_i \mathbf{V}_j^\top\] <p>Then for each row $i$, compute the row-wise sum weighted by $\mathbf{P}_{ij}$:</p> \[d\mathbf{S}_{ij} = \mathbf{P}_{ij} \odot \left( \mathbf{A}_{ij} - D_i \right)\] <p>where $D_i$ is broadcast across the columns of the subtraction.</p> </li> <li> <p><strong>Compute query gradients:</strong> Accumulate into $d\mathbf{Q}_i$:</p> \[d\mathbf{Q}_i \leftarrow d\mathbf{Q}_i + d\mathbf{S}_{ij} \mathbf{K}_j\] <p>This is $(B_r \times B_c) \times (B_c \times d) \to (B_r \times d)$.</p> </li> <li> <p><strong>Compute key gradients:</strong> Accumulate into $d\mathbf{K}_j$:</p> \[d\mathbf{K}_j \leftarrow d\mathbf{K}_j + d\mathbf{S}_{ij}^\top \mathbf{Q}_i\] <p>This is $(B_c \times B_r) \times (B_r \times d) \to (B_c \times d)$.</p> </li> <li><strong>Write query gradients back:</strong> Write updated $d\mathbf{Q}_i$ to HBM. If the outer loop is parallelized across different GPU threads or blocks, this requires an atomic addition (since multiple $j$ values accumulate into the same $d\mathbf{Q}_i$). In sequential execution, a simple write suffices.</li> </ol> <p><strong>End Inner Loop</strong></p> </li> <li> <p><strong>Write key-value gradients:</strong> After the inner loop completes (all $Q$ blocks have been processed), write the accumulated gradients to HBM:</p> \[\text{Write } d\mathbf{K}_j, d\mathbf{V}_j \text{ to HBM}\] </li> </ol> <p><strong>End Outer Loop</strong></p> <h4 id="backward-pass-complexity-analysis-1">Backward Pass Complexity Analysis</h4> <p><strong>FLOP count:</strong></p> <ul> <li>Recomputing $\mathbf{S}_{ij} = \mathbf{Q}_i \mathbf{K}_j^\top$: $T_c \times T_r \times 2 B_r B_c d = 2 N^2 d$ FLOPs (same as forward)</li> <li>Gradient computations (matrix multiplies for $dV, dQ, dK$): $2 N^2 d$ FLOPs</li> <li><strong>Total backward FLOPs:</strong> $\Theta(N^2 d)$, same as forward pass.</li> </ul> <p><strong>HBM access (IO complexity):</strong></p> <ul> <li>Load $Q, K, V$ for recomputation: $T_c \times Nd = O(N^2 d^2 / M)$ (same scaling as forward)</li> <li>Load $dO, O$ (once per sequence): $O(Nd)$</li> <li>Load/write statistics $L, m, D$ (once): $O(N)$</li> <li>Write $dQ, dK, dV$: $O(Nd)$</li> </ul> <p><strong>Total backward IO:</strong> $\Theta(N^2 d^2 / M)$ HBM accesses</p> <p>This is the same asymptotic complexity as the forward pass. Crucially, we <strong>avoid storing the $O(N^2)$ attention matrix</strong>, saving the dominant memory footprint.</p> <h4 id="comparison-standard-vs-flashattention-backward-pass">Comparison: Standard vs. FlashAttention Backward Pass</h4> <table> <thead> <tr> <th style="text-align: left">Aspect</th> <th style="text-align: left">Standard Backprop</th> <th style="text-align: left">FlashAttention</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Saved state</strong></td> <td style="text-align: left">$Q, K, V, P, S$</td> <td style="text-align: left">$Q, K, V, O, L, m$</td> </tr> <tr> <td style="text-align: left"><strong>Saved memory</strong></td> <td style="text-align: left">$O(Nd + 2N^2)$</td> <td style="text-align: left">$O(Nd)$</td> </tr> <tr> <td style="text-align: left"><strong>Backward recomputes</strong></td> <td style="text-align: left">Nothing</td> <td style="text-align: left">$S, P$ (on-the-fly)</td> </tr> <tr> <td style="text-align: left"><strong>Backward IO</strong></td> <td style="text-align: left">$\Theta(N^2)$ reads of $P$</td> <td style="text-align: left">$\Theta(N^2 d^2 / M)$</td> </tr> <tr> <td style="text-align: left"><strong>Backward FLOPs</strong></td> <td style="text-align: left">$\Theta(N^2 d)$</td> <td style="text-align: left">$\Theta(N^2 d)$ (same)</td> </tr> </tbody> </table> <p>The memory saved ($2N^2$ elements) outweighs the extra compute cost (recomputing $\Theta(N^2 d)$ FLOPs), resulting in a net speedup on memory-bound hardware.</p> <h2 id="flashattention-v2">FlashAttention V2</h2> <h3 id="background-1">Background</h3> <p>Despite the success of v1, analysis revealed that it still achieved only 30-50% of the theoretical maximum FLOPs of the A100 GPU. The two primary causes were suboptimal parallelism leading to low occupancy and inefficient work partioning.</p> <h4 id="parallelism-bottleneck">Parallelism Bottleneck</h4> <p>FlashAttention V1 parallelized the computaton over batch size (B) and the number of heads (H). This means the total number of thread blocks launched is $B x H$. In many long context scenarios, to fit the model in memory, the batch size is reduced (e.g B=1). If the number of heads is also small (e.g 12 or 32), the GPU may only launch 12-32 thread blocks. To put things into perspective, an NVIDIA A100 has 108 SMs; if only 32 thread blocks are launched over 70% of the GPUs compute resources will sit idlt. THis is known as Low occupancy</p> <p>Furthermore, FlashAttention V1’s online softmax performs rescaling at every iteration which is a non-matmul operation. On modern GPUs with Tensor Cores, non-matmul operations are 16x more expensive per FLOP than matrix multiplications. Ex - The A100 delivery 312 TFLOPS for FP16 matmul but only 19.5 TFLOPS for other FP32 operations.</p> <h3 id="improvements-over-fa1">Improvements over FA1</h3> <p>FlashAttention V2 released in July 2023, achieves a ~2x speedup over V1 by addressing architectural inefficiencies invisible in the algorithm’s complexity analysis. The paper identified two key limitations of v1: 1. each head or block is processed largely serially by one threadblock, so GPUs with many SMs were underutilized; (2) within a block, warps did unncessary communication.</p> <h4 id="optimization-1-reducing-non-matmul-flops">Optimization 1: Reducing Non-Matmul FLOPs</h4> <p>FlashAttention-2 introduces algebraic simplifications to minimize non-matrix-multiply operations, which are significantly slower than matrix multiplications on Tensor Core GPUs. While matrix multiplications (GEMMs) run on Tensor Cores (very fast), operations like exp, sum, max and divison run on the Special Function Units (SFUs) or CUDA cores (slower)</p> <p><strong>1. Deferring Normalization:</strong> In FlashAttention-1, the output matrix $O$ is rescaled at every iteration to maintain numerical stability: \(O^{\text{new}} = \text{diag}(\ell^{\text{new}})^{-1} \left( \text{diag}(\ell) e^{m - m^{\text{new}}} O + e^{\tilde{m} - m^{\text{new}}} \tilde{V} \right)\) This requires performing vector-matrix divisions at every step. FlashAttention-2 instead maintains an <strong>un-normalized</strong> output accumulator $\tilde{O}$ throughout the loop: \(\tilde{O}^{\text{new}} = \text{diag}(e^{m - m^{\text{new}}}) \tilde{O} + e^{\tilde{m} - m^{\text{new}}} \tilde{V}\) The expensive division operation is performed only once at the very end of the loop: $O = \text{diag}(\ell^{\text{final}})^{-1} \tilde{O}$. This simple reordering significantly reduces the number of non-matmul FLOPs.</p> <p><strong>2. LogSumExp Storage:</strong> To further reduce memory overhead, FlashAttention-2 changes the statistics stored for the backward pass. Instead of storing both the maximum $m$ and the sum of exponentials $\ell$, it stores a single log-sum-exp value $L$: \(L = m + \log(\ell)\) During the backward pass, the required statistics can be derived as $\ell = \exp(L - m)$. This halves the memory footprint for metadata, allowing for larger block sizes and better occupancy.</p> <h3 id="work-partitioning-between-warps">Work Partitioning Between Warps</h3> <p>A major innovation in v2 is parallelizing across the sequence dimension. Instead of one threadblock handling one $(i,j)$ pair at a time, FlashAttention-2 splits the work of a single attention head into multiple threadblocks and warps. For example, one can process different row-blocks $i$ (or even parts of a block) in parallel. The core idea is warp specialization: some warps (or threadblocks) act as producers (loading next $K/V$ block via TMA, the Tensor Memory Accelerator), while others act as consumers (doing GEMM/softmax on the currently loaded data). By staggering loads and computes, the kernel hides global memory latency. Within each threadblock, they also reorganize warp work to minimize shared-memory usage. Instead of all warps reading/writing the same $O_i,\ell_i$ buffers, they partition that space so that each warp updates a disjoint slice, reducing inter-warp sync and bank conflicts. The net effect is that multiple GPU blocks work on the same attention head concurrently, increasing SM occupancy.</p> <h2 id="flashattention-v3">FlashAttention V3</h2> <h3 id="improvements-over-fa1--fa2">Improvements over FA1 &amp; FA2</h3> <h2 id="open-problems-and-future-directions">Open Problems and Future Directions</h2> <p>Despite these advances, several challenges remain. One is scaling beyond on-chip limits. Current FlashAttention relies on fitting entire blocks in hundreds of KB of SRAM. But LLMs push contexts to hundreds of thousands or even a million tokens, far beyond what fits on one GPU’s chip. Techniques like PagedAttention <d-cite key="kwon2023efficientmemorymanagementlarge"> </d-cite> (streaming attention from host memory in blocks) or Hydragen <d-cite key="juravsky2024hydragenhighthroughputllminference"> </d-cite> (optimizing shared prefixes) are only beginning to address this, but a fully general solution for trillion-token context still awaits. In theory, FlashAttention is IO-optimal for a given SRAM size so beyond-chip hierarchies (CPU memory, disk) must come into play, raising new algorithmic questions about streaming, compression, and multi-node attention.</p> <p>Another issue is programming and scheduling. The rapid improvements have largely come from hand-tuned CUDA kernels. Future efficiency will require better integration with compilers and ML frameworks. As Tri Dao notes, there is ongoing effort to make these optimizations “easily programmable” since current designs rely on manual warp scheduling and custom intrinsics. Moving to other platforms (AMD GPUs, TPUs, even CPUs) adds complexity: for example, AMD ROCm now supports FlashAttention via Triton, but the performance gap and engineering effort remain substantial. We see similar concerns in learned kernels: will XLA, MLIR, or DSLs be able to generate these tiled and overlapped patterns? Bridging the gap between compile-time scheduling (static tiling) and runtime adaptivity is an open compiler/hardware co-design problem.</p> <p>Finally, precision limits and numerical issues persist. Pushing to FP4 or mixed-integer kernels could double throughput again, but needs new algorithmic care (e.g. stochastic rounding, specialized normalization). FlashAttention-4’s lesson – that even math functions can be rethought in software – suggests any future hardware bottleneck (e.g. FP4 support) will inspire creative software solution</p> <h3 id="implications-for-long-context-llms">Implications for Long Context LLMs</h3> <p>Together, these principles directly serve the long-context frontier. Faster, memory-frugal attention means models can actually use very large windows of text. Today’s FlashAttention-enabled LLMs already handle contexts of 128K–1M tokens by carefully overlapping computation and memory. Tomorrow’s algorithms will push farther: for instance, if attention kernels reach multi-petaflop rates on next-gen GPUs, then 10× longer sequences become feasible in practice. In short, hardware-software co-design is the key enabler for ultra-long-context LLMs. By combining IO-efficient kernels (tiling and recompute), parallel pipelines, and smart approximations or sparsity, the community is paving the way for Transformer attention to scale to truly massive contexts with manageable compute and memory costs</p> <h2 id="appendix-a-proving-standard-attentions-quadratic-complexity">Appendix A: Proving Standard Attention’s Quadratic Complexity</h2> <p>The standard self-attention mechanism in Transformers is formally defined as:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>where inputs $Q, K \in \mathbb{R}^{n \times d_k}$ and $V \in \mathbb{R}^{n \times d_v}$ represent the query, key, and value matrices respectively. The computation proceeds in three sequential stages, each contributing to the overall computational cost.</p> <p>First, we compute the attention score matrix $S = \frac{QK^T}{\sqrt{d_k}}$. This operation involves a matrix multiplication between $Q$ and $K^T$, resulting in an $n \times n$ matrix. Since each of the $n^2$ entries requires a dot product of size $d_k$, the computational cost is proportional to the number of elements multiplied by the dimension size.</p> \[\text{Time Complexity (Scores)} = O(n^2 d_k)\] <p>Crucially, this step requires storing the intermediate matrix $S$, which scales quadratically with the sequence length $n$. This $O(n^2)$ memory requirement is the primary bottleneck for long sequences.</p> <p>Second, we apply the softmax function row-wise to normalize the scores. For each of the $n$ rows, we compute exponentials, sum them, and divide to normalize. As this is performed for every element in the $n \times n$ matrix, the cost is quadratic.</p> \[\text{Time Complexity (Softmax)} = O(n^2)\] <p>Finally, we compute the weighted sum of values by multiplying the normalized probability matrix $P = \text{softmax}(S)$ with the value matrix $V$. This results in an output matrix of size $n \times d_v$. Similar to the first step, each of the $n \times d_v$ output elements requires a dot product of length $n$.</p> \[\text{Time Complexity (Aggregation)} = O(n^2 d_v)\] <p>Summing these components gives the total time complexity:</p> \[\text{Total Time} = O(n^2 d_k) + O(n^2) + O(n^2 d_v) = O(n^2(d_k + d_v))\] <p>In typical Transformer architectures, the head dimensions $d_k$ and $d_v$ are proportional to the model dimension $d_{\text{model}}$. Thus, the complexity is often simplified to $O(n^2 d_{\text{model}})$.</p> <h3 id="scalability-implications">Scalability Implications</h3> <p>The quadratic dependency on sequence length $n$ creates significant resource challenges as context length increases. The table below illustrates how computational cost and memory usage grow with sequence length, assuming a hidden dimension of $d=768$.</p> <table> <thead> <tr> <th style="text-align: left">Sequence Length</th> <th style="text-align: left">Relative Compute Cost</th> <th style="text-align: left">Memory (GB)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">512</td> <td style="text-align: left">1×</td> <td style="text-align: left">0.001</td> </tr> <tr> <td style="text-align: left">2,048</td> <td style="text-align: left">16×</td> <td style="text-align: left">0.016</td> </tr> <tr> <td style="text-align: left">8,192</td> <td style="text-align: left">256×</td> <td style="text-align: left">0.25</td> </tr> <tr> <td style="text-align: left">65,536</td> <td style="text-align: left">16,384×</td> <td style="text-align: left">16</td> </tr> <tr> <td style="text-align: left">1M</td> <td style="text-align: left">4,000,000×</td> <td style="text-align: left">4,000</td> </tr> </tbody> </table> <h3 id="theoretical-lower-bounds">Theoretical Lower Bounds</h3> <p>One might ask if it is possible to compute attention more efficiently than $O(n^2)$. Research grounded in the Strong Exponential Time Hypothesis (SETH) suggests that this quadratic cost is fundamental. Specifically, it has been proven that for any $\epsilon &gt; 0$, computing the softmax dot-product attention requires $\Omega(n^{2-\epsilon})$ time.</p> <p>This lower bound holds for exact computation as well as for multiplicative and additive approximations. The proof relies on a reduction from the Orthogonal Vectors Problem (OVP). Intuitively, to accurately determine the attention distribution, the algorithm must evaluate pairwise interactions between query and key vectors. In high-dimensional spaces, distinguishing between orthogonal and nearly-orthogonal vectors requires checking all pairs, which establishes the $\Omega(n^2)$ barrier.</p> <h2 id="appendix-b--proving-standard-attentions-memory-bound">Appendix B : Proving Standard Attention’s Memory Bound</h2> <p>The standard Attention Implementation requires $\Theta(Nd + N^2)$ HBM accesses. This can be computed as</p> <ol> <li> <p>Computing $S = QK^T$: Reads $Q$ and $K$, writes $S$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$</p> </li> <li> <p>Computing $P = \text{softmax}(S)$: Reads $S$, writes $P$ to HBM $\rightarrow$ $\Theta(N^2)$.</p> </li> <li> <p>Computing $O = PV$: Reads $P$ and $V$, writes $O$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$.</p> </li> </ol>]]></content><author><name>Anonymous</name></author><category term="FlashAttention"/><category term="transformers"/><category term="GPU"/><category term="optimization"/><category term="machine-learning"/><summary type="html"><![CDATA[We present a deeply mathematical & technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms.]]></summary></entry><entry><title type="html">Can we really identify LLM Generated Text? The promise and limits of watermarking</title><link href="https://emharsha1812.github.io/blog/2025/llmgenerated/" rel="alternate" type="text/html" title="Can we really identify LLM Generated Text? The promise and limits of watermarking"/><published>2025-11-25T00:00:00+00:00</published><updated>2025-11-25T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/llmgenerated</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/llmgenerated/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The rise of Large Language Models (LLMs) and Multimodal models has changed the nature of digital content, often making the boundary between human and machine authorship increasingly porous. While this capability offers utility, it also introduces risks, such as the spread of misinformation, academic dishonesty, and a general erosion of trust in digital communication <d-cite key="chen2024llmgeneratedmisinformationdetected,singh2025on"></d-cite>.</p> <div class="l-body" style="border-left: 3px solid #ccc; padding-left: 15px; margin: 20px 0;"> For instance, ask yourself this, how will you confidently determine that this blog post you are reading is written by a human or is purely LLM-Generated? </div> <p>The central question is whether the text generated by an LLM (or in case of Multimodal models any content in the form of text, image or audio) can be reliably distinguished from that written by a human. Several papers have come up with watermarking techniques, notably among them appearing in ICLR presentations and conference <d-cite key="liu2024unforgeablepubliclyverifiablewatermark,liu2024semanticinvariantrobustwatermark,gu2024learnabilitywatermarkslanguagemodels,zhao2023provablerobustwatermarkingaigenerated,hu2023unbiasedwatermarklargelanguage,kirchenbauer2024reliabilitywatermarkslargelanguage,tian2024multiscalepositiveunlabeleddetectionaigenerated,arabi2025hiddennoisetwostagerobust,hu2025videoshieldregulatingdiffusionbasedvideo,lu2025robustwatermarkingusinggenerative"></d-cite>.</p> <p>In the sections that follow, we aim to give the readers a robust theoretical as well as practical understanding of watermarking while emphasizing the trade-offs and failure modes of these techniques. We also aim to tie in information-theoretic limits—how much signal one can embed without degrading text—as well as to what extent detection error bounds are acceptable</p> <p>All of the detection can be roughly bifurcated into two distinct methods; Detection Methods which operate post-hoc on finished text, and watermarking which represents a more proactive approach to establishing provenance. Watermarking is a step which aims to embed an imperceptible statistical signal into text during the generation process, thus creating a verifiable link between an output and its source. This signal is not a secret message itself more a detectable pattern that identifies the text as machine generated.</p> <p>The AI Generated writing has two notable characteristics that makes it seem a little too-perfect (and a little less human). These two characteristics are Perplexity and Burstiness.</p> <h2 id="perplexity">Perplexity</h2> <p>Perplexity - In Language modeling, perplexity quantifies a model’s uncertainty or “confusion” when predicting the next token in the sequence. Mathematically, it is the exponential of the average negative log-likelihood per token.</p> \[\text{Perplexity}(x_{1:T}) = \exp\left(-\frac{1}{T} \sum_{t=1}^{T} \log p(x_t \mid x_{&lt;t})\right)\] <p>A lower perplexity scores indicates that the model is more confident in its predictions, as it is effectively choosing from a smaller set of likely next words.</p> <p>The goal of LLM training is to minimize perplexity on a corpus of human text. Thus they tend to sample high-probability tokens and their generated text often has a lower perplexity score when evaluated by a language model than typical human-written text.</p> <div class="l-body" style="border-left: 3px solid #ccc; padding-left: 15px; margin: 20px 0;"> <strong>Note -</strong> Perplexity can be understood more intuitively through its geometric mean formulation. The geometric mean of a set of numbers is the Tth root of their product (where T is the number of values) and perplexity is the geometric mean of the inverse probabilities: $$ \text{Perplexity}(x_{1:T}) = \left(\prod_{t=1}^{T} \frac{1}{p(x_t \mid x_{&lt;t})}\right)^{1/T} $$ </div> <h2 id="burstiness">Burstiness</h2> <p>Burstiness - While Perplexity measures the average predictability of a text, burstiness measures its variance. it is defined as the change in perplexity over the course of a document. Mathematically,</p> \[B = \frac{\lambda - k}{\lambda + k}\] <p>where \(B\) = Burstiness, \(\lambda\) = Mean inter-arrival time between bursts, \(k\) = Mean burst length</p> <p>Human writing is often characterized by a “bursts” of high perplexity, where a writer uses a creative metaphor, a rare word, or an unconventional sentence structure. In contrast, LLM-generated text tends to maintain a more uniform level of perplexity, resulting in low burstiness.</p> <p>Beyond such raw statistics, it can also be observed that machine text exhibits noticeable stylistic patterns. Studies like <d-cite key="doi:10.1177/0261927X231200201"></d-cite> indicate that AI-generated text often adopts a more analytic and formal tone, may use a higher density of adjectives, and can be less readable than human generated text. Human writing on the other hand, has much more variability with more sophisticated discourse patterns, reflecting a more dynamic and uniform authoring process <d-cite key="kim-etal-2024-threads"></d-cite>.</p>]]></content><author><name>Anonymous</name></author><category term="LLM"/><category term="watermarking"/><category term="AI-detection"/><category term="machine-learning"/><summary type="html"><![CDATA[Exploring the theoretical and practical aspects of watermarking techniques for detecting AI-generated content, including trade-offs, failure modes, and information-theoretic limits]]></summary></entry><entry><title type="html">R squared in Machine Learning</title><link href="https://emharsha1812.github.io/blog/2025/rsquared/" rel="alternate" type="text/html" title="R squared in Machine Learning"/><published>2025-09-01T00:12:00+00:00</published><updated>2025-09-01T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/rsquared</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/rsquared/"><![CDATA[<p>The \(R^2\) metric, often called the coefficient of determination, is one of the most widely used measures for evaluating regression models. At its core, \(R^2\) tells us how well the model explains the variability of the dependent variable relative to a very simple baseline: the mean of that variable. To understand it deeply, let us start from the ground up.</p> <p>Suppose we have a dataset with a target variable \(y\), and our regression model produces predictions \(\hat{y}\). If we did not have any model at all, the best we could do to “predict” \(y\) would be to use its mean, \(\bar{y}\), for every data point. This simple strategy captures no nuance of the data, but it provides a baseline for comparison. The total variability in the data, called the total sum of squares (\(SS_{tot}\)), measures how much the actual values deviate from the mean. Mathematically, it is written as</p> \[SS_{tot} = \sum_i (y_i - \bar{y})^2.\] <p>Now, when we bring in a model, it produces predictions \(\hat{y}_i\). Naturally, those predictions won’t always be perfect, and the deviations of the predictions from the true values are called residuals. The sum of squared residuals (\(SS_{res}\)) captures how much unexplained error remains after using the model:</p> \[SS_{res} = \sum_i (y_i - \hat{y}_i)^2.\] <p>The magic of \(R^2\) lies in how it compares these two quantities. Specifically,</p> \[R^2 = 1 - \frac{SS_{res}}{SS_{tot}}.\] <p>If the model’s predictions are perfect, then the residual sum of squares vanishes to zero, giving \(R^2 = 1\). This indicates that the model explains all the variability of the data. On the other hand, if the model is no better than just predicting the mean, then \(SS_{res} = SS_{tot}\), and \(R^2\) becomes zero. Intriguingly, \(R^2\) can even be negative. This happens when the model is worse than the mean, in the sense that the residual errors are larger than the variability of the data itself. In such cases, the model is actively harmful as an explanatory tool.</p> <p>The intuitive interpretation of \(R^2\) is that it represents the fraction of variance in the dependent variable that is explained by the independent variables. If you think of variance as the “spread” or unpredictability in the data, then a model’s job is to account for as much of that spread as possible by relating it to explanatory features. For instance, if \(R^2 = 0.7\), it suggests that 70% of the variance in the outcome can be explained by the model, while the remaining 30% is still noise or unexplained. This interpretation makes \(R^2\) appealing because it ties the effectiveness of the model directly to the concept of variance explanation.</p> <p>However, one must also be cautious. \(R^2\) only measures variance explained relative to the mean model. It does not tell you whether the model is correct in a predictive sense, nor does it penalize overfitting directly. For example, adding more features to a model will never decrease \(R^2\); it can only stay the same or increase, even if the additional features have no real explanatory power. This is why adjusted \(R^2\) is often introduced, which penalizes the inclusion of unnecessary predictors by taking into account the number of features relative to the number of data points.</p> <p>Another subtle point is that \(R^2\) assumes that variance is the right quantity to explain. This makes sense in linear regression, where the goal is indeed to reduce squared error, but in contexts like nonlinear models or when distributions are highly skewed, the variance explanation picture may not fully align with predictive accuracy. For example, a model might achieve a high \(R^2\) but still perform poorly in predicting new data if it overfits. Similarly, in time series where temporal dependence is critical, variance explanation might be misleading without proper validation.</p> <p>So, in essence, \(R^2\) is a measure of how much better your model is compared to a naive mean predictor in terms of explaining variance in the target variable. It provides a normalized sense of fit: \(R^2 = 1\) means perfect explanation, \(R^2 = 0\) means no better than chance, and negative values indicate worse than chance. Thinking of it through the lens of variance explanation grounds it in the idea of “how much of the spread in the data have we accounted for?”—but it is always best interpreted alongside other metrics and validation strategies.</p> <p>Good. Let us now step into the geometric view, because it enriches our intuition for what \(R^2\) truly measures. Regression can be understood not only as a statistical minimization of squared error but also as a geometric projection in a high-dimensional vector space.</p> <p>Imagine your dataset of responses \(y = (y_1, y_2, \dots, y_n)\) as a vector sitting in an \(n\)-dimensional space. Each coordinate represents one observation. When we perform regression with predictors \(X\), what we are really doing is trying to find another vector \(\hat{y}\) that lies in the subspace spanned by the columns of \(X\). In other words, we are projecting the outcome vector \(y\) onto the space formed by linear combinations of the predictors. The projection gives us the fitted values \(\hat{y}\), while the leftover piece—the residuals \(y - \hat{y}\)—is orthogonal to that subspace.</p> <p>Now, why is this picture powerful for understanding \(R^2\)? Because variance explained corresponds to how much of the “length” (technically, squared norm) of \(y\) is captured in the projection. The total variability of \(y\) around its mean can be written as the squared length of the centered vector \(y - \bar{y}\mathbf{1}\), where \(\mathbf{1}\) is the all-ones vector. That is the total sum of squares, \(SS_{tot}\). The part explained by the regression is the squared length of the projection of this centered vector onto the column space of \(X\). That is called the regression sum of squares, \(SS_{reg}\). The residual sum of squares \(SS_{res}\) is simply the squared length of the orthogonal residual vector. By Pythagoras, these satisfy the neat identity</p> \[SS_{tot} = SS_{reg} + SS_{res}.\] <p>And from this decomposition, you see that</p> \[R^2 = \frac{SS_{reg}}{SS_{tot}},\] <p>which is literally “how much of the squared length is explained by the projection.”</p> <p>This geometry also reveals another interpretation: \(R^2\) is the square of the correlation coefficient between \(y\) and \(\hat{y}\). If you think of correlation as measuring alignment between two vectors, then \(R^2\) measures the degree to which the predicted vector lies in the same direction as the true vector. Perfect alignment gives correlation \(1\) and thus \(R^2 = 1\). A poor model, on the other hand, produces predictions that are only weakly aligned with the actual responses, giving a small \(R^2\). Negative \(R^2\) in this picture corresponds to the situation where the projection chosen by the model actually misaligns the predicted vector in such a way that it increases squared error compared to the mean baseline.</p> <p>So geometrically, variance explainability means this: you start with the cloud of data points in high-dimensional space, you draw the straightest line or hyperplane you can through them (given by the regression model), and you measure how much of the original data’s spread is captured along that line. The closer your data vector \(y\) lies to the subspace spanned by your predictors, the more variance you have explained, and the higher your \(R^2\).</p> <p>This perspective unifies the algebraic and statistical definitions. From one side, \(R^2\) is “1 minus unexplained variance over total variance.” From the other, it is the squared cosine of the angle between the true outcomes and the predictions. Both tell the same story: it quantifies alignment, projection, and variance accounted for by the model.</p> <hr/> <p>let’s now move from \(R^2\) to its refined cousin: <strong>adjusted \(R^2\)</strong>. The motivation for this adjustment emerges from a subtle flaw in plain \(R^2\). Remember that \(R^2\) never decreases as you add more predictors to a model. Even if the new variable has no true relationship with the target, the mere act of giving the model more flexibility allows it to fit the data slightly better, thereby reducing the residual sum of squares. This means that \(R^2\) is biased toward models with more features, and left unchecked, it can reward overfitting.</p> <p>Adjusted \(R^2\) was introduced to correct this. Its guiding idea is simple: yes, adding predictors can reduce error, but unless they genuinely improve explanatory power, they should be penalized for consuming degrees of freedom. The formula makes this precise. If you have \(n\) data points and \(p\) predictors, then adjusted \(R^2\) is defined as</p> \[R^2_{adj} = 1 - \frac{SS_{res}/(n-p-1)}{SS_{tot}/(n-1)}.\] <p>Notice the two denominators: instead of just comparing raw sums of squares, we are now comparing <strong>mean squared residuals per degree of freedom</strong>. The denominator \(n-1\) corresponds to the total variability after estimating a single mean, while the numerator \(n-p-1\) corresponds to the leftover variability after fitting \(p\) predictors. In essence, this formula asks: how much better is the model than the mean predictor, once we take into account the “cost” of the parameters used?</p> <p>The behavior of adjusted \(R^2\) is revealing. If a new predictor improves the model enough that the reduction in residual variance outweighs the penalty of losing a degree of freedom, adjusted \(R^2\) will rise. But if the predictor does not help much, the penalty dominates and adjusted \(R^2\) will actually fall. This makes it a more balanced tool for comparing models of different complexity.</p> <p>Another way to view it is through the lens of variance explanation again. While \(R^2\) asks “what fraction of variance do we explain,” adjusted \(R^2\) sharpens the question to “what fraction of variance do we explain per unit of explanatory effort?” It acknowledges that variance can be explained trivially by throwing in more variables, but meaningful explanation comes only when the gain surpasses the cost.</p> <p>It is important, however, to keep adjusted \(R^2\) in perspective. It is not a panacea. It still assumes that linear regression is the right modeling framework and that squared error is the right measure of fit. It is also influenced by sample size: with a small \(n\), the penalty for adding predictors is heavy, while with very large \(n\), adjusted \(R^2\) behaves more like plain \(R^2\). Nonetheless, within the family of linear regression comparisons, it is often the preferred metric because it guards against the seductive but misleading climb of \(R^2\) as more predictors are introduced.</p> <p>So, if you think of \(R^2\) as a measure of variance explanation in absolute terms, adjusted \(R^2\) is the disciplined version that insists on efficiency—explaining variance, yes, but doing so responsibly, without inflating the sense of achievement by smuggling in unnecessary variables.</p> <hr/>]]></content><author><name></name></author><category term="machine-learning"/><category term="machine-learning"/><summary type="html"><![CDATA[Meaning, Explanation & more]]></summary></entry><entry><title type="html">Training a simple bigram character level model on tiny stories</title><link href="https://emharsha1812.github.io/blog/2025/bigram/" rel="alternate" type="text/html" title="Training a simple bigram character level model on tiny stories"/><published>2025-05-24T00:12:00+00:00</published><updated>2025-05-24T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/bigram</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/bigram/"><![CDATA[<h1 id="building-a-bigram-language-model-a-step-by-step-guide-to-character-level-text-generation">Building a Bigram Language Model: A Step-by-Step Guide to Character-Level Text Generation</h1> <p>I wrote this small snippet as part of my learning process from Andrej’s video (link).</p> <h2 id="what-is-a-bigram-language-model">What is a Bigram Language Model?</h2> <p>A bigram language model predicts the next character in a sequence based solely on the current character. It’s called “bigram” because it considers pairs of characters (bi = two).</p> <p>The model learns a probability distribution over all possible next characters given the current character, essentially building a lookup table that says “when I see character X, what’s the most likely next character?”</p> <h2 id="dataset-preparation-and-text-loading">Dataset Preparation and Text Loading</h2> <p>Our journey begins with loading and examining our text data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">stories.text</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span><span class="o">=</span><span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">text</span><span class="p">[:</span><span class="mi">500</span><span class="p">])</span>
</code></pre></div></div> <p>The Tiny Stories dataset contains simple, child-friendly stories that are perfect for training language models. We load the entire text file into memory as a single string. This approach works well for smaller datasets, though larger datasets would require more sophisticated data loading strategies.</p> <h2 id="character-level-tokenization">Character-Level Tokenization</h2> <p>Unlike word-based models, our character-level approach treats each individual character as a token. This has several advantages:</p> <ul> <li><strong>Simplicity</strong>: No need for complex word segmentation</li> <li><strong>Robustness</strong>: Can handle any text, including typos and rare words</li> <li><strong>Fine-grained control</strong>: Learns spelling patterns and character relationships</li> </ul> <p>Let’s build our character vocabulary:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chars</span><span class="o">=</span><span class="nf">sorted</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">text</span><span class="p">)))</span>
<span class="n">vocab_size</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['\n', ' ', '!', '"', '#', '$', '&amp;', "'", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '&lt;', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¦', '©', '\xad', '±', '´', 'Â', 'Ã', 'â', 'ð', 'œ', 'Š', 'Ÿ', 'Ž', '˜', '“', '”', '‹', '€', '™']
101

 !"#$&amp;'()*+,-./0123456789:;&lt;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz¦©­±´ÂÃâðœŠŸŽ˜“”‹€™
</code></pre></div></div> <p>This code extracts all unique characters from our text and sorts them alphabetically. The vocabulary size tells us how many different characters our model needs to handle. Typically, this includes letters (both cases), numbers, punctuation, and whitespace characters.</p> <h2 id="building-the-tokenizer">Building the Tokenizer</h2> <p>Tokenization is the process of converting text into numerical representations that neural networks can process. We create two essential mappings:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stoi</span><span class="o">=</span><span class="p">{</span><span class="n">ch</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>  <span class="c1"># string to integer
</span><span class="n">itos</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)}</span>  <span class="c1"># integer to string
</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">])</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">stoi</code> (string-to-integer) dictionary maps each character to a unique integer ID, while <code class="language-plaintext highlighter-rouge">itos</code> (integer-to-string) provides the reverse mapping. Our encoder and decoder functions handle the conversion between text and numerical sequences.</p> <p>Testing our tokenizer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">exa</span><span class="o">=</span><span class="sh">"</span><span class="s">My name is Harshwardhan</span><span class="sh">"</span>
<span class="n">output</span><span class="o">=</span><span class="nf">encoder</span><span class="p">(</span><span class="n">exa</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">decoder</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[42, 80, 1, 69, 56, 68, 60, 1, 64, 74, 1, 37, 56, 73, 74, 63, 78, 56, 73, 59, 63, 56, 69]
My name is Harshwardhan
</code></pre></div></div> <p>This verification step ensures our encoding and decoding process is lossless - we can convert text to numbers and back to the original text perfectly.</p> <h2 id="converting-to-pytorch-tensors">Converting to PyTorch Tensors</h2> <p>Neural networks work with tensors, so we convert our encoded text into a PyTorch tensor:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nf">encoder</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([19212308])
tensor([48, 71, 70, 75, 14,  1, 48, 71, 70, 75,  1, 74, 56, 78,  1, 75, 63, 60,
         1, 74])
</code></pre></div></div> <p>The resulting tensor contains integer indices representing each character in our text. The shape tells us the total length of our dataset, while examining the first 100 elements helps us verify the conversion worked correctly.</p> <h2 id="dataset-splitting">Dataset Splitting</h2> <p>Machine learning requires separate training and validation sets to properly evaluate model performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">train</span><span class="o">=</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
<span class="n">validate</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span>
</code></pre></div></div> <p>We use a 90-10 split, dedicating 90% of our data to training and 10% to validation. The validation set helps us monitor whether our model is learning genuine patterns or simply memorizing the training data (overfitting).</p> <h2 id="understanding-context-windows">Understanding Context Windows</h2> <p>Language models don’t process entire texts at once. Instead, they work with fixed-size context windows.</p> <p>To give a context of what I am trying to say, here’s a snippet you can run to get an idea</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">block_size</span><span class="o">=</span><span class="mi">8</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">train</span><span class="p">[:</span><span class="n">block_size</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">block_size</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">when input is </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s"> the target: </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>when input is tensor([48]) the target: 71
when input is tensor([48, 71]) the target: 70
when input is tensor([48, 71, 70]) the target: 75
when input is tensor([48, 71, 70, 75]) the target: 14
when input is tensor([48, 71, 70, 75, 14]) the target: 1
when input is tensor([48, 71, 70, 75, 14,  1]) the target: 48
when input is tensor([48, 71, 70, 75, 14,  1, 48]) the target: 71
when input is tensor([48, 71, 70, 75, 14,  1, 48, 71]) the target: 70
</code></pre></div></div> <p>This code demonstrates a crucial concept: from a single sequence of length 8, we can create 8 different training examples. Each example uses a progressively longer context to predict the next character:</p> <ul> <li>Given just the first character, predict the second</li> <li>Given the first two characters, predict the third</li> <li>And so on…</li> </ul> <p>This approach maximizes the learning opportunities from our data and teaches the model to work with contexts of varying lengths.</p> <h2 id="batch-processing-for-efficient-training">Batch Processing for Efficient Training</h2> <p>Neural networks train more efficiently when processing multiple examples simultaneously. Our batch generation function creates random samples:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span>
<span class="n">block_size</span><span class="o">=</span><span class="mi">8</span>

<span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
    <span class="n">data</span><span class="o">=</span><span class="n">train</span> <span class="k">if</span> <span class="n">split</span><span class="o">==</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span> <span class="k">else</span> <span class="n">validate</span>
    <span class="n">ix</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="n">block_size</span><span class="p">,(</span><span class="n">batch_size</span><span class="p">,))</span>
    <span class="n">x</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span>
</code></pre></div></div> <p>This function randomly selects starting positions in our dataset and extracts sequences of length <code class="language-plaintext highlighter-rouge">block_size</code>. The result is two tensors:</p> <ul> <li><code class="language-plaintext highlighter-rouge">x</code>: Input sequences (what the model sees)</li> <li><code class="language-plaintext highlighter-rouge">y</code>: Target sequences (what the model should predict)</li> </ul> <p>The random sampling ensures our model sees different parts of the text in each batch, promoting better generalization.</p> <h2 id="the-bigram-language-model-architecture">The Bigram Language Model Architecture</h2> <p>Now we build our neural network. Despite its simplicity, this model embodies key language modeling concepts:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BigramLanguageModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_embedding_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</code></pre></div></div> <p>The core of our model is an embedding table - essentially a learned lookup table where each character is associated with a vector of probabilities for the next character. The embedding dimension equals our vocabulary size, creating a direct mapping from current character to next character probabilities.</p> <h2 id="forward-pass-and-loss-calculation">Forward Pass and Loss Calculation</h2> <p>The forward pass transforms input sequences into predictions and calculates the training loss:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding_table</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="c1"># (B,T,C)
</span>    
    <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="o">*</span><span class="n">T</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div> <p>The embedding table produces “logits” - raw prediction scores for each possible next character. When we have targets (during training), we calculate cross-entropy loss, which measures how well our predictions match the actual next characters.</p> <p>The reshaping operations (<code class="language-plaintext highlighter-rouge">view</code>) are necessary because PyTorch’s cross-entropy function expects 2D inputs, but our model produces 3D tensors (batch, time, characters).</p> <h2 id="text-generation">Text Generation</h2> <p>The generation function is where our trained model becomes useful:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="c1"># focus on last time step
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># convert to probabilities
</span>        <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># sample
</span>        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># append
</span>    <span class="k">return</span> <span class="n">idx</span>
</code></pre></div></div> <p>This function implements autoregressive generation:</p> <ol> <li>Get predictions for the current sequence</li> <li>Focus only on the last position (most recent character)</li> <li>Convert logits to probabilities using softmax</li> <li>Sample a character based on these probabilities</li> <li>Add the sampled character to our sequence</li> <li>Repeat</li> </ol> <p>The sampling step is crucial - rather than always picking the most likely character (which would be deterministic and repetitive), we sample according to the probability distribution, introducing controlled randomness that makes the generated text more interesting and varied.</p> <h2 id="training-loop">Training Loop</h2> <p>Training a neural network involves repeatedly showing it examples and adjusting its parameters to reduce prediction errors:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="k">for</span> <span class="n">steps</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">(</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">m</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p>Each training step follows a standard pattern:</p> <ol> <li><strong>Forward pass</strong>: Feed data through the model to get predictions</li> <li><strong>Loss calculation</strong>: Compare predictions to actual targets</li> <li><strong>Backward pass</strong>: Calculate gradients showing how to improve</li> <li><strong>Parameter update</strong>: Adjust model weights to reduce loss</li> </ol> <p>We use the AdamW optimizer, which adapts the learning rate for each parameter individually, leading to more stable and efficient training than basic gradient descent.</p> <h2 id="monitoring-progress">Monitoring Progress</h2> <p>Before training, our model generates mostly gibberish:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="nf">decoder</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sha inth ge jonin out, peroamy aveppedan s lld het
</code></pre></div></div> <p>After 10,000 training steps, the same generation call produces much more coherent text. The loss value also decreases significantly, indicating that our model is learning the character patterns in our dataset.</p> <h2 id="key-insights-and-limitations">Key Insights and Limitations</h2> <p>Our bigram model, while simple, demonstrates several important concepts:</p> <p><strong>Strengths:</strong></p> <ul> <li><strong>Simplicity</strong>: Easy to understand and implement</li> <li><strong>Speed</strong>: Fast training and inference</li> <li><strong>Foundational</strong>: Introduces core language modeling concepts</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li><strong>Limited context</strong>: Only considers the immediately previous character</li> <li><strong>No long-range dependencies</strong>: Cannot capture relationships between distant characters</li> <li><strong>Basic patterns</strong>: Learns simple character transitions but misses complex linguistic structures</li> </ul> <p>Here is the link to the Google Colab Notebook -</p>]]></content><author><name></name></author><category term="coding,"/><category term="python"/><category term="coding,"/><category term="python"/><summary type="html"><![CDATA[Training a simple bigram character level model on tiny stories]]></summary></entry><entry><title type="html">Machine Learning and AI Resources</title><link href="https://emharsha1812.github.io/blog/2025/nptel-ml/" rel="alternate" type="text/html" title="Machine Learning and AI Resources"/><published>2025-02-13T00:00:00+00:00</published><updated>2025-02-13T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/nptel-ml</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/nptel-ml/"><![CDATA[<blockquote> <p>“The goal is to turn data into information, and information into insight.”<br/> ~ Carly Fiorina</p> </blockquote> <p>Machine learning and artificial intelligence have revolutionized the way we approach problem-solving in many fields, from healthcare to robotics to natural language processing. If you’re looking to deepen your understanding of these technologies, here are some of the best online resources and courses available.</p> <h2 id="traditional-machine-learning">Traditional Machine Learning</h2> <ol> <li><a href="https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f/">Micro, Macro &amp; Weighted Averages of F1 Score, Clearly Explained</a></li> </ol> <h2 id="computer-vision">Computer Vision</h2> <ol> <li><a href="https://medium.com/@RobuRishabh/convolution-in-cnns-65e1655b5901">Convolution in CNNs</a></li> </ol> <h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2> <ol> <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs - Chris Olah</a></li> <li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable effectiveness of Recurrent Neural Networks</a></li> <li><a href="https://mmuratarat.github.io/2019-02-07/bptt-of-rnn">Backpropogation through time - Mathematical Derivation</a></li> <li><a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html">Why LSTMs Stop Your Gradients From Vanishing: A View from the Backwards Pass</a></li> </ol> <h2 id="learning-methods">Learning Methods</h2> <ol> <li><a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">Contrastive Representation Learning</a></li> </ol> <h2 id="famous-papers">Famous Papers</h2> <ol> <li><a href="https://medium.com/one-minute-machine-learning/clip-paper-explained-easily-in-3-levels-of-detail-61959814ad13">OpenAI’s CLIP Paper - Explanation</a></li> </ol> <h3 id="machine-learning--deep-learning-courses">Machine Learning &amp; Deep Learning Courses</h3> <ol> <li> <p><strong><a href="https://nptel.ac.in/courses/106106213">Practical Machine Learning with TensorFlow</a></strong><br/> Learn to build machine learning models using TensorFlow.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111105489">Mathematics for Machine Learning</a></strong><br/> A deep dive into the mathematical concepts that underpin machine learning algorithms.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111108066">Advanced Matrix Theory and Linear Algebra for Engineers</a></strong><br/> Understand matrix theory and linear algebra with an emphasis on engineering applications.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111108157">Matrix Theory</a></strong><br/> Learn the foundations of matrix theory, crucial for deep learning and machine learning algorithms.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/111107137">Essential Mathematics for Machine Learning</a></strong><br/> Build a strong mathematical foundation for machine learning and AI.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/108103192">Machine Learning and Deep Learning Fundamentals</a></strong><br/> This course provides a comprehensive introduction to machine learning and deep learning concepts.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106139">Machine Learning</a></strong><br/> A fundamental course to kickstart your journey in machine learning.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106198">Machine Learning for Engineering and Science Applications</a></strong><br/> Learn how machine learning is applied in engineering and scientific research.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/108103192">Machine Learning And Deep Learning – Fundamentals and Applications</a></strong><br/> A blend of theory and practical applications in machine learning and deep learning.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106184">Deep Learning - Part 1</a></strong><br/> Introduction to deep learning fundamentals, including neural networks and optimization techniques.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106201">Deep Learning - Part 2</a></strong><br/> Dive deeper into advanced deep learning concepts, architectures, and frameworks.</p> </li> </ol> <h3 id="natural-language-processing-nlp">Natural Language Processing (NLP)</h3> <ol> <li> <p><strong><a href="https://nptel.ac.in/courses/106105158">Natural Language Processing</a></strong><br/> Learn the fundamentals of NLP, including text processing and feature extraction.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106101007">Natural Language Processing</a></strong><br/> A comprehensive NLP course exploring algorithms and applications.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106211">Applied Natural Language Processing</a></strong><br/> Learn how to apply NLP techniques in real-world projects.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106224">Deep Learning for Computer Vision</a></strong><br/> Explore how deep learning models are applied to computer vision problems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/108105103">Deep Learning for Visual Computing</a></strong><br/> Understand the intersection of deep learning and visual computing.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106102576">Introduction to Large Language Models - Tanmoy Chakraborty</a></strong><br/> A course dedicated to large language models and their applications in NLP.</p> </li> <li> <p><strong><a href="https://www.youtube.com/playlist?list=PLZ2ps__7DhBbaMNZoyW2Hizl8DG6ikkjo">Introduction to Large Language Models - Mitesh Khapra</a></strong><br/> Learn about large language models from an industry expert.</p> </li> </ol> <h3 id="reinforcement-learning--ai">Reinforcement Learning &amp; AI</h3> <ol> <li> <p><strong><a href="https://nptel.ac.in/courses/106101466">Distributed Optimization and Machine Learning</a></strong><br/> Explore the optimization techniques used in distributed machine learning systems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/110101145">Bandit Algorithm</a></strong><br/> Learn the fundamentals of multi-armed bandit algorithms, useful in reinforcement learning.</p> </li> <li> <p><strong><a href="https://www.youtube.com/playlist?list=PLL1s8qiaGy0LwIajdxKZr_FRL7KZeQK9r">Deep Generative Models</a></strong><br/> Delve into the theory and applications of generative models like GANs and VAEs.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106143">Reinforcement Learning</a></strong><br/> An introduction to reinforcement learning, where agents learn by interacting with the environment.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106140">Artificial Intelligence: Knowledge Representation and Reasoning</a></strong><br/> Learn how knowledge can be represented and reasoned within AI systems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106226">Artificial Intelligence Search Methods For Problem Solving</a></strong><br/> Study search algorithms, essential for AI problem-solving.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106106238">Applied Accelerated Artificial Intelligence</a></strong><br/> Learn how to speed up and apply AI techniques in various industries.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106105077">Artificial Intelligence</a></strong><br/> A comprehensive introduction to the field of artificial intelligence.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/106105078">Artificial Intelligence</a></strong><br/> Learn AI concepts and techniques applicable in real-world problems.</p> </li> <li> <p><strong><a href="https://nptel.ac.in/courses/117108048">Pattern Recognition</a></strong><br/> Understand pattern recognition and its applications across diverse fields.</p> </li> </ol> <hr/> <p>Credits - <a href="https://www.linkedin.com/in/bastyajayshenoy/">Ajay Shenoy</a></p> <h2 id="large-language-models">Large Language Models</h2> <h3 id="llm-reasoning-papers">LLM Reasoning Papers</h3> <ul> <li><strong>LM Post-Training: A Deep Dive into Reasoning</strong> <ul> <li><a href="https://arxiv.org/pdf/2502.21321">https://arxiv.org/pdf/2502.21321</a></li> </ul> </li> <li><strong>A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1</strong> <ul> <li><a href="https://arxiv.org/html/2502.10867v1">https://arxiv.org/html/2502.10867v1</a></li> </ul> </li> <li><strong>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</strong> <ul> <li><a href="https://arxiv.org/html/2502.14768v1">https://arxiv.org/html/2502.14768v1</a></li> </ul> </li> </ul> <h3 id="llm-research-blogs">LLM Research Blogs</h3> <ul> <li><strong>LLM Research Newsletter</strong> <ul> <li><a href="https://www.llmsresearch.com/">https://www.llmsresearch.com/</a></li> </ul> </li> <li><strong>Transformer Circuit thread</strong> <ul> <li><a href="https://transformer-circuits.pub/">https://transformer-circuits.pub/</a></li> </ul> </li> </ul> <h3 id="internals">Internals</h3> <ul> <li><strong>Pytorch Internals</strong> <ul> <li><a href="https://blog.ezyang.com/2019/05/pytorch-internals/">https://blog.ezyang.com/2019/05/pytorch-internals/</a></li> </ul> </li> <li><strong>Transformer Internals</strong> <ul> <li><a href="https://goyalpramod.github.io/blogs/Transformers_laid_out/#coding-the-transformer/">https://goyalpramod.github.io/blogs/Transformers_laid_out/#coding-the-transformer</a></li> </ul> </li> </ul> <h3 id="iclr-blog-posts">ICLR BLog Posts</h3> <ul> <li><strong>A New Alchemy: Language Model Development as a Subfield?</strong> <ul> <li><a href="https://iclr-blogposts.github.io/2024/blog/language-model-development-as-a-new-subfield/">https://iclr-blogposts.github.io/2024/blog/language-model-development-as-a-new-subfield/</a></li> </ul> </li> <li><strong>Fairness in AI: two philosophies or just one?</strong> <ul> <li><a href="https://iclr-blogposts.github.io/2024/blog/fairness-ai-two-phil-or-just-one/">https://iclr-blogposts.github.io/2024/blog/fairness-ai-two-phil-or-just-one/</a></li> </ul> </li> </ul> <h2 id="generative-ai">Generative AI</h2> <ol> <li> <p><a href="https://huyenchip.com/2025/01/16/ai-engineering-pitfalls.html"><em>Common pitfalls when building generative AI applications</em></a> by Chip Huyen</p> </li> <li> <p><a href="https://blog.ml.cmu.edu/#"><em>ML CMU Blog</em></a></p> </li> </ol> <h2 id="mechanistic-interpretibility-mi">Mechanistic Interpretibility (MI)</h2> <p>Mechanistic interpretability aims to reverse-engineer a neural network into human-understandable mechanisms. MI focuses on transformers (specifically LLMs) but is not limited to these neural network architectures</p> <h3 id="people">People</h3> <ol> <li><a href="https://www.neelnanda.io/mechanistic-interpretability">Neel Nanda</a></li> <li><a href="https://www.alignmentforum.org/">Alignment Forum</a></li> <li></li> </ol> <h3 id="primer-on-llms">Primer on LLMs</h3> <ol> <li><a href="https://www.understandingai.org/p/large-language-models-explained-with">Large language models, explained with a minimum of math and jargon</a></li> <li></li> </ol> <h3 id="transformers">Transformers</h3> <ol> <li><a href="https://arena-chapter1-transformer-interp.streamlit.app/">Transformers Interpretibility</a></li> <li><a href="https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability">200 Concrete Open Problems in MI</a></li> <li></li> </ol> <h3 id="quick-guides-to-mi">Quick Guides to MI</h3> <ol> <li><a href="https://mindfulmodeler.substack.com/p/what-is-mechanistic-interpretability">What is Mechanistic Interpretability and where did it come from?</a></li> <li><a href="https://bluedot.org/blog/introduction-to-mechanistic-interpretability">Introduction to Mechanistic Interpretability</a></li> <li><a href="https://seantrott.substack.com/p/mechanistic-interpretability-for">“Mechanistic interpretability” for LLMs, explained</a></li> </ol> <h4 id="how-to-get-started-with-mi-">How to get started with MI ?</h4> <ol> <li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Concrete Steps to Get Started in Transformer Mechanistic Interpretability</a></li> </ol> <h3 id="relevant-papers">Relevant Papers</h3> <ol> <li><a href="https://arxiv.org/abs/2310.14491">Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models</a></li> <li><a href="https://arxiv.org/html/2407.02646v1">A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models</a></li> <li><a href="https://arxiv.org/pdf/2404.14082">Mechanistic Interpretability for AI Safety : A Review</a></li> </ol> <h3 id="straight-from-anthropic">Straight from Anthropic</h3> <ol> <li><a href="https://www.anthropic.com/research/mapping-mind-language-model">Mapping the mind of a Large Language model</a></li> <li><a href="https://transformer-circuits.pub/2023/interpretability-dreams/index.html">Interpretibility Dreams</a></li> <li><a href="https://www.anthropic.com/news/golden-gate-claude">Golden Gate Claude</a></li> <li><a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</a></li> <li><a href="https://transformer-circuits.pub/">Transformer Circuits Thread</a></li> </ol> <h3 id="blogs">Blogs</h3> <ol> <li><a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability">Neel Nanda’s case on why we need interpretibility research</a></li> <li><a href="https://hkamath.me/blog/2024/rqae/">A Microscope into the Dark Matter of Interpretability</a></li> </ol> <h3 id="libraries">Libraries</h3> <ol> <li><a href="https://transformerlensorg.github.io/TransformerLens/content/getting_started_mech_interp.html">Transfomer Lens</a></li> <li><a href="https://www.neuronpedia.org/">Neuronpedia</a></li> <li><a href="https://nnsight.net/">Interpretable Neural Networks</a></li> </ol> <h2 id="why-we-need-mi-research-">Why we need MI Research ?</h2> <p>Neel Nanda makes a couple of strong arguments <a href="https://www.lesswrong.com/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability">here</a> (15 in fact!) on why interpretibility research is needed and how it will help us resolve x-issues</p>]]></content><author><name></name></author><category term="machine-learning,"/><category term="ai,"/><category term="deep-learning,"/><category term="nlp"/><category term="machine-learning,"/><category term="deep-learning,"/><category term="ai,"/><category term="nlp"/><summary type="html"><![CDATA[A collection of links to essential courses on machine learning, deep learning, natural language processing, and artificial intelligence.]]></summary></entry><entry><title type="html">Python Notes</title><link href="https://emharsha1812.github.io/blog/2025/python-notes/" rel="alternate" type="text/html" title="Python Notes"/><published>2025-01-22T00:12:00+00:00</published><updated>2025-01-22T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/python-notes</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/python-notes/"><![CDATA[<p>Hello everyone,<br/> Below, I am sharing the GitHub repository containing all my Python notebooks, which I created while teaching an AI-ML Ops training program to esteemed DRDO scientists. You can access the GitHub repository here: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">GitHub Link</a>.</p> <p>This repository is a work in progress, and I will continue to update it as I create new notebooks. Here is the current plan for upcoming content:</p> <ol class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Python One-Liners Notebook</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Walrus Operator in Python</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Building a simple project using Python</li> </ol> <p>Feel free to explore and stay tuned for updates!</p> <p>Here’s the direct GitHub link for quick access: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">https://github.com/emharsha1812/Python_Programming_Notebooks</a></p> <p>Don’t forget to ⭐ star the repository to stay updated with new additions!</p> <h3 id="ps---here-is-a-list-of-helpful-links-that-you-can-reference-from-time-to-time">PS - Here is a list of helpful links that you can reference from time to time</h3> <ol> <li> <p><a href="https://realpython.com/python-iterators-iterables/">Python Iterators</a> - A super friendly blog on Python Iterators. I recommend taking this <a href="https://realpython.com/quizzes/python-iterators-iterables/">quiz</a> as well after reading the whole blog.</p> </li> <li> <p><a href="https://realpython.com/python-memory-management/">Memory Management in Python</a> - One of those “You should definitely know this” topics.</p> </li> </ol>]]></content><author><name></name></author><category term="coding,"/><category term="python"/><category term="coding,"/><category term="python"/><summary type="html"><![CDATA[A collection of Python notebooks for quick reference]]></summary></entry><entry><title type="html">KAN (Kolmogorov-Arnold Networks)</title><link href="https://emharsha1812.github.io/blog/2025/kan/" rel="alternate" type="text/html" title="KAN (Kolmogorov-Arnold Networks)"/><published>2025-01-07T00:12:00+00:00</published><updated>2025-01-07T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/kan</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/kan/"><![CDATA[<h2 id="1-mathematical-foundations--architecture">1. Mathematical Foundations &amp; Architecture</h2> <p><strong>Kolmogorov-Arnold Representation Theorem:</strong> KANs are founded on a classic result by A. N. Kolmogorov and V. Arnold, which states that <em>any continuous multivariate function can be represented as a finite superposition of univariate functions</em>(<a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov%E2%80%99s%20theorem%2C%20proposed%20in%201957%2C,The%20CFL%20condition%2C%20introduced%20by">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a>). In practical terms, this theorem guarantees that for a function \(f(x_1,\dots,x_n)\), there exist some continuous 1D functions \(\{\Phi_q\}\) and \(\{\Psi_{q,p}\}\) such that:</p> \[f(x_1,\ldots,x_n) \;=\; \sum_{q=0}^{2n} \; \Phi_q\!\Big( \sum_{p=1}^n \Psi_{q,p}(x_p) \Big)\,,\] <p>i.e. \(f\) can be decomposed into <strong>inner</strong> univariate functions \(\Psi_{q,p}\) (each depending on a single input variable $x_p$) and <strong>outer</strong> univariate functions \(\Phi_q\) aggregated by addition. This theorem provides a constructive blueprint for function approximation using single-variable building blocks, which is the key inspiration for KANs</p> <p><strong>KAN Architecture</strong> - Instead of the traditional neuron model with linear weighted sums and fixed activations, a KAN implements the above idea by making <strong>each edge</strong> of the network carry a <em>learnable univariate function</em>. In other words, every connection between neurons is parameterized as a nonlinear function (originally chosen as a B-spline) rather than a scalar weight. Each neuron simply sums up the outputs of the incoming edge-functions. Formally, if \(z_i^{(l)}\) denotes the \(i\)-th activation in layer \(l\), then a <strong>KAN layer</strong> computes each output neuron \(j\) as: (<a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview</a>) \(z_j^{(l+1)} \;=\; \sum_{i=1}^{N_l} f_{ij}^{(l)}\!\Big(z_i^{(l)}\Big)\,,\)</p> <p>where \(f_{ij}^{(l)}: \mathbb{R}\to\mathbb{R}\) is a learnable univariate function on the edge from neuron \(i\) (layer \(l\)) to neuron \(j\) (layer \(l+1\)). There are no separate linear weight matrices; the nonlinearity of \(f_{ij}\) itself provides the transformation. In the <em>shallowest</em> case (two-layer KAN), this architecture directly mirrors Kolmogorov’s decomposition: the first layer learns inner functions \(h_{p}(x_p)\) on each input dimension, and the second layer learns outer functions \(g_q(\cdot)\) that combine those results <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Quanta Magazine</a>.</p> <p><strong>Parameterized Functions (B-Splines):</strong> In practice, each learnable edge-function \(f_{ij}\) is parameterized as a spline (often a B-spline) with a set of control points that can be tuned during training. B-splines are piecewise polynomial curves defined by control points, offering a flexible yet smooth basis for approximating arbitrary 1D functions. By adjusting the control points, the shape of the spline changes locally without affecting the entire function. This choice ensures the learned activation functions are <em>smooth</em> addressing potential non-smoothness in Kolmogorov’s original construction and stable to train. Each edge thus has multiple parameters (the spline control values) instead of a single weight. For example, a KAN might initialize each \(f_{ij}\) as a near-linear spline and then let training mold each into the required nonlinear shape. This edge-centric design lets KANs <em>dynamically adapt their activation functions</em> to the data, rather than relying on a fixed function like ReLU or tanh.</p> <p><strong>Illustrative Pseudocode:</strong> The following pseudocode contrasts a single layer of an MLP vs. a KAN:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanmlp.svg-480.webp 480w,/assets/img/kanmlp.svg-800.webp 800w,/assets/img/kanmlp.svg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanmlp.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Comparison of MLP Layer with KAN Layer in Pytorch </div> <p>In the KAN layer, <code class="language-plaintext highlighter-rouge">f_ij</code> is a learned function (e.g. a spline) specific to edge \((i\to j)\), replacing both the weight and the neuron’s activation for that connection. The neuron simply aggregates these contributions (here via summation). Deep KANs can be built by stacking such layers, allowing composition of these univariate transformations across multiple levels.</p> <h2 id="2-comparison-with-mlps">2. Comparison with MLPs</h2> <p><strong>Structural Differences:</strong> Traditional Multi-Layer Perceptrons (MLPs) use <em>linear weights</em> and <em>fixed activation functions at neurons</em>, whereas KANs use <em>no linear weights at all</em> – every “weight” is replaced by a flexible function on the input signal. In effect, MLPs learn parameters for <strong>nodes</strong> (the weight matrix between layers is trained, then a fixed nonlinearity like ReLU is applied), while KANs learn parameters for <strong>edges</strong> (each connection has a trainable nonlinear mapping). This leads to a duality: <em>MLP = fixed nonlinearity + learned linear weights; KAN = fixed linear sum + learned nonlinear functions</em>. The figure below (from Liu et al. 2024) illustrates this difference, highlighting that MLPs apply activations at neurons (circles) whereas KANs apply learned functions on each connecting edge before summing.(<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanvsmlp-480.webp 480w,/assets/img/kanvsmlp-800.webp 800w,/assets/img/kanvsmlp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanvsmlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Source: <a href="https://arxiv.org/pdf/2404.19756" target="_blank">Liu et al. (2024)</a> </div> <p><strong>Learnable Functions vs Fixed Weights:</strong> In an MLP, the transformation from layer to layer is \(\sigma(Wx + b)\), with \(\sigma\) (e.g. ReLU) fixed and \(W,b\) learned. In a KAN, the transformation is \(\sum_i f_i(x_i)\) (plus bias if needed), with each $f_i$ being learned and no separate \(W\). Essentially, KANs “allocate” more flexibility per connection, whereas MLPs rely on combining many fixed nonlinear units to build complexity. This means KANs move the bulk of learnable parameters into the activation functions themselves, often resulting in <em>fewer total connections</em> needed than an equivalent MLP (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a>).</p> <p><strong>Expressive Power (Universal Approximation):</strong> Both MLPs and KANs are universal function approximators, but via different theorems. MLPs leverage the Universal Approximation Theorem (with enough neurons, an MLP can approximate any continuous function on a domain), while KANs directly leverage the Kolmogorov-Arnold (K-A) theorem to construct such approximations. In theory, a single hidden-layer KAN with sufficiently complex edge functions can exactly represent any continuous function (the K-A theorem provides an existence proof), whereas an MLP might require many more neurons or layers to approximate the same function with fixed activations. KANs thus excel at modeling functions with complex or “spiky” behavior in each input dimension, because each edge can carve out a detailed univariate relationship. In practice, KANs implement the K-A decomposition <em>explicitly</em>, using B-spline basis functions to approximate the required univariate mappings. This can translate to <em>greater expressivity per parameter</em>.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>).</p> <p><strong>Parameter Efficiency &amp; Neural Scaling:</strong> A striking reported advantage is that <em>much smaller KANs can achieve accuracy comparable or superior to much larger MLPs</em> on certain tasks. Each KAN edge function (with, say, $k$ control points) can encode a nonlinear relation that an MLP might need multiple neurons and layers to capture. Empirically, Liu <em>et al.</em> (2024) found KANs follow faster <strong>neural scaling laws</strong> – the error decreases more rapidly as model size increases, compared to MLPs. In other words, to reach a given accuracy, a KAN required fewer trainable parameters than an MLP in their tests. The flexibility of splines allows KANs to fit complex patterns without blowing up the network width/depth. One study noted that KANs can <em>match</em> MLP performance at equal parameter counts, and sometimes exceed it, though they require careful tuning (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=KANs%20definitely%20don%27t%20feel%20like,compared%20to%20regular%20neural%20networks">Trying Kolmogorov-Arnold Networks in Practice</a>). The original KAN paper demonstrated that a KAN with significantly fewer nodes could outperform a dense ReLU network on function-fitting benchmarks.</p> <p><strong>Continuous Learning and Locality:</strong> Because each KAN weight is a localized function (with local control points), learning in a KAN can be more localized. This has implications for <strong>continual learning</strong>. In standard nets, fine-tuning on new data often alters weights globally and can erode old capabilities (catastrophic forgetting). In KANs, adding new data primarily adjusts the spline control points <em>in relevant regions of the input space</em>, leaving other regions (and other functions) mostly unchanged. For example, if a KAN-based language model learns a new vocabulary or coding style, only certain edge-functions for those inputs might reshape, while others retain their previously learned shape. This property means KANs can integrate new knowledge without overwriting all weights, potentially enabling more <strong>seamless continual learning</strong>. MLPs, by contrast, have distributed representations where a single weight doesn’t correspond to an isolated input relationship, making targeted updates harder.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,local%20control%20point%20parameters%20change">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>)</p> <p><strong>Interpretability:</strong> A major motivation for KANs is interpretability. In an MLP, each weight by itself is usually not meaningful, and neurons combine many weights making interpretation difficult. In a KAN, each edge’s function $f_{ij}(x)$ can be visualized as a curve, directly showing how the input from neuron $i$ influences neuron $j$ across the range of values. After training, one can <em>extract these learned univariate functions</em> and inspect them.They might correspond to intuitive relations (e.g. an edge function might learn a sinusoidal shape if the output depends sinusoidally on an input).This transparency is especially useful in scientific or engineering tasks where understanding the learned model is as important as its accuracy. MLPs lack this fine-grained interpretability, since their learned mapping is entangled across many parameters. Thus, KANs offer a more human-understandable model: as the saying goes, they turn the <strong>“black box”</strong> into a collection of readable 1D transformations.</p> <p><strong>Summary:</strong> KANs and MLPs both approximate complex functions, but KANs do so by <em>baking learnable math into the connections</em>. This difference yields advantages in function approximation fidelity, parameter efficiency, and interpretability. However, it also comes with computational challenges (will uupdate later). In essence, KANs can be seen as a <strong>new paradigm</strong>: they trade the simple, generic structure of MLPs for a structure with built-in mathematical richness (the Kolmogorov-Arnold basis). This seemingly small change – moving from scalar weights to learned functions – has profound implications on how the network learns and what it can represent (<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>).</p> <p>The original paper can be found <a href="https://arxiv.org/pdf/2404.19756">here</a></p> <p>Last Updated - 25/02/2025</p> <h3 id="references">References</h3> <p>[1] - <a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov,the%20model%E2%80%99s%20flexibility%20and%20interpretability">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a></p> <p>[2] - <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Novel Architecture Makes Neural Networks More Understandable</a></p> <p>[3] - <a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview on KAN: Kolmogorov–Arnold Networks</a></p> <p>[4] - <a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a></p> <p>[5] - <a href="https://www.datacamp.com/tutorial/kolmogorov-arnold-networks#:~:text=simpler%2C%20univariate%20ones,edges%20are%20used%20for%20approximation">Kolmogorov-Arnold Networks (KANs): A Guide With Implementation</a></p> <p>[6] - <a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a></p> <p>[7] - <a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a></p> ]]></content><author><name></name></author><category term="llm,machine-learning,python"/><category term="kan,deep-learning,research"/><summary type="html"><![CDATA[An Alternative to traditional MLPs]]></summary></entry><entry><title type="html">A Visit to Hungarian Mathematics</title><link href="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/" rel="alternate" type="text/html" title="A Visit to Hungarian Mathematics"/><published>2024-09-01T00:12:00+00:00</published><updated>2024-09-01T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/hungarian-mathematics</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/"><![CDATA[<p>Because my domain is machine learning (ML) and artificial intelligence (AI), it’s quite common for me to come across abstract mathematical equations that initially made no sense to me. However, as I dove deep into those arcane-looking symbols, I discovered another interesting thing aside from the meaning: I realized that most of the founders of these equations were from Hungary!</p> <p>I googled, and then, a few clicks later, I stumbled on this very interesting read titled <strong>‘A Visit to Hungarian Mathematics’</strong>. Here’s the <a href="https://gwern.net/doc/math/1993-hersh.pdf">link</a> 🔗 to this pape . It contained exactly what I was looking for; a detailed introspection into Hungarian mathematicss and the mathematicians surrounding them. While reading the paper, I found a very simple but profoundly impactful quote that I would like to share. It says</p> <blockquote> <p>Everyone has ideas, both right ideas and wrong ideas. Scientific work consists merely of seperating them</p> </blockquote> <p>I immediately started voraciously reading the paper from top to bottom, left to right, and backwards too. Even though I am not a mathematician and do not have a mathematics degree (although I have a fairly good amount of mathematical exposure), I love to explore mathematics as a hobby. I sincerely hope that my interest in mathematics is still relevant.</p> <p>One of the key takeaways from this paper will be a quote by Alfred Renyi, a Hungarian mathematician known for his work in probability theory, combinatorics, and other fields. Once, a gifted mathematician told him that his working ability was heavily dependent on external circumstances. Renyi responded,</p> <blockquote> <p><strong>“When I’m unhappy, I use math to find happiness; when I’m content, I use math to maintain my happiness.”</strong></p> </blockquote> <p>Reading about these mathematicians and their passion for pondering, fighting, and finally solving math problems fills me with a deep sense of gratitude towards math.</p>]]></content><author><name></name></author><category term="mathematics,culture"/><category term="mathematics,"/><category term="axioms"/><summary type="html"><![CDATA[Why Hungarians are so darn good at mathematics ?]]></summary></entry><entry><title type="html">Welcome!</title><link href="https://emharsha1812.github.io/blog/2024/welcome/" rel="alternate" type="text/html" title="Welcome!"/><published>2024-08-27T00:00:00+00:00</published><updated>2024-08-27T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/welcome</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/welcome/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.]]></summary></entry></feed>