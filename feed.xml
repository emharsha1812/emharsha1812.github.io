<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://emharsha1812.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emharsha1812.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-12T14:07:16+00:00</updated><id>https://emharsha1812.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Research Resouces</title><link href="https://emharsha1812.github.io/blog/2025/reasoning/" rel="alternate" type="text/html" title="Research Resouces"/><published>2025-03-07T00:12:00+00:00</published><updated>2025-03-07T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/reasoning</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/reasoning/"><![CDATA[<h2 id="llm-reasoning-papers">LLM Reasoning Papers</h2> <ul> <li><strong>LM Post-Training: A Deep Dive into Reasoning</strong> <ul> <li><a href="https://arxiv.org/pdf/2502.21321">https://arxiv.org/pdf/2502.21321</a></li> </ul> </li> <li><strong>A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1</strong> <ul> <li><a href="https://arxiv.org/html/2502.10867v1">https://arxiv.org/html/2502.10867v1</a></li> </ul> </li> <li><strong>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</strong> <ul> <li><a href="https://arxiv.org/html/2502.14768v1">https://arxiv.org/html/2502.14768v1</a></li> </ul> </li> </ul> <h2 id="llm-research-blogs">LLM Research Blogs</h2> <ul> <li><strong>LLM Research Newsletter</strong> <ul> <li><a href="https://www.llmsresearch.com/">https://www.llmsresearch.com/</a></li> </ul> </li> <li><strong>Transformer Circuit thread</strong> <ul> <li><a href="https://transformer-circuits.pub/">https://transformer-circuits.pub/</a></li> </ul> </li> </ul> <h2 id="internals">Internals</h2> <ul> <li><strong>Pytorch Internals</strong> <ul> <li><a href="https://blog.ezyang.com/2019/05/pytorch-internals/">https://blog.ezyang.com/2019/05/pytorch-internals/</a></li> </ul> </li> <li><strong>Transformer Internals</strong> <ul> <li><a href="https://goyalpramod.github.io/blogs/Transformers_laid_out/#coding-the-transformer/">https://goyalpramod.github.io/blogs/Transformers_laid_out/#coding-the-transformer</a></li> </ul> </li> </ul> <h2 id="iclr-blog-posts">ICLR BLog Posts</h2> <ul> <li><strong>A New Alchemy: Language Model Development as a Subfield?</strong> <ul> <li><a href="https://iclr-blogposts.github.io/2024/blog/language-model-development-as-a-new-subfield/">https://iclr-blogposts.github.io/2024/blog/language-model-development-as-a-new-subfield/</a></li> </ul> </li> <li><strong>Fairness in AI: two philosophies or just one?</strong> <ul> <li><a href="https://iclr-blogposts.github.io/2024/blog/fairness-ai-two-phil-or-just-one/">https://iclr-blogposts.github.io/2024/blog/fairness-ai-two-phil-or-just-one/</a></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="llm,machine-learning,python"/><category term="llm,python,machine-learning"/><summary type="html"><![CDATA[Go-to papers and blogs for everything AI. Contains gathered resources]]></summary></entry><entry><title type="html">Survey Papers List</title><link href="https://emharsha1812.github.io/blog/2025/survey-paper/" rel="alternate" type="text/html" title="Survey Papers List"/><published>2025-03-07T00:12:00+00:00</published><updated>2025-03-07T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/survey-paper</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/survey-paper/"><![CDATA[<p align="center"> <a href="https://www.linkedin.com/in/kalyanksnlp/"> <img src="https://custom-icon-badges.demolab.com/badge/Kalyan%20KS-0A66C2?logo=linkedin-white&amp;logoColor=fff" alt="LinkedIn"/> </a> <a href="https://x.com/kalyan_kpl"> <img src="https://img.shields.io/badge/Kalyan%20KS-%23000000.svg?logo=X&amp;logoColor=white" alt="Twitter"/> </a> <a href="https://www.youtube.com/@kalyanksnlp"> <img src="https://img.shields.io/badge/Kalyan%20KS-%23FF0000.svg?logo=YouTube&amp;logoColor=white" alt="Twitter"/> </a> </p> <h2 id="quick-links">Quick links</h2> <p>|||| |—|—|—| | <a href="#general">🚀 General</a> | <a href="#prompting-techniques">🧱 Prompting Techniques</a> | <a href="#small-language-models">🩸Small Language Models</a> | | <a href="#multi-modal-llms">🟩 Multi Modal LLMs</a>| <a href="#multilingual-llms">🚧 MUltilingual LLMs</a> | <a href="#data-management">📤 Data Management</a> | | <a href="#post-training">🌠 Post Training</a> | <a href="#llm-fine-tuning">💎 LLM Fine Tuning</a>|<a href="#llm-inference">⚖️ LLM Inference</a> | | <a href="#llm-architectures">🔍 LLM Architectures</a> | <a href="#llm-abilities-and-limitations">📅 LLM Abilities and Limitations</a> | <a href="#llm-reasoning">📝 LLM Reasoning</a> | | <a href="#chain-of-thought">🛑 Chain of Thought</a> | <a href="#llm-rag">💠 LLM RAG</a> | <a href="#llm-agents">❇️ LLM Agents</a> | | <a href="#data-augmentation">✒️ Data Augmentation</a> | <a href="#hallucination">🎈 Hallucination</a> | <a href="#fairness">🟢 Fairness</a> | |<a href="#embedding-models">✅ Embedding Models</a>| <a href="#llm-evaluation">⚓️ LLM Evaluation</a>| <a href="#synthetic-data-generation">📝 Synthetic Data Generation</a>| | <a href="#llm-safety">🔴 LLM Safety</a> | <a href="#llms-for-specific-domains">🔶 LLMs for Specific Domains</a>|<a href="#others">☑️ Others</a>|</p> <h2 id="related-repositories">Related Repositories</h2> <ul> <li>🧱<a href="https://github.com/KalyanKS-NLP/llm-engineer-toolkit">LLM Engineer Toolkit</a> - Contains a curated list of 120+ LLM libraries category wise.</li> <li>🚀<a href="https://github.com/KalyanKS-NLP/rag-zero-to-hero-guide">RAG Zero to Hero Guide</a> - Comprehensive guide to learn RAG from basics to advanced.</li> </ul> <h2 id="general">General</h2> <p>| Paper | Link | |——-|——| | Large Language Models: A Survey | <a href="https://arxiv.org/abs/2402.06196">Link</a> | | A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4 | <a href="https://arxiv.org/abs/2310.12321">Link</a> | | A Comprehensive Overview of Large Language Models | <a href="https://arxiv.org/abs/2307.06435">Link</a> | | A Survey of Large Language Models | <a href="https://arxiv.org/abs/2303.18223">Link</a> | | Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects | <a href="https://www.techrxiv.org/users/618307/articles/682263-large-language-models-a-comprehensive-survey-of-its-applications-challenges-limitations-and-future-prospects">Link</a> |</p> <h2 id="prompting-techniques">Prompting Techniques</h2> <p>| Paper | Link | |——-|——| | The Prompt Report: A Systematic Survey of Prompting Techniques | <a href="https://arxiv.org/abs/2406.06608">Link</a> | | Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization | <a href="https://arxiv.org/abs/2406.01171">Link</a> | | Efficient Prompting Methods for Large Language Models: A Survey | <a href="https://arxiv.org/abs/2404.01077">Link</a> | | A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications | <a href="https://arxiv.org/abs/2402.07927">Link</a> | | In-context Learning with Retrieved Demonstrations for Language Models: A Survey | <a href="https://arxiv.org/abs/2401.11624">Link</a> |</p> <h2 id="small-language-models">Small Language Models</h2> <p>| Paper | Link | |——-|——| | Small Language Models (SLMs) Can Still Pack a Punch: A survey | <a href="https://arxiv.org/abs/2501.05465">Link</a> | | A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness | <a href="https://arxiv.org/abs/2411.03350">Link</a> | | A Survey of Small Language Models | <a href="https://arxiv.org/abs/2410.20011">Link</a> | | Small Language Models: Survey, Measurements, and Insights | <a href="https://arxiv.org/abs/2409.15790">Link</a> |</p> <h2 id="multi-modal-llms">Multi Modal LLMs</h2> <p>| Paper | Link | |——-|——| | Surveying the MLLM Landscape: A Meta-Review of Current Surveys | <a href="https://arxiv.org/abs/2409.18991">Link</a> | | From Specific-MLLM to Omni-MLLM: A Survey about the MLLMs alligned with Multi-Modality | <a href="https://arxiv.org/abs/2412.11694">Link</a> | | How to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model | <a href="https://arxiv.org/abs/2311.07594">Link</a> | | Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey | <a href="https://arxiv.org/abs/2412.02104">Link</a> | | MM-LLMs: Recent Advances in MultiModal Large Language Models | <a href="https://arxiv.org/abs/2401.13601">Link</a> | | A Survey on Multimodal Large Language Models | <a href="https://arxiv.org/abs/2306.13549">Link</a> |</p> <h2 id="multilingual-llms">Multilingual LLMs</h2> <p>| Paper | Link | |——-|——| | Multilingual Large Language Models: A Systematic Survey | <a href="https://arxiv.org/abs/2411.11072">Link</a> | | A Survey of Large Language Models for European Languages | <a href="https://arxiv.org/abs/2408.15040">Link</a> | | A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers | <a href="https://arxiv.org/abs/2405.10936">Link</a> | | Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers | <a href="https://arxiv.org/abs/2404.04925">Link</a> | | A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias | <a href="https://arxiv.org/abs/2404.00929">Link</a> |</p> <h2 id="data-management">Data Management</h2> <p>| Paper | Link | |——-|——| | A Survey on Data Selection for Language Models | <a href="https://arxiv.org/abs/2402.16827">Link</a> | | Datasets for Large Language Models: A Comprehensive Survey | <a href="https://arxiv.org/abs/2402.18041">Link</a> | | A Survey on Data Selection for LLM Instruction Tuning | <a href="https://arxiv.org/abs/2402.05123">Link</a> | | Data Management For Training Large Language Models: A Survey | <a href="https://arxiv.org/abs/2312.01700">Link</a> |</p> <h2 id="post-training">Post Training</h2> <p>| Paper | Link | |——-|——| | A Survey on Post-training of Large Language Models | <a href="https://arxiv.org/abs/2503.06072">Link</a> | | LLM Post-Training: A Deep Dive into Reasoning Large Language Models | <a href="https://arxiv.org/abs/2502.21321">Link</a> | | Reinforcement Learning Enhanced LLMs: A Survey | <a href="https://arxiv.org/abs/2412.10400">Link</a> | | Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models | <a href="https://arxiv.org/abs/2408.02085">Link</a> | | A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More | <a href="https://arxiv.org/abs/2407.16216">Link</a> | | Towards Scalable Automated Alignment of LLMs: A Survey | <a href="https://arxiv.org/abs/2406.01252">Link</a> | | Large Language Model Alignment: A Survey | <a href="https://arxiv.org/abs/2309.15025">Link</a> | | Instruction Tuning for Large Language Models: A Survey | <a href="https://arxiv.org/abs/2308.10792">Link</a> |</p> <h2 id="llm-fine-tuning">LLM Fine-Tuning</h2> <p>| Paper | Link | |——-|——| | Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies | <a href="https://arxiv.org/abs/2410.19878">Link</a> | | Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey | <a href="https://arxiv.org/abs/2403.14608">Link</a> | | Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment | <a href="https://arxiv.org/abs/2312.12148">Link</a> |</p> <h2 id="llm-inference">LLM Inference</h2> <p>| Paper | Link | |——-|——| | A Survey on LLM Inference-Time Self-Improvement | <a href="https://arxiv.org/abs/2412.14352">Link</a> | | A Comprehensive Survey of Accelerated Generation Techniques in Large Language Models | <a href="https://arxiv.org/abs/2405.13019">Link</a> | | Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems | <a href="https://arxiv.org/abs/2312.15234">Link</a> | | A Thorough Examination of Decoding Methods in the Era of LLMs | <a href="https://arxiv.org/abs/2402.06925">Link</a> | | Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward | <a href="https://arxiv.org/abs/2402.01799">Link</a> |</p> <h2 id="llm-architectures">LLM Architectures</h2> <p>| Paper | Link | |——-|——| | A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications | <a href="https://arxiv.org/abs/2503.07137">Link</a> | | A Survey on Mixture of Experts | <a href="https://arxiv.org/abs/2407.06204">Link</a> |</p> <h2 id="llm-abilities-and-limitations">LLM Abilities and Limitations</h2> <p>| Paper | Link | |——-|——| | Emergent Abilities in Large Language Models: A Survey | <a href="https://arxiv.org/abs/2503.05788">Link</a> | | A Survey on Multi-Turn Interaction Capabilities of Large Language Models | <a href="https://arxiv.org/abs/2501.09959">Link</a> | | A Survey on Large Language Models with some Insights on their Capabilities and Limitations | <a href="https://arxiv.org/abs/2501.04040">Link</a> | | Knowledge Boundary of Large Language Models: A Survey | <a href="https://arxiv.org/abs/2412.12472">Link</a> | | A Survey on the Honesty of Large Language Models | <a href="https://arxiv.org/abs/2409.18786">Link</a> |</p> <h2 id="llm-reasoning">LLM Reasoning</h2> <p>| Paper | Link | |——-|——| | A Survey on Enhancing Causal Reasoning Ability of Large Language Models | <a href="https://arxiv.org/abs/2503.09326">Link</a> | | Reasoning Language Models: A Blueprint | <a href="https://arxiv.org/abs/2501.11223">Link</a> | | Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models | <a href="https://arxiv.org/abs/2503.09567">Link</a> | | A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method &amp; Challenges | <a href="https://arxiv.org/abs/2412.11936">Link</a> | | A Survey of Table Reasoning with Large Language Models | <a href="https://arxiv.org/abs/2402.08259">Link</a> | | Large Language Models for Mathematical Reasoning: Progresses and Challenges | <a href="https://arxiv.org/abs/2402.00157">Link</a> | | Towards Reasoning in Large Language Models: A Survey | <a href="https://arxiv.org/abs/2212.10403">Link</a> |</p> <h2 id="chain-of-thought">Chain of Thought</h2> <p>| Paper | Link | |——-|——| | Igniting Language Intelligence: The Hitchhiker’s Guide From Chain-of-Thought Reasoning to Language Agents | <a href="https://arxiv.org/abs/2311.11797">Link</a> | | Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future | <a href="https://arxiv.org/abs/2309.15402v3">Link</a> | | Towards Better Chain-of-Thought Prompting Strategies: A Survey | <a href="https://arxiv.org/abs/2310.04959">Link</a> |</p> <h2 id="llm-rag">LLM RAG</h2> <p>| Paper | Link | |——-|——| | Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG | <a href="https://arxiv.org/abs/2501.09136">Link</a> | | A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models | <a href="https://arxiv.org/abs/2501.13958">Link</a> | | Exploring AI Text Generation, Retrieval-Augmented Generation, and Detection Technologies: a Comprehensive Overview | <a href="https://arxiv.org/abs/2412.03933">Link</a> | | Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely | <a href="https://arxiv.org/abs/2409.14924">Link</a> | | Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey | <a href="https://arxiv.org/abs/2409.13385">Link</a> | | Trustworthiness in Retrieval-Augmented Generation Systems: A Survey | <a href="https://arxiv.org/abs/2409.10102">Link</a> | | Graph Retrieval-Augmented Generation: A Survey | <a href="https://arxiv.org/abs/2408.08921">Link</a> | | Evaluation of Retrieval-Augmented Generation: A Survey | <a href="https://arxiv.org/abs/2405.07437">Link</a> | | A Survey on Knowledge-Oriented Retrieval-Augmented Generation | <a href="https://arxiv.org/abs/2503.10677">Link</a> | | Retrieval-Augmented Generation for AI-Generated Content: A Survey | <a href="https://arxiv.org/abs/2402.19473">Link</a> |</p> <h2 id="llm-agents">LLM Agents</h2> <p>| Paper | Link | |——-|——| | Survey on Evaluation of LLM-based Agents | <a href="https://arxiv.org/abs/2503.16416">Link</a> | | GUI Agents: A Survey | <a href="https://arxiv.org/abs/2412.13501">Link</a> | | Large Language Model-Brained GUI Agents: A Survey | <a href="https://arxiv.org/abs/2411.18279">Link</a> | | GUI Agents with Foundation Models: A Comprehensive Survey | <a href="https://arxiv.org/abs/2411.04890v2">Link</a> | | The Rise and Potential of Large Language Model Based Agents: A Survey | <a href="https://arxiv.org/abs/2309.07864">Link</a> | | A Survey on Large Language Model-based Agents for Statistics and Data Science | <a href="https://arxiv.org/abs/2412.14222">Link</a> | | Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents | <a href="https://arxiv.org/abs/2411.09523">Link</a> | | Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems | <a href="https://arxiv.org/abs/2504.01990v1">Link</a> | | A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios | <a href="https://arxiv.org/abs/2412.03920">Link</a> | | From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents | <a href="https://arxiv.org/abs/2412.03563">Link</a> | | Agents in Software Engineering: Survey, Landscape, and Vision | <a href="https://arxiv.org/abs/2409.09030">Link</a> | | Large Language Model-Based Agents for Software Engineering: A Survey | <a href="https://arxiv.org/abs/2409.02977">Link</a> | | From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future | <a href="https://arxiv.org/abs/2408.02479">Link</a> | | Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security | <a href="https://arxiv.org/abs/2401.05459">Link</a> | | Large Language Model Agent in Financial Trading: A Survey | <a href="https://arxiv.org/abs/2408.06361">Link</a> | | Tool Learning with Large Language Models: A Survey | <a href="https://arxiv.org/abs/2405.17935">Link</a> | | What Are Tools Anyway? A Survey from the Language Model Perspective | <a href="https://arxiv.org/abs/2403.15452">Link</a> | | Large Language Model based Multi-Agents: A Survey of Progress and Challenges | <a href="https://arxiv.org/abs/2402.01680">Link</a> | | Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects | <a href="https://arxiv.org/abs/2401.03428">Link</a> |</p> <h2 id="data-augmentation">Data Augmentation</h2> <p>| Paper | Link | |——-|——| | Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities | <a href="https://arxiv.org/abs/2501.18845">Link</a> | | Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges | <a href="https://arxiv.org/abs/2403.02990">Link</a> | | A Survey on Data Augmentation in Large Model Era | <a href="https://arxiv.org/abs/2401.15422">Link</a> |</p> <h2 id="hallucination">Hallucination</h2> <p>| Paper | Link | |——-|——| | A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions | <a href="https://arxiv.org/abs/2311.05232">Link</a> | | A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models | <a href="https://arxiv.org/abs/2401.01313">Link</a> | | Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey | <a href="https://arxiv.org/abs/2311.07914">Link</a> |</p> <h2 id="fairness">Fairness</h2> <p>| Paper | Link | |——-|——| | Fairness Definitions in Language Models Explained | <a href="https://arxiv.org/abs/2407.18454">Link</a> | | A survey on fairness of large language models in e-commerce: progress, application, and challenge | <a href="https://arxiv.org/abs/2405.13025">Link</a> | | Fairness in Large Language Models: A Taxonomic Survey | <a href="https://arxiv.org/abs/2404.01349">Link</a> | | A Survey on Fairness in Large Language Models | <a href="https://arxiv.org/abs/2308.10149">Link</a> |</p> <h2 id="embedding-models">Embedding Models</h2> <p>| Paper | Link | |——-|——| | LLMs are Also Effective Embedding Models: An In-depth Overview | <a href="https://arxiv.org/abs/2412.12591">Link</a> | | When Text Embedding Meets Large Language Model: A Comprehensive Survey | <a href="https://arxiv.org/abs/2412.09165">Link</a> |</p> <h2 id="llm-evaluation">LLM Evaluation</h2> <p>| Paper | Link | |——-|——| | LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods | <a href="https://arxiv.org/abs/2412.05579">Link</a> | | From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge | <a href="https://arxiv.org/abs/2411.16594">Link</a> | | A Survey on LLM-as-a-Judge | <a href="https://arxiv.org/abs/2411.15594">Link</a> | | Automatic Metrics in Natural Language Generation: A Survey of Current Evaluation Practices | <a href="https://arxiv.org/abs/2408.09169">Link</a> | | A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations | <a href="https://arxiv.org/abs/2407.04069">Link</a> | | The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches | <a href="https://arxiv.org/abs/2406.03339">Link</a> | | A Survey of Useful LLM Evaluation | <a href="https://arxiv.org/abs/2406.00936">Link</a> | | Leveraging Large Language Models for NLG Evaluation: A Survey | <a href="https://arxiv.org/abs/2401.07103">Link</a> | | Evaluating Large Language Models: A Comprehensive Survey | <a href="https://arxiv.org/abs/2310.19736">Link</a> | | Through the Lens of Core Competency: Survey on Evaluation of Large Language Models | <a href="https://arxiv.org/abs/2308.07902">Link</a> | | A Survey on Evaluation of Large Language Models | <a href="https://arxiv.org/abs/2307.03109">Link</a> |</p> <h2 id="synthetic-data-generation">Synthetic Data Generation</h2> <p>| Paper | Link | |——-|——| | Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models | <a href="https://arxiv.org/abs/2412.02980">Link</a> | | On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey | <a href="https://arxiv.org/abs/2406.15126">Link</a> | | Synthetic Data Generation Using Large Language Models: Advances in Text and Code | <a href="https://arxiv.org/abs/2503.14023">Link</a> | | A Survey on Recent Advances in Conversational Data Generation | <a href="https://arxiv.org/abs/2405.13003">Link</a> |</p> <h2 id="llm-safety">LLM Safety</h2> <p>| Paper | Link | |——-|——| | Recent advancements in LLM Red-Teaming: Techniques, Defenses, and Ethical Considerations | <a href="https://arxiv.org/abs/2410.09097">Link</a> | | AI Safety in Generative AI Large Language Models: A Survey | <a href="https://arxiv.org/abs/2407.18369">Link</a> | | Jailbreak Attacks and Defenses Against Large Language Models: A Survey | <a href="https://arxiv.org/abs/2407.04295">Link</a> | | Against The Achilles’ Heel: A Survey on Red Teaming for Generative Models | <a href="https://arxiv.org/abs/2404.00629">Link</a> | | On Protecting the Data Privacy of Large Language Models (LLMs): A Survey | <a href="https://arxiv.org/abs/2403.05156">Link</a> | | Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models | <a href="https://arxiv.org/abs/2403.04786">Link</a> | | Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey | <a href="https://arxiv.org/abs/2402.09283">Link</a> | | Building Guardrails for Large Language Models | <a href="https://arxiv.org/abs/2402.01822">Link</a> | | Security and Privacy Challenges of Large Language Models: A Survey | <a href="https://arxiv.org/abs/2402.00888">Link</a> | | A Security Risk Taxonomy for Large Language Models | <a href="https://arxiv.org/abs/2311.11415">Link</a> | | A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation | <a href="https://arxiv.org/abs/2305.11391">Link</a> |</p> <h2 id="llms-for-specific-domains">LLMs for Specific Domains</h2> <p>| Paper | Domain | Link | |——-|——–|——| | Large language models (LLM) in computational social science: prospects, current state, and challenges | Social Media | <a href="URL">Link</a> | | A Survey on Large Language Models for Code Generation | Coding | <a href="https://arxiv.org/abs/2503.01245v2">Link</a> | | Large Language Models Meet NL2Code: A Survey | Coding | <a href="https://arxiv.org/abs/2212.09420">Link</a> | | Large Language Models for Education: A Survey | Education | <a href="https://arxiv.org/abs/2405.13001">Link</a> | | Political-LLM: Large Language Models in Political Science | Political Science | <a href="https://arxiv.org/abs/2412.06864">Link</a> | | Code LLMs: A Taxonomy-based Survey | Coding | <a href="https://arxiv.org/abs/2412.08291">Link</a> | | Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey | Data Science | <a href="https://arxiv.org/abs/2411.10478">Link</a> | | A Survey of Large Language Models in Finance (FinLLMs) | Finance | <a href="https://arxiv.org/abs/2402.02315">Link</a> | | Large Language Models in Finance: A Survey | Finance | <a href="https://arxiv.org/abs/2311.10723">Link</a> | | Revolutionizing Finance with LLMs: An Overview of Applications and Insights | Finance | <a href="https://arxiv.org/abs/2401.11641">Link</a> | | Legal Evalutions and Challenges of Large Language Models | Legal | <a href="https://arxiv.org/abs/2411.10137">Link</a> | | Large Language Models in Law: A Survey | Legal | <a href="https://arxiv.org/abs/2312.03718">Link</a> | | A Survey for Large Language Models in Biomedicine | Healthcare | <a href="https://arxiv.org/abs/2409.00133">Link</a> | | A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences | Healthcare | <a href="https://arxiv.org/abs/2412.03531">Link</a> | | Demystifying Large Language Models for Medicine: A Primer | Healthcare | <a href="https://arxiv.org/abs/2410.18856">Link</a> | | Large Language Models for Disease Diagnosis: A Scoping Review | Healthcare | <a href="https://arxiv.org/abs/2409.00097">Link</a> | | A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations | Healthcare | <a href="https://arxiv.org/abs/2406.10303">Link</a> | | A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges | Healthcare | <a href="https://arxiv.org/abs/2311.05112">Link</a> | | Large Language Models for Medicine: A Survey | Healthcare | <a href="https://arxiv.org/abs/2405.13055">Link</a> | | Large Language Models in Biomedical and Health Informatics: A Bibliometric Review | Healthcare | <a href="https://arxiv.org/abs/2403.16303">Link</a> | | A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions | Healthcare | <a href="https://arxiv.org/abs/2406.03712">Link</a> | | A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics | Healthcare | <a href="https://arxiv.org/abs/2310.05694">Link</a> | | AI for Biomedicine in the Era of Large Language Models | Healthcare | <a href="https://arxiv.org/abs/2403.15673">Link</a> | | Large Language Models in Bioinformatics: A Survey | Healthcare | <a href="https://arxiv.org/abs/2503.04490">Link</a> | | Large language models for automated scholarly paper review: A survey | Scientific Research | <a href="https://arxiv.org/abs/2501.10326">Link</a> | | LLM4SR: A Survey on Large Language Models for Scientific Research | Scientific Research | <a href="https://arxiv.org/abs/2501.04306">Link</a> | | A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery | Scientific Research | <a href="https://arxiv.org/abs/2406.10833">Link</a> | | Harnessing Large Language Models for Disaster Management: A Survey | Disaster Management | <a href="https://arxiv.org/abs/2501.06932">Link</a> | | Leveraging Foundation Models for Crafting Narrative Visualization: A Survey | Visualization | <a href="https://arxiv.org/abs/2401.14010">Link</a> | | LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights | Security | <a href="https://arxiv.org/abs/2502.07049">Link</a> | | Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey | Security | <a href="https://arxiv.org/abs/2408.07583">Link</a> | | A Comprehensive Overview of Large Language Models (LLMs) for Cyber Defences: Opportunities and Directions | Security | <a href="https://arxiv.org/abs/2405.14487">Link</a> | | Mathematical Language Models: A Survey | Maths | <a href="https://arxiv.org/abs/2312.07622">Link</a> | | Software Testing with Large Language Model: Survey, Landscape, and Vision | Software Engineering | <a href="https://arxiv.org/abs/2307.07221">Link</a> | | Large Language Models for Software Engineering: Survey and Open Problems | Software Engineering | <a href="https://arxiv.org/abs/2310.03533">Link</a> |</p> <h2 id="others">Others</h2> <p>| Paper | Category | Link | |——-|———-|——| | Awes, Laws, and Flaws From Today’s LLM Research | LLM Research | <a href="https://arxiv.org/abs/2408.15409">Link</a> | | Efficient Large Language Models: A Survey | Efficient LLMs | <a href="https://arxiv.org/abs/2312.03863">Link</a> | | Continual Learning for Large Language Models: A Survey | Continual Learning | <a href="https://arxiv.org/abs/2402.01364">Link</a> | | From Selection to Generation: A Survey of LLM-based Active Learning | Active Learning | <a href="https://arxiv.org/abs/2502.11767">Link</a> | | Towards Lifelong Learning of Large Language Models: A Survey | Life Long Learning | <a href="https://arxiv.org/abs/2406.06391">Link</a> | | Privacy-Preserving Large Language Models: Mechanisms, Applications, and Future Directions | Privacy Preserving Mechanism | | | A Survey on Human Preference Learning for Large Language Models | Human Preference Learning | <a href="https://arxiv.org/abs/2406.11191">Link</a> | | A Survey on Human-Centric LLMs | Human centric LLMs | <a href="https://arxiv.org/abs/2411.14491">Link</a> | | Prompt Compression for Large Language Models: A Survey | Prompts | <a href="https://arxiv.org/abs/2410.12388">Link</a> | | The Oscars of AI Theater: A Survey on Role-Playing with Language Models | Prompts | <a href="https://arxiv.org/abs/2407.16216">Link</a> | | From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems | Text-to-SQL | <a href="https://arxiv.org/abs/2410.01066">Link</a> | | Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL | Text-to-SQL | <a href="https://arxiv.org/abs/2406.08426">Link</a> | | A Survey of NL2SQL with Large Language Models: Where are we, and where are we going? | Text-to-SQL | <a href="https://arxiv.org/abs/2408.05109">Link</a> | | A Survey on Employing Large Language Models for Text-to-SQL Tasks | Text-to-SQL | <a href="https://arxiv.org/abs/2407.15186">Link</a> | | Undesirable Memorization in Large Language Models: A Survey | LLM Memorization | <a href="https://arxiv.org/abs/2410.02650">Link</a> | | A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms | Low bit LLMs | <a href="https://arxiv.org/abs/2409.16694">Link</a> | | Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges | Data Contamination | <a href="https://arxiv.org/abs/2409.09927">Link</a> | | Benchmark Data Contamination of Large Language Models: A Survey | Data Contamination | <a href="https://arxiv.org/abs/2406.04244">Link</a> | | Attention Heads of Large Language Models: A Survey | LLM Attention Heads | <a href="https://arxiv.org/abs/2409.03752">Link</a> | | Claim Verification in the Age of Large Language Models: A Survey | Fact Checking | <a href="https://arxiv.org/abs/2408.14317">Link</a> | | Generative Large Language Models in Automated Fact-Checking: A Survey | Fact Checking | <a href="https://arxiv.org/abs/2407.02351">Link</a> | | Controllable Text Generation for Large Language Models: A Survey | Controllable Text Generation | <a href="https://arxiv.org/abs/2408.12599">Link</a> | | A Survey on Symbolic Knowledge Distillation of Large Language Models | Knowledge Distillation | <a href="https://arxiv.org/abs/2408.10210">Link</a> | | Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application | Knowledge Distillation | <a href="https://arxiv.org/abs/2407.01885">Link</a> | | Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities | Model Merging | <a href="https://arxiv.org/abs/2408.07666">Link</a> | | Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions | Privacy Preserving | <a href="https://arxiv.org/abs/2408.05212">Link</a> | | Keep the Cost Down: A Review on Methods to Optimize LLM’ s KV-Cache Consumption | LLM’s KV-Cache | <a href="https://arxiv.org/abs/2407.18003">Link</a> | | Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models | Collaborative Strategies | <a href="https://arxiv.org/abs/2407.06089">Link</a> | | When Search Engine Services meet Large Language Models: Visions and Challenges | Search Engines meet LLMs | <a href="https://arxiv.org/abs/2407.00128">Link</a> | | A Systematic Survey of Text Summarization: From Statistical Methods to Large Language Models | Text Summarization | <a href="https://arxiv.org/abs/2406.11289">Link</a> | | LLMs Meet Multimodal Generation and Editing: A Survey | Multi-modal Generation | <a href="https://arxiv.org/abs/2405.19334">Link</a> | | Large Language Models on Tabular Data - A Survey | Tabular Data | <a href="https://arxiv.org/abs/2402.17944">Link</a> | | Large Language Model for Table Processing: A Survey | Tabular Data | <a href="https://arxiv.org/abs/2402.05121">Link</a> | | Large Language Models for Time Series: A Survey | Time Series Data | <a href="https://arxiv.org/abs/2402.01801">Link</a> | | A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems | Dialogue Systems | <a href="https://arxiv.org/abs/2402.18013">Link</a> | | Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques | Graph Representation Learning | <a href="https://arxiv.org/abs/2402.05952">Link</a> | | When Large Language Models Meet Vector Databases: A Survey | Vector Databases | <a href="https://arxiv.org/abs/2402.01763">Link</a> | | Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models | Context Length | <a href="https://arxiv.org/abs/2402.02244">Link</a> | | The What, Why, and How of Context Length Extension Techniques in Large Language Models – A Detailed Survey | Context Length | <a href="https://arxiv.org/abs/2401.07872">Link</a> | | Leveraging Large Models for Crafting Narrative Visualization: A Survey | Visualization | <a href="https://arxiv.org/abs/2401.14010">Link</a> | | Large Language Models for Social Networks : Applications, Challenges and Solutions | Social Networks | <a href="https://arxiv.org/abs/2401.02575">Link</a> | | Large Language Models for Generative Information Extraction: A Survey | Information Extraction | <a href="https://arxiv.org/abs/2312.17617">Link</a> | | A Survey of Text Watermarking in the Era of Large Language Models | Water Marking | <a href="https://arxiv.org/abs/2312.07913">Link</a> | | Large Language Models on Graphs: A Comprehensive Survey | Graphs | <a href="https://arxiv.org/abs/2312.02783">Link</a> | | A Survey of Graph Meets Large Language Model: Progress and Future Directions | Graphs | <a href="https://arxiv.org/abs/2311.12399">Link</a> | | A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future Directions | LLM Text Detection | <a href="https://arxiv.org/abs/2310.14724">Link</a> | | A Survey on Detection of LLMs-Generated Content | LLM Text Detection | <a href="https://arxiv.org/abs/2310.15654">Link</a> | | Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text | LLM Text Detection | <a href="https://aclanthology.org/2023.ranlp-stud.1/">Link</a> | | Large Language Models for Generative Recommendation: A Survey and Visionary Discussions | Recommendations | <a href="https://arxiv.org/abs/2309.01157">Link</a> | | Large Language Models for Information Retrieval: A Survey | Information Retrieval | <a href="https://arxiv.org/abs/2308.07107">Link</a> | | A Survey on Model Compression for Large Language Models | Model Compression | <a href="https://arxiv.org/abs/2308.07633">Link</a> | | A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges | Vision LLMs | <a href="https://arxiv.org/abs/2501.02189v5">Link</a> | | Knowledge Editing for Large Language Models: A Survey | Knowledge Editing | <a href="https://arxiv.org/abs/2310.16218">Link</a> | | A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods | Knowledge Bases | <a href="https://arxiv.org/abs/2501.13947">Link</a> |</p>]]></content><author><name></name></author><category term="llm,machine-learning,python"/><category term="llm,python,machine-learning"/><summary type="html"><![CDATA[A curated list of survey papers on LLMs]]></summary></entry><entry><title type="html">Python Notes</title><link href="https://emharsha1812.github.io/blog/2025/python-notes/" rel="alternate" type="text/html" title="Python Notes"/><published>2025-01-22T00:12:00+00:00</published><updated>2025-01-22T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/python-notes</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/python-notes/"><![CDATA[<p>Hello everyone,<br/> Below, I am sharing the GitHub repository containing all my Python notebooks, which I created while teaching an AI-ML Ops training program to esteemed DRDO scientists. You can access the GitHub repository here: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">GitHub Link</a>.</p> <p>This repository is a work in progress, and I will continue to update it as I create new notebooks. Here is the current plan for upcoming content:</p> <ol class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Python One-Liners Notebook</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Walrus Operator in Python</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Building a simple project using Python</li> </ol> <p>Feel free to explore and stay tuned for updates!</p> <p>Here’s the direct GitHub link for quick access: <a href="https://github.com/emharsha1812/Python_Programming_Notebooks">https://github.com/emharsha1812/Python_Programming_Notebooks</a></p> <p>Don’t forget to ⭐ star the repository to stay updated with new additions!</p> <h3 id="ps---here-is-a-list-of-helpful-links-that-you-can-reference-from-time-to-time">PS - Here is a list of helpful links that you can reference from time to time</h3> <ol> <li> <p><a href="https://realpython.com/python-iterators-iterables/">Python Iterators</a> - A super friendly blog on Python Iterators. I recommend taking this <a href="https://realpython.com/quizzes/python-iterators-iterables/">quiz</a> as well after reading the whole blog.</p> </li> <li> <p><a href="https://realpython.com/python-memory-management/">Memory Management in Python</a> - One of those “You should definitely know this” topics.</p> </li> </ol>]]></content><author><name></name></author><category term="coding,"/><category term="python"/><category term="coding,"/><category term="python"/><summary type="html"><![CDATA[A collection of Python notebooks for quick reference]]></summary></entry><entry><title type="html">Tech Blog Repository - My Learning Journey</title><link href="https://emharsha1812.github.io/blog/2025/blogstack/" rel="alternate" type="text/html" title="Tech Blog Repository - My Learning Journey"/><published>2025-01-16T09:50:00+00:00</published><updated>2025-01-16T09:50:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/blogstack</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/blogstack/"><![CDATA[<blockquote> <p>Knowledge is of no value unless you put it into practice ~ Anton Chekhov</p> </blockquote> <p>As a software engineer and ML enthusiast, I often come across insightful blogs that provide valuable knowledge and practical insights. This post serves as my personal repository of such resources, organized by topics for easy reference.</p> <h3 id="generative-ai">Generative AI</h3> <ol> <li><a href="https://huyenchip.com/2025/01/16/ai-engineering-pitfalls.html"><em>Common pitfalls when building generative AI applications</em></a> by Chip Huyen</li> </ol> <p>This is a living document that I’ll continue to update as I discover more valuable resources. Each blog has been selected based on its depth, practical applicability, and clear explanations.</p> <p>Feel free to suggest any additions or share your thoughts on these resources!</p> <p><em>Last Updated: January 17, 2025</em></p>]]></content><author><name></name></author><category term="learning,resources"/><category term="learning,resources,tech"/><summary type="html"><![CDATA[A curated collection of influential technical blogs that shaped my understanding]]></summary></entry><entry><title type="html">KAN (Kolmogorov-Arnold Networks)</title><link href="https://emharsha1812.github.io/blog/2025/kan/" rel="alternate" type="text/html" title="KAN (Kolmogorov-Arnold Networks)"/><published>2025-01-07T00:12:00+00:00</published><updated>2025-01-07T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2025/kan</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2025/kan/"><![CDATA[<h2 id="1-mathematical-foundations--architecture">1. Mathematical Foundations &amp; Architecture</h2> <p><strong>Kolmogorov-Arnold Representation Theorem:</strong> KANs are founded on a classic result by A. N. Kolmogorov and V. Arnold, which states that <em>any continuous multivariate function can be represented as a finite superposition of univariate functions</em>(<a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov%E2%80%99s%20theorem%2C%20proposed%20in%201957%2C,The%20CFL%20condition%2C%20introduced%20by">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a>). In practical terms, this theorem guarantees that for a function \(f(x_1,\dots,x_n)\), there exist some continuous 1D functions \(\{\Phi_q\}\) and \(\{\Psi_{q,p}\}\) such that:</p> \[f(x_1,\ldots,x_n) \;=\; \sum_{q=0}^{2n} \; \Phi_q\!\Big( \sum_{p=1}^n \Psi_{q,p}(x_p) \Big)\,,\] <p>i.e. \(f\) can be decomposed into <strong>inner</strong> univariate functions \(\Psi_{q,p}\) (each depending on a single input variable $x_p$) and <strong>outer</strong> univariate functions \(\Phi_q\) aggregated by addition. This theorem provides a constructive blueprint for function approximation using single-variable building blocks, which is the key inspiration for KANs</p> <p><strong>KAN Architecture</strong> - Instead of the traditional neuron model with linear weighted sums and fixed activations, a KAN implements the above idea by making <strong>each edge</strong> of the network carry a <em>learnable univariate function</em>. In other words, every connection between neurons is parameterized as a nonlinear function (originally chosen as a B-spline) rather than a scalar weight. Each neuron simply sums up the outputs of the incoming edge-functions. Formally, if \(z_i^{(l)}\) denotes the \(i\)-th activation in layer \(l\), then a <strong>KAN layer</strong> computes each output neuron \(j\) as: (<a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview</a>) \(z_j^{(l+1)} \;=\; \sum_{i=1}^{N_l} f_{ij}^{(l)}\!\Big(z_i^{(l)}\Big)\,,\)</p> <p>where \(f_{ij}^{(l)}: \mathbb{R}\to\mathbb{R}\) is a learnable univariate function on the edge from neuron \(i\) (layer \(l\)) to neuron \(j\) (layer \(l+1\)). There are no separate linear weight matrices; the nonlinearity of \(f_{ij}\) itself provides the transformation. In the <em>shallowest</em> case (two-layer KAN), this architecture directly mirrors Kolmogorov’s decomposition: the first layer learns inner functions \(h_{p}(x_p)\) on each input dimension, and the second layer learns outer functions \(g_q(\cdot)\) that combine those results <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Quanta Magazine</a>.</p> <p><strong>Parameterized Functions (B-Splines):</strong> In practice, each learnable edge-function \(f_{ij}\) is parameterized as a spline (often a B-spline) with a set of control points that can be tuned during training. B-splines are piecewise polynomial curves defined by control points, offering a flexible yet smooth basis for approximating arbitrary 1D functions. By adjusting the control points, the shape of the spline changes locally without affecting the entire function. This choice ensures the learned activation functions are <em>smooth</em> addressing potential non-smoothness in Kolmogorov’s original construction and stable to train. Each edge thus has multiple parameters (the spline control values) instead of a single weight. For example, a KAN might initialize each \(f_{ij}\) as a near-linear spline and then let training mold each into the required nonlinear shape. This edge-centric design lets KANs <em>dynamically adapt their activation functions</em> to the data, rather than relying on a fixed function like ReLU or tanh.</p> <p><strong>Illustrative Pseudocode:</strong> The following pseudocode contrasts a single layer of an MLP vs. a KAN:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanmlp.svg-480.webp 480w,/assets/img/kanmlp.svg-800.webp 800w,/assets/img/kanmlp.svg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanmlp.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Comparison of MLP Layer with KAN Layer in Pytorch </div> <p>In the KAN layer, <code class="language-plaintext highlighter-rouge">f_ij</code> is a learned function (e.g. a spline) specific to edge \((i\to j)\), replacing both the weight and the neuron’s activation for that connection. The neuron simply aggregates these contributions (here via summation). Deep KANs can be built by stacking such layers, allowing composition of these univariate transformations across multiple levels.</p> <h2 id="2-comparison-with-mlps">2. Comparison with MLPs</h2> <p><strong>Structural Differences:</strong> Traditional Multi-Layer Perceptrons (MLPs) use <em>linear weights</em> and <em>fixed activation functions at neurons</em>, whereas KANs use <em>no linear weights at all</em> – every “weight” is replaced by a flexible function on the input signal. In effect, MLPs learn parameters for <strong>nodes</strong> (the weight matrix between layers is trained, then a fixed nonlinearity like ReLU is applied), while KANs learn parameters for <strong>edges</strong> (each connection has a trainable nonlinear mapping). This leads to a duality: <em>MLP = fixed nonlinearity + learned linear weights; KAN = fixed linear sum + learned nonlinear functions</em>. The figure below (from Liu et al. 2024) illustrates this difference, highlighting that MLPs apply activations at neurons (circles) whereas KANs apply learned functions on each connecting edge before summing.(<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kanvsmlp-480.webp 480w,/assets/img/kanvsmlp-800.webp 800w,/assets/img/kanvsmlp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kanvsmlp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Source: <a href="https://arxiv.org/pdf/2404.19756" target="_blank">Liu et al. (2024)</a> </div> <p><strong>Learnable Functions vs Fixed Weights:</strong> In an MLP, the transformation from layer to layer is \(\sigma(Wx + b)\), with \(\sigma\) (e.g. ReLU) fixed and \(W,b\) learned. In a KAN, the transformation is \(\sum_i f_i(x_i)\) (plus bias if needed), with each $f_i$ being learned and no separate \(W\). Essentially, KANs “allocate” more flexibility per connection, whereas MLPs rely on combining many fixed nonlinear units to build complexity. This means KANs move the bulk of learnable parameters into the activation functions themselves, often resulting in <em>fewer total connections</em> needed than an equivalent MLP (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a>).</p> <p><strong>Expressive Power (Universal Approximation):</strong> Both MLPs and KANs are universal function approximators, but via different theorems. MLPs leverage the Universal Approximation Theorem (with enough neurons, an MLP can approximate any continuous function on a domain), while KANs directly leverage the Kolmogorov-Arnold (K-A) theorem to construct such approximations. In theory, a single hidden-layer KAN with sufficiently complex edge functions can exactly represent any continuous function (the K-A theorem provides an existence proof), whereas an MLP might require many more neurons or layers to approximate the same function with fixed activations. KANs thus excel at modeling functions with complex or “spiky” behavior in each input dimension, because each edge can carve out a detailed univariate relationship. In practice, KANs implement the K-A decomposition <em>explicitly</em>, using B-spline basis functions to approximate the required univariate mappings. This can translate to <em>greater expressivity per parameter</em>.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>).</p> <p><strong>Parameter Efficiency &amp; Neural Scaling:</strong> A striking reported advantage is that <em>much smaller KANs can achieve accuracy comparable or superior to much larger MLPs</em> on certain tasks. Each KAN edge function (with, say, $k$ control points) can encode a nonlinear relation that an MLP might need multiple neurons and layers to capture. Empirically, Liu <em>et al.</em> (2024) found KANs follow faster <strong>neural scaling laws</strong> – the error decreases more rapidly as model size increases, compared to MLPs. In other words, to reach a given accuracy, a KAN required fewer trainable parameters than an MLP in their tests. The flexibility of splines allows KANs to fit complex patterns without blowing up the network width/depth. One study noted that KANs can <em>match</em> MLP performance at equal parameter counts, and sometimes exceed it, though they require careful tuning (<a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=KANs%20definitely%20don%27t%20feel%20like,compared%20to%20regular%20neural%20networks">Trying Kolmogorov-Arnold Networks in Practice</a>). The original KAN paper demonstrated that a KAN with significantly fewer nodes could outperform a dense ReLU network on function-fitting benchmarks.</p> <p><strong>Continuous Learning and Locality:</strong> Because each KAN weight is a localized function (with local control points), learning in a KAN can be more localized. This has implications for <strong>continual learning</strong>. In standard nets, fine-tuning on new data often alters weights globally and can erode old capabilities (catastrophic forgetting). In KANs, adding new data primarily adjusts the spline control points <em>in relevant regions of the input space</em>, leaving other regions (and other functions) mostly unchanged. For example, if a KAN-based language model learns a new vocabulary or coding style, only certain edge-functions for those inputs might reshape, while others retain their previously learned shape. This property means KANs can integrate new knowledge without overwriting all weights, potentially enabling more <strong>seamless continual learning</strong>. MLPs, by contrast, have distributed representations where a single weight doesn’t correspond to an isolated input relationship, making targeted updates harder.(<a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,local%20control%20point%20parameters%20change">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a>)</p> <p><strong>Interpretability:</strong> A major motivation for KANs is interpretability. In an MLP, each weight by itself is usually not meaningful, and neurons combine many weights making interpretation difficult. In a KAN, each edge’s function $f_{ij}(x)$ can be visualized as a curve, directly showing how the input from neuron $i$ influences neuron $j$ across the range of values. After training, one can <em>extract these learned univariate functions</em> and inspect them.They might correspond to intuitive relations (e.g. an edge function might learn a sinusoidal shape if the output depends sinusoidally on an input).This transparency is especially useful in scientific or engineering tasks where understanding the learned model is as important as its accuracy. MLPs lack this fine-grained interpretability, since their learned mapping is entangled across many parameters. Thus, KANs offer a more human-understandable model: as the saying goes, they turn the <strong>“black box”</strong> into a collection of readable 1D transformations.</p> <p><strong>Summary:</strong> KANs and MLPs both approximate complex functions, but KANs do so by <em>baking learnable math into the connections</em>. This difference yields advantages in function approximation fidelity, parameter efficiency, and interpretability. However, it also comes with computational challenges (will uupdate later). In essence, KANs can be seen as a <strong>new paradigm</strong>: they trade the simple, generic structure of MLPs for a structure with built-in mathematical richness (the Kolmogorov-Arnold basis). This seemingly small change – moving from scalar weights to learned functions – has profound implications on how the network learns and what it can represent (<a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a>).</p> <p>The original paper can be found <a href="https://arxiv.org/pdf/2404.19756">here</a></p> <p>Last Updated - 25/02/2025</p> <h3 id="references">References</h3> <p>[1] - <a href="https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov,the%20model%E2%80%99s%20flexibility%20and%20interpretability">A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)</a></p> <p>[2] - <a href="https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement">Novel Architecture Makes Neural Networks More Understandable</a></p> <p>[3] - <a href="https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than">OpenReview on KAN: Kolmogorov–Arnold Networks</a></p> <p>[4] - <a href="https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node">Trying Kolmogorov-Arnold Networks in Practice</a></p> <p>[5] - <a href="https://www.datacamp.com/tutorial/kolmogorov-arnold-networks#:~:text=simpler%2C%20univariate%20ones,edges%20are%20used%20for%20approximation">Kolmogorov-Arnold Networks (KANs): A Guide With Implementation</a></p> <p>[6] - <a href="https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting">Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists</a></p> <p>[7] - <a href="https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here">GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks</a></p> ]]></content><author><name></name></author><category term="llm,machine-learning,python"/><category term="kan,deep-learning,research"/><summary type="html"><![CDATA[An Alternative to traditional MLPs]]></summary></entry><entry><title type="html">A Visit to Hungarian Mathematics</title><link href="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/" rel="alternate" type="text/html" title="A Visit to Hungarian Mathematics"/><published>2024-09-01T00:12:00+00:00</published><updated>2024-09-01T00:12:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/hungarian-mathematics</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/hungarian-mathematics/"><![CDATA[<p>Because my domain is machine learning (ML) and artificial intelligence (AI), it’s quite common for me to come across abstract mathematical equations that initially made no sense to me. However, as I dove deep into those arcane-looking symbols, I discovered another interesting thing aside from the meaning: I realized that most of the founders of these equations were from Hungary!</p> <p>I googled, and then, a few clicks later, I stumbled on this very interesting read titled <strong>‘A Visit to Hungarian Mathematics’</strong>. Here’s the <a href="https://gwern.net/doc/math/1993-hersh.pdf">link</a> 🔗 to this pape . It contained exactly what I was looking for; a detailed introspection into Hungarian mathematicss and the mathematicians surrounding them. While reading the paper, I found a very simple but profoundly impactful quote that I would like to share. It says</p> <blockquote> <p>Everyone has ideas, both right ideas and wrong ideas. Scientific work consists merely of seperating them</p> </blockquote> <p>I immediately started voraciously reading the paper from top to bottom, left to right, and backwards too. Even though I am not a mathematician and do not have a mathematics degree (although I have a fairly good amount of mathematical exposure), I love to explore mathematics as a hobby. I sincerely hope that my interest in mathematics is still relevant.</p> <p>One of the key takeaways from this paper will be a quote by Alfred Renyi, a Hungarian mathematician known for his work in probability theory, combinatorics, and other fields. Once, a gifted mathematician told him that his working ability was heavily dependent on external circumstances. Renyi responded,</p> <blockquote> <p><strong>“When I’m unhappy, I use math to find happiness; when I’m content, I use math to maintain my happiness.”</strong></p> </blockquote> <p>Reading about these mathematicians and their passion for pondering, fighting, and finally solving math problems fills me with a deep sense of gratitude towards math.</p>]]></content><author><name></name></author><category term="mathematics,culture"/><category term="mathematics,"/><category term="axioms"/><summary type="html"><![CDATA[Why Hungarians are so darn good at mathematics ?]]></summary></entry><entry><title type="html">Welcome!</title><link href="https://emharsha1812.github.io/blog/2024/welcome/" rel="alternate" type="text/html" title="Welcome!"/><published>2024-08-27T00:00:00+00:00</published><updated>2024-08-27T00:00:00+00:00</updated><id>https://emharsha1812.github.io/blog/2024/welcome</id><content type="html" xml:base="https://emharsha1812.github.io/blog/2024/welcome/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Hi! My name is Harshwardhan Fartale. I am an Active Machine learning enthusiast. I studied electrical engineering at National Institute of Technology, Hamirpur and currently serving as a project associate at Indian Institute of Science Bangalore.]]></summary></entry></feed>