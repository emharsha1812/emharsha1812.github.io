@article{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.14135},
  year={2022}
}

@article{sun2025efficient,
  title={Efficient attention mechanisms for large language models: A survey},
  author={Sun, Yutao and Li, Zhenyu and Zhang, Yike and Pan, Tengyu and Dong, Bowen and Guo, Yuyi and Wang, Jianyong},
  journal={arXiv preprint arXiv:2507.19595},
  year={2025}
}


@article{vaswani2023attentionneed,
  title={Attention Is All You Need}, 
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017},
  url={https://arxiv.org/abs/1706.03762}
}



@techreport{nvidia2022h100,
  title={NVIDIA H100 Tensor Core GPU Architecture},
  author={{NVIDIA Corporation}},
  year={2022},
  institution={NVIDIA},
  url={https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-architecture-whitepaper},
  note={Whitepaper}
}


@misc{keles2022computationalcomplexityselfattention,
      title={On The Computational Complexity of Self-Attention}, 
      author={Feyza Duman Keles and Pruthuvi Mahesakya Wijewardena and Chinmay Hegde},
      year={2022},
      eprint={2209.04881},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.04881}, 
}

@misc{hazyresearch2024brrr,
  title={GPUs Go Brrr},
  author={{Hazy Research}},
  year={2024},
  howpublished={Stanford Hazy Research Blog},
  url={https://hazyresearch.stanford.edu/blog/2024-05-12-tk},
  note={Accessed: 2025-11-29}
}
