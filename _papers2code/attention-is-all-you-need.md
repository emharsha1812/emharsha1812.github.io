---
layout: page
title: "Attention Is All You Need - Transformer Implementation"
description: "Complete PyTorch implementation of the Transformer architecture"
img: assets/img/attention.png
importance: 1
category: deep-learning
github: https://github.com/emharsha1812/Paper2sCode/tree/main/attention_is_all_you_need
arxiv: https://arxiv.org/abs/1706.03762
---


Hi there! This is a complete PyTorch implementation of the Transformer architecture from scratch, inspired by the seminal paper "Attention Is All You Need" by Vaswani et al. (2017). The implementation includes all components of the Transformer model, such as multi-head attention, positional encoding, and feed-forward networks.

## [Github Repository](https://github.com/emharsha1812/Paper2sCode/tree/main/attention_is_all_you_need)