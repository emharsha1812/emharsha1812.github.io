---
layout: post
title: KAN (Kolmogorov-Arnold Networks)
date: 2025-01-07 00:12:00
description: An Alternative to traditional MLPs
tags: kan,deep-learning,research
categories: llm,machine-learning,python
tabs: true
nav: false
draft: false
---
## 1. Mathematical Foundations & Architecture

**Kolmogorov-Arnold Representation Theorem:** KANs are founded on a classic result by A. N. Kolmogorov and V. Arnold, which states that *any continuous multivariate function can be represented as a finite superposition of univariate functions*([A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)](https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov%E2%80%99s%20theorem%2C%20proposed%20in%201957%2C,The%20CFL%20condition%2C%20introduced%20by)). In practical terms, this theorem guarantees that for a function $$f(x_1,\dots,x_n)$$, there exist some continuous 1D functions $$\{\Phi_q\}$$ and $$\{\Psi_{q,p}\}$$ such that: 

$$
f(x_1,\ldots,x_n) \;=\; \sum_{q=0}^{2n} \; \Phi_q\!\Big( \sum_{p=1}^n \Psi_{q,p}(x_p) \Big)\,,
$$ 

i.e. $$f$$ can be decomposed into **inner** univariate functions $$\Psi_{q,p}$$ (each depending on a single input variable $x_p$) and **outer** univariate functions $$\Phi_q$$ aggregated by addition. This theorem provides a constructive blueprint for function approximation using single-variable building blocks, which is the key inspiration for KANs


**KAN Architecture** - Instead of the traditional neuron model with linear weighted sums and fixed activations, a KAN implements the above idea by making **each edge** of the network carry a *learnable univariate function*. In other words, every connection between neurons is parameterized as a nonlinear function (originally chosen as a B-spline) rather than a scalar weight. Each neuron simply sums up the outputs of the incoming edge-functions. Formally, if $$z_i^{(l)}$$ denotes the $$i$$-th activation in layer $$l$$, then a **KAN layer** computes each output neuron $$j$$ as:
([OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than))
$$
z_j^{(l+1)} \;=\; \sum_{i=1}^{N_l} f_{ij}^{(l)}\!\Big(z_i^{(l)}\Big)\,,
$$ 

where $$f_{ij}^{(l)}: \mathbb{R}\to\mathbb{R}$$ is a learnable univariate function on the edge from neuron $$i$$ (layer $$l$$) to neuron $$j$$ (layer $$l+1$$). There are no separate linear weight matrices; the nonlinearity of $$f_{ij}$$ itself provides the transformation. In the *shallowest* case (two-layer KAN), this architecture directly mirrors Kolmogorov’s decomposition: the first layer learns inner functions $$h_{p}(x_p)$$ on each input dimension, and the second layer learns outer functions $$g_q(\cdot)$$ that combine those results [Quanta Magazine](https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement).

**Parameterized Functions (B-Splines):** In practice, each learnable edge-function $$f_{ij}$$ is parameterized as a spline (often a B-spline) with a set of control points that can be tuned during training. B-splines are piecewise polynomial curves defined by control points, offering a flexible yet smooth basis for approximating arbitrary 1D functions. By adjusting the control points, the shape of the spline changes locally without affecting the entire function. This choice ensures the learned activation functions are *smooth* addressing potential non-smoothness in Kolmogorov’s original construction and stable to train. Each edge thus has multiple parameters (the spline control values) instead of a single weight. For example, a KAN might initialize each $$f_{ij}$$ as a near-linear spline and then let training mold each into the required nonlinear shape. This edge-centric design lets KANs *dynamically adapt their activation functions* to the data, rather than relying on a fixed function like ReLU or tanh.

**Illustrative Pseudocode:** The following pseudocode contrasts a single layer of an MLP vs. a KAN:

{% include figure.liquid loading="eager" path="assets/img/kanmlp.svg" class="img-fluid rounded z-depth-1" %}
<div class="caption">
    Comparison of MLP Layer with KAN Layer in Pytorch
</div>



In the KAN layer, `f_ij` is a learned function (e.g. a spline) specific to edge $$(i\to j)$$, replacing both the weight and the neuron’s activation for that connection. The neuron simply aggregates these contributions (here via summation). Deep KANs can be built by stacking such layers, allowing composition of these univariate transformations across multiple levels.

## 2. Comparison with MLPs

**Structural Differences:** Traditional Multi-Layer Perceptrons (MLPs) use *linear weights* and *fixed activation functions at neurons*, whereas KANs use *no linear weights at all* – every “weight” is replaced by a flexible function on the input signal. In effect, MLPs learn parameters for **nodes** (the weight matrix between layers is trained, then a fixed nonlinearity like ReLU is applied), while KANs learn parameters for **edges** (each connection has a trainable nonlinear mapping). This leads to a duality: *MLP = fixed nonlinearity + learned linear weights; KAN = fixed linear sum + learned nonlinear functions*. The figure below (from Liu et al. 2024) illustrates this difference, highlighting that MLPs apply activations at neurons (circles) whereas KANs apply learned functions on each connecting edge before summing.([GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks](https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here))

{% include figure.liquid loading="eager" path="assets/img/kanvsmlp.png" class="img-fluid rounded z-depth-1" %}
<div class="caption">
    Source: <a href="https://arxiv.org/pdf/2404.19756" target="_blank">Liu et al. (2024)</a>
</div>

**Learnable Functions vs Fixed Weights:** In an MLP, the transformation from layer to layer is $$\sigma(Wx + b)$$, with $$\sigma$$ (e.g. ReLU) fixed and $$W,b$$ learned. In a KAN, the transformation is $$\sum_i f_i(x_i)$$ (plus bias if needed), with each $f_i$ being learned and no separate $$W$$. Essentially, KANs “allocate” more flexibility per connection, whereas MLPs rely on combining many fixed nonlinear units to build complexity. This means KANs move the bulk of learnable parameters into the activation functions themselves, often resulting in *fewer total connections* needed than an equivalent MLP ([Trying Kolmogorov-Arnold Networks in Practice](https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node)).

**Expressive Power (Universal Approximation):** Both MLPs and KANs are universal function approximators, but via different theorems. MLPs leverage the Universal Approximation Theorem (with enough neurons, an MLP can approximate any continuous function on a domain), while KANs directly leverage the Kolmogorov-Arnold (K-A) theorem to construct such approximations. In theory, a single hidden-layer KAN with sufficiently complex edge functions can exactly represent any continuous function (the K-A theorem provides an existence proof), whereas an MLP might require many more neurons or layers to approximate the same function with fixed activations. KANs thus excel at modeling functions with complex or “spiky” behavior in each input dimension, because each edge can carve out a detailed univariate relationship. In practice, KANs implement the K-A decomposition *explicitly*, using B-spline basis functions to approximate the required univariate mappings. This can translate to *greater expressivity per parameter*.([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting)). 

**Parameter Efficiency & Neural Scaling:** A striking reported advantage is that *much smaller KANs can achieve accuracy comparable or superior to much larger MLPs* on certain tasks. Each KAN edge function (with, say, $k$ control points) can encode a nonlinear relation that an MLP might need multiple neurons and layers to capture. Empirically, Liu *et al.* (2024) found KANs follow faster **neural scaling laws** – the error decreases more rapidly as model size increases, compared to MLPs. In other words, to reach a given accuracy, a KAN required fewer trainable parameters than an MLP in their tests. The flexibility of splines allows KANs to fit complex patterns without blowing up the network width/depth. One study noted that KANs can *match* MLP performance at equal parameter counts, and sometimes exceed it, though they require careful tuning ([Trying Kolmogorov-Arnold Networks in Practice](https://cprimozic.net/blog/trying-out-kans/#:~:text=KANs%20definitely%20don%27t%20feel%20like,compared%20to%20regular%20neural%20networks)). The original KAN paper demonstrated that a KAN with significantly fewer nodes could outperform a dense ReLU network on function-fitting benchmarks.

**Continuous Learning and Locality:** Because each KAN weight is a localized function (with local control points), learning in a KAN can be more localized. This has implications for **continual learning**. In standard nets, fine-tuning on new data often alters weights globally and can erode old capabilities (catastrophic forgetting). In KANs, adding new data primarily adjusts the spline control points *in relevant regions of the input space*, leaving other regions (and other functions) mostly unchanged. For example, if a KAN-based language model learns a new vocabulary or coding style, only certain edge-functions for those inputs might reshape, while others retain their previously learned shape. This property means KANs can integrate new knowledge without overwriting all weights, potentially enabling more **seamless continual learning**. MLPs, by contrast, have distributed representations where a single weight doesn’t correspond to an isolated input relationship, making targeted updates harder.([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,local%20control%20point%20parameters%20change))

**Interpretability:** A major motivation for KANs is interpretability. In an MLP, each weight by itself is usually not meaningful, and neurons combine many weights making interpretation difficult. In a KAN, each edge’s function $f_{ij}(x)$ can be visualized as a curve, directly showing how the input from neuron $i$ influences neuron $j$ across the range of values. After training, one can *extract these learned univariate functions* and inspect them.They might correspond to intuitive relations (e.g. an edge function might learn a sinusoidal shape if the output depends sinusoidally on an input).This transparency is especially useful in scientific or engineering tasks where understanding the learned model is as important as its accuracy. MLPs lack this fine-grained interpretability, since their learned mapping is entangled across many parameters. Thus, KANs offer a more human-understandable model: as the saying goes, they turn the **“black box”** into a collection of readable 1D transformations.

**Summary:** KANs and MLPs both approximate complex functions, but KANs do so by *baking learnable math into the connections*. This difference yields advantages in function approximation fidelity, parameter efficiency, and interpretability. However, it also comes with computational challenges (will uupdate later). In essence, KANs can be seen as a **new paradigm**: they trade the simple, generic structure of MLPs for a structure with built-in mathematical richness (the Kolmogorov-Arnold basis). This seemingly small change – moving from scalar weights to learned functions – has profound implications on how the network learns and what it can represent ([GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks](https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here)).

The original paper can be found [here](https://arxiv.org/pdf/2404.19756)

Last Updated - 25/02/2025

### References
[1] - [A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)](https://arxiv.org/html/2407.11075v4#:~:text=Kolmogorov,the%20model%E2%80%99s%20flexibility%20and%20interpretability)

[2] - [Novel Architecture Makes Neural Networks More Understandable](https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Liu%20worked%20on%20the%20idea,neurons%20%E2%80%94%20a%20common%20arrangement)

[3] - [OpenReview on KAN: Kolmogorov–Arnold Networks](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than)

[4] - [Trying Kolmogorov-Arnold Networks in Practice](https://cprimozic.net/blog/trying-out-kans/#:~:text=single%20output%20node)

[5] - [Kolmogorov-Arnold Networks (KANs): A Guide With Implementation](https://www.datacamp.com/tutorial/kolmogorov-arnold-networks#:~:text=simpler%2C%20univariate%20ones,edges%20are%20used%20for%20approximation)

[6] - [Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting)

[7] - [GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks](https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here)


<!-- ## 3. Analysis of Key Papers on KANs

**Liu et al. (2024) – “KAN: Kolmogorov-Arnold Networks” (arXiv:2404.19756):** This paper by Z. Liu and colleagues introduced KANs as a *more interpretable and potentially more efficient alternative* to standard MLPs ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Kolmogorov,faster%20neural%20scaling%20laws%20than)). The core contribution is the **KAN architecture** itself, inspired directly by the Kolmogorov-Arnold theorem. The authors replace all neural network weights with spline-based functions, demonstrating that this “seemingly simple change” yields significant benefits ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=%28,faster%20neural%20scaling%20laws%20than)). Key contributions and findings from the paper include:

- **Architecture & Theory:** The paper formalizes the KAN design and shows it is a universal approximator (recovering Kolmogorov’s result). It draws a parallel between MLPs (grounded in the universal approximation theorem) and KANs (grounded in the K-A theorem), positioning them as dual approaches ([GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks](https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here)). The authors emphasize how having activation functions on edges (instead of nodes) makes KANs both *“more accurate and interpretable”* than MLPs in many cases ([GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks](https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here)).

- **Empirical Performance:** Liu *et al.* demonstrate that on several *small-scale tasks* – including fitting mathematical functions and solving certain PDEs – **smaller KAN models matched or surpassed the accuracy of much larger MLPs** ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=univariate%20function%20parametrized%20as%20a,faster%20neural%20scaling%20laws%20than)). For instance, a KAN with fewer hidden units outperformed a dense ReLU network in fitting a complicated target function, thanks to the rich edge-wise nonlinearities. They also note that *neural scaling laws* favor KAN: as you increase model size or training data, KANs’ performance improves faster than MLPs’ ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=univariate%20function%20parametrized%20as%20a,collaborators%27%27%20helping%20scientists%20%28re%29discover)).

- **Interpretability & Scientific Discovery:** A highlight of the paper is the demonstration of interpretability in *scientific applications*. They provide two case studies (one in mathematics, one in physics) where KANs acted as “collaborators” to scientists ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Theoretically%20and%20empirically%2C%20KANs%20possess,KANs%2C%20their%20improved%20accuracy%20and)). For example, in a mathematical experiment, a KAN was trained on data generated from a known formula; by examining the learned spline functions, the researchers could **rediscover the underlying formula or law** that produced the data ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Theoretically%20and%20empirically%2C%20KANs%20possess,KANs%2C%20their%20improved%20accuracy%20and)). Similarly, in a physics problem, the KAN’s learned functions corresponded to meaningful physical relationships, showing that KANs can yield insight into the data-generating process – something traditional neural nets rarely offer. This supports the claim that KANs aren’t just accurate, but *explainable* AI tools for scientific discovery ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=Theoretically%20and%20empirically%2C%20KANs%20possess,KANs%2C%20their%20improved%20accuracy%20and)).

- **Challenges Noted:** While extolling KANs’ advantages, the paper is candid about current drawbacks: training a network of spline-functions was slower and more complex than training an equivalent MLP ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=interact%20with%20human%20users,make%20KANs%27%20training%20more%20efficient)). The authors observed that optimization takes longer (due to the added complexity per parameter), and existing deep learning infrastructure isn’t optimized for this type of network. They stress that **further research is needed to improve KAN training efficiency** ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=interact%20with%20human%20users,make%20KANs%27%20training%20more%20efficient)). Despite this, their results were compelling enough that the paper was accepted at ICLR 2025 as an oral presentation, underscoring the significance of the contribution.

In summary, Liu et al. (2024) established KANs as *“promising alternatives for MLPs”*, providing both theoretical foundation and empirical evidence for their advantages ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=interact%20with%20human%20users,make%20KANs%27%20training%20more%20efficient)). They open-sourced a library `pykan` and encouraged the community to build on these results.

**Association of Data Scientists (ADaSci) Article (May 2024) – “Revolutionizing Language Models with KAN: A Deep Dive”:** This article ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=1,this%20approximation%2C%20offering%20a%20different)) ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,and%20enabling%20seamless%20continual%20learning)) provides an accessible overview of KANs with a particular focus on their implications for **large language models (LLMs)**. While not a peer-reviewed paper, it distills the excitement around KANs around mid-2024 and connects it to trends in continual learning and model design. Core points from the article include:

- **Key Differences from MLPs:** The article reiterates the fundamental differences: KANs use *learnable functions instead of static weight matrices*, and specifically B-splines as the function class ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=1,this%20approximation%2C%20offering%20a%20different)). It compares this to the Universal Approximation Theorem, emphasizing that KANs too have a universal function approximation capability, but via a “different mathematical approach” using B-splines ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=2.%20Universal%20Approximation%20with%20B,often%20suffer%20from%20catastrophic%20forgetting)). The piece nicely summarizes how *local control points* in a spline give KANs a flexibility that standard networks lack.

- **Continual Learning Emphasis:** A novel angle in this article is the focus on **continual learning**. The author explains that KAN’s use of spline control points can mitigate catastrophic forgetting ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,and%20enabling%20seamless%20continual%20learning)). The intuition given is that when new data or tasks are introduced, *only the relevant portions of each spline (i.e., certain control points) need to be adjusted*, leaving previously learned functional mappings largely intact ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=where%20fine,and%20enabling%20seamless%20continual%20learning)). For example, if an LLM built on KAN learns a new language, it might only modify some edge-functions related to that language’s token patterns, without completely altering functions that were important for English or other previously learned tasks. This property could enable *efficient fine-tuning*, a crucial requirement for LLMs that need continual updates.

- **Challenges for Adoption:** The ADaSci article also provides a candid look at what’s needed for KANs to succeed in practice ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=1.%20Efficient%20Implementations%3A%20,remaining%20purely%20a%20research%20project)). Three main challenges are noted: (1) **Efficiency** – current KAN implementations are not as optimized as MLP/Transformer ones, so improving the speed and memory usage is “crucial” ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=1.%20Efficient%20Implementations%3A%20,established%20architectures%20like%20transformers)). (2) **Demonstration in LLMs** – to gain traction, KANs need to prove themselves by powering large-scale language models that match or beat today’s transformer-based LLMs ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=of%20well,remaining%20purely%20a%20research%20project)). Without a flagship application, the article warns KAN might remain a research curiosity. (3) **Ecosystem** – it stresses the importance of a supportive developer and research community (tools, forums, documentation) around KAN, akin to what transformers have ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=will%20be%20key%20to%20their,for%20KAN%20will%20be%20essential)). Essentially, the piece recognizes that bridging the gap from promising research to widespread adoption will require engineering and community effort in addition to theoretical appeal.

- **Outlook:** The tone of the article is optimistic, calling KAN “groundbreaking” and “poised to revolutionize language model architecture” ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=Kolmogorov,splines)) ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=Kolmogorov,for%20the%20future%20of%20AI)). It concludes that while challenges remain, the potential of KANs – especially in making models more *continually learnable* and interpretable – could be *“significant” for the future of AI* ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=KAN%20presents%20a%20promising%20alternative,technology%20in%20the%20AI%20landscape)). The authors encourage experimentation and note that even if KANs require more work, the payoff of models that are both *efficient learners* and *transparent* could be transformative.

Together, the Liu et al. paper and the ADaSci article provide a balanced perspective: the former rigorously introduces KANs and evidences their benefits on controlled tasks ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=univariate%20function%20parametrized%20as%20a,collaborators%27%27%20helping%20scientists%20%28re%29discover)), and the latter extrapolates those ideas to the realm of large-scale language modeling ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,local%20control%20point%20parameters%20change)). Both works agree that KANs offer a new **mathematically grounded architecture** that can potentially surpass traditional MLP-based networks in key areas, while also highlighting that realizing this potential at scale is an ongoing effort.

*(Aside: Since those key publications, there have been follow-up explorations, including surveys ([A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)](https://arxiv.org/html/2407.11075v4#:~:text=In%20recent%20years%2C%20the%20application,114)), practical reimplementations ([Trying Kolmogorov-Arnold Networks in Practice - Casey Primozic's Homepage](https://cprimozic.net/blog/trying-out-kans/#:~:text=KANs%20definitely%20don%27t%20feel%20like,compared%20to%20regular%20neural%20networks)), and even extensions like complex-valued KANs and convolutional KANs. This indicates a growing interest, although the two works above remain foundational for understanding KAN’s core concept and promise.)*

## 4. Training Methodology

Training a KAN shares many principles with training conventional neural networks (gradient-based optimization, backpropagation) but also introduces unique considerations due to the functional parameters:

- **Backpropagation Through Spline Functions:** KANs are typically trained via stochastic gradient descent (or variants like Adam) by backpropagating errors and updating the spline parameters (control points) ([A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)](https://arxiv.org/html/2407.11075v4#:~:text=relationships%20by%20using%20spline%20functions,adopts%20a%20backpropagation%20algorithm%20similar)). Each edge function $f_{ij}$ is differentiable (B-splines are differentiable up to a certain order), so one can compute $\partial \text{Loss} / \partial (P_{ij,k})$ for each control point of that spline and update it. The training loop looks similar to an MLP’s: forward pass computes outputs via the current splines, then gradients are computed for each control point and applied. In pseudocode, one might use a framework’s auto-differentiation to handle this:

```python
# Pytorch-style pseudocode for one training step
outputs = model(inputs)           # forward pass (with splines internally)
loss = loss_fn(outputs, targets)  
loss.backward()                   # backpropagates through all f_ij
optimizer.step()                  # updates spline control points
```

The key difference is that the parameter space is larger (many control points per edge) and the gradients flow through the piecewise polynomial computations of the splines.

- **Optimization Techniques:** Standard optimizers (SGD, Adam, etc.) have been used to train KANs ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=,01)) ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=,Epoch%20%7Bepoch%2B1%7D%2C%20Loss%3A%20%7Bloss.item)). However, practitioners have noted that *careful tuning* is required. Because splines can be very flexible, one might need to use smaller learning rates or specialized schedulers to avoid oscillations (since a slight change in one control point can drastically change the function shape if not careful). In some implementations, second-order methods like L-BFGS have been experimented with for faster convergence on the spline parameters ([Trying Kolmogorov-Arnold Networks in Practice - Casey Primozic's Homepage](https://cprimozic.net/blog/trying-out-kans/#:~:text=,LBFGS%20Optimizer)). The *PyKAN* library also includes some “tricks” such as **entropy regularization** to encourage certain splines to stay near-zero if not needed (facilitating pruning) and dynamic grid updates (discussed below) ([Demystifying Kolmogorov-Arnold Networks: A Beginner-Friendly Guide with Code · Daniel Bethell](https://daniel-bethell.co.uk/posts/kan/#:~:text=So%2C%20now%20let%E2%80%99s%20train%20this,into%20in%20this%20blog%20post)).

- **Hyperparameters:** Training a KAN introduces new hyperparameters that have no analog in standard nets:
  - *Number of control points per spline:* This determines the complexity of each edge function (akin to the “capacity” of a weight). A larger number of control points (or knots) allows more wiggles in the function. Too few might underfit (function can’t take the needed shape), too many might overfit or slow down training. Liu et al. chose moderate values (e.g. 10 control points) in experiments ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=,and%20enabling%20seamless%20continual%20learning)), but this likely needs tuning per problem.
  - *Spline degree:* Often cubic B-splines are used for smoothness, but one could use linear or quadratic splines as well. Higher-degree splines can represent functions more smoothly but are costlier to compute.
  - *Initialization of functions:* A sensible initialization is to start each $f_{ij}(x)$ as a nearly linear function (so the KAN begins similarly to an MLP). For example, one might initialize a spline to represent $f_{ij}(x) \approx 0$ or $f_{ij}(x)\approx x$ (identity), plus small noise. This ensures the network starts in a reasonable regime. Randomly initialized control points could also be used, but might lead to odd shapes that slow early training. The *survey* literature mentions initialization strategies to maintain stability ([A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)](https://arxiv.org/html/2407.11075v4#:~:text=These%20activation%20functions%20are%20typically,shape%20without%20affecting%20its%20global)) ([A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)](https://arxiv.org/html/2407.11075v4#:~:text=match%20at%20L504%20By%20using,and%20stable%20when%20dealing%20with)).
  - *Regularization:* Because each edge is highly flexible, regularization is important to prevent overfitting. L2 regularization on control points can keep the functions smoother. Additional penalties have been used: e.g., a “spline entropy” penalty which encourages unnecessary spline functions to flatten out (allowing the model to effectively drop redundant connections) ([Demystifying Kolmogorov-Arnold Networks: A Beginner-Friendly Guide with Code · Daniel Bethell](https://daniel-bethell.co.uk/posts/kan/#:~:text=So%2C%20now%20let%E2%80%99s%20train%20this,into%20in%20this%20blog%20post)) ([Demystifying Kolmogorov-Arnold Networks: A Beginner-Friendly Guide with Code · Daniel Bethell](https://daniel-bethell.co.uk/posts/kan/#:~:text=match%20at%20L240%20Image%3A%20KAN,KAN%20after%20training)). Such techniques help maintain interpretability by simplifying the learned model (pruning edges that carry no information).

- **Computational Complexity:** A naive KAN implementation is computationally heavier than an MLP. If each edge has $k$ control points, then computing one edge function might require $O(k)$ operations (evaluating the spline basis). For a fully-connected layer with $m$ inputs and $n$ outputs, a KAN would require on the order of $m \times n \times k$ operations, versus $m \times n$ for a standard linear layer. For example, a MLP layer with 256 inputs and 256 outputs has 65k weight multiplications; a KAN layer with the same connectivity and, say, 10 control points per edge would perform ~650k operations. This overhead, combined with more parameters, means **training is slower**. Indeed, Liu et al. note that KAN training was relatively slow and not as well optimized in modern ML libraries ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=interact%20with%20human%20users,make%20KANs%27%20training%20more%20efficient)). One mitigation is that KANs might not need to be as wide or deep as an MLP for the same accuracy, potentially reducing $m$ or $n$ to compensate for larger $k$. Another is that many edge functions might turn out to be nearly linear or unused, allowing pruning or simplification after training ([Kolmogorov-Arnold Networks (KANs): A Guide With Implementation | DataCamp](https://www.datacamp.com/tutorial/kolmogorov-arnold-networks#:~:text=implement%20learnable%20functions%20along%20the,edges%20are%20used%20for%20approximation)) ([Kolmogorov-Arnold Networks (KANs): A Guide With Implementation | DataCamp](https://www.datacamp.com/tutorial/kolmogorov-arnold-networks#:~:text=architectural%20shift%20allows%20KANs%20to,edges%20are%20used%20for%20approximation)).

- **Dynamic Spline Grid Updates:** A practical issue arises because splines are typically defined over a fixed input range. During training, neuron activations might go beyond the initial range (especially early on or if inputs are unbounded). One recommended technique is to **dynamically update the spline’s domain (grid)** as needed ([A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)](https://arxiv.org/html/2407.11075v4#:~:text=)). For example, if a certain neuron's activation exceeds the current spline knot range, one can extend the knots or rescale the input. This ensures the spline continues to cover the relevant input values. Failing to do so could result in extrapolation, which might be unstable. The KAN authors built in procedures to adjust knot placement based on the distribution of activations observed during training ([A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)](https://arxiv.org/html/2407.11075v4#:~:text=)).

- **Convergence and Stability:** Training KANs can be more finicky. Anecdotally, researchers have found that KANs may require more epochs to converge to low loss, possibly because the optimization landscape is more complex (many more parameters, and nonlinearly related due to spline basis) ([Trying Kolmogorov-Arnold Networks in Practice - Casey Primozic's Homepage](https://cprimozic.net/blog/trying-out-kans/#:~:text=KANs%20definitely%20don%27t%20feel%20like,compared%20to%20regular%20neural%20networks)). Monitoring training curves and adjusting learning rates is important. Some have used a combination of Adam for an initial phase and then switched to LBFGS to refine spline parameters for higher precision fitting ([Trying Kolmogorov-Arnold Networks in Practice - Casey Primozic's Homepage](https://cprimozic.net/blog/trying-out-kans/#:~:text=,LBFGS%20Optimizer)). Despite these challenges, the end result is often a model with very low training error (KANs can fit data extremely well given their flexibility), so early stopping or strong regularization may be needed to avoid overfitting to noise.

In summary, training a KAN is doable with existing deep learning tools (the ADaSci article even provides a PyTorch snippet using `torch.matmul` on a B-spline basis ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=,Implementation)) ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=self,randn%28out_features%2C%20grid_points))). But it requires attentiveness to new hyperparameters and typically runs slower than training an equivalent MLP. The research community is actively looking into improving this – for instance, creating more efficient spline evaluation routines, or exploring alternative parameterizations like Fourier or Chebyshev series which might be optimized by FFTs, etc. The **computational complexity vs. accuracy trade-off** is a current barrier to large-scale KAN training. As better implementations and perhaps hardware support (GPUs/TPUs optimized for spline ops) emerge, we can expect this gap to narrow. The original authors themselves highlight making KAN training more efficient as an important area for future work ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=interact%20with%20human%20users,make%20KANs%27%20training%20more%20efficient)).

## 5. Applications in Large Language Models (LLMs)

Large Language Models (like GPT series, BERT, etc.) currently rely on transformer architectures, which include MLP sub-layers in each block. There is growing interest in how KANs could be applied in this context, given their theoretical benefits:

- **Theoretical Benefits for Language Modeling:** Language modeling involves learning very high-dimensional functions (mapping input token sequences to next-token probabilities). KANs offer two potential advantages here:
  1. **Parameter Efficiency:** If KANs can achieve the same function approximation with fewer neurons (as seen in smaller tasks) ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=univariate%20function%20parametrized%20as%20a,faster%20neural%20scaling%20laws%20than)), an LLM could be made more compact. For example, the feed-forward layers of a transformer (which are essentially two-layer MLPs) could potentially be replaced by a single KAN layer that achieves similar expressivity. This might reduce the depth or width needed. It’s conceivable that a transformer with KAN feed-forward modules would need fewer total parameters to reach the same perplexity on language data, thanks to the richer edge functions.
  2. **Continual Learning and Adaptability:** LLMs often need fine-tuning for specific domains or updating as new data arrives, but they suffer from catastrophic forgetting if not carefully trained. KAN-based LLMs could alleviate this, as highlighted by the ADaSci article: when new language data is introduced, a KAN can adjust its local functions without globally perturbing unrelated parts of the network ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=3.%20Continual%20Learning%20Capability%3A%20,and%20enabling%20seamless%20continual%20learning)). For instance, imagine an LLM that has learned general English, and we fine-tune it on legal text: with KANs, the model might only reshape certain edge functions relevant to legal jargon, while preserving others. This *locality of updates* could make continuous learning feasible, enabling models that accumulate knowledge over time more gracefully than today’s finetuning methods.

- **Interpretability in LLMs:** One of the criticisms of large language models is their black-box nature. KANs could introduce interpretability to some components of an LLM. After training a KAN-based language model, one could, in principle, inspect the learned functions on certain edges. For example, consider a simplified setting: an LLM’s embedding layer outputs a continuous vector for each token; a subsequent KAN layer takes each dimension of that embedding through a learned spline before combining them. Those spline shapes might reveal how the model treats certain ranges of a semantic feature. Perhaps a particular dimension corresponds to “formality level” and the spline on that edge shows that above a certain threshold, the model strongly boosts the probability of words like “Dear Sir” (a hypothesis). While this is speculative, the ability to project high-dimensional behavior onto interpretable 1D pieces is attractive. It might help in *debugging* models or understanding how input qualities (length, sentiment, etc.) are being utilized internally.

- **Efficiency and Inference:** There is a flip side: inference in a KAN-based LLM could be slower due to the overhead of computing splines. However, one could imagine optimizing the frequently used ones. If many learned edge functions turn out to be roughly linear or simple, they could be approximated or fused for faster computation at inference time (essentially pruning the complexity). Another idea is **hybrid models** where only certain parts of the network use KAN-style functions (for crucial pieces where interpretability or adaptivity is needed), and others remain standard. For example, the transformer's self-attention mechanism might remain unchanged, but the feed-forward network (which currently is an MLP like $\text{GeLU}(xW_1)W_2$) could be replaced with a KAN block. This way, the core sequence modeling (attention) stays fast, while the intermediate representation is processed by a potentially more powerful KAN function approximator.

- **Empirical Performance (Current State):** As of now, KANs have not yet been deployed in any state-of-the-art LLM at scale. However, early experiments are encouraging. In a recent overview, researchers found that on **natural language processing tasks, KANs performed roughly on par with standard neural nets** ([Novel Architecture Makes Neural Networks More Understandable | Quanta Magazine](https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Singapore%20was%20more%20mixed,Liu%2C%20those%20results%20were%20not)). Specifically, one report noted KANs and MLPs were “roughly equal” on NLP benchmarks, while KANs excelled in interpretability-focused tasks and MLPs slightly edged them out in certain vision/audio tasks ([Novel Architecture Makes Neural Networks More Understandable | Quanta Magazine](https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Singapore%20was%20more%20mixed,Liu%2C%20those%20results%20were%20not)). This suggests that *at least on smaller language tasks, KANs can achieve similar accuracy*, which is an important proof of concept. To truly assess KANs for LLMs, we’d want to see them scaled to millions or billions of parameters. That hasn’t been done publicly yet. The main bottleneck is training cost – training a large transformer is already resource-intensive, and replacing components with KANs might multiply that cost in the current implementation. So experimental evidence is limited. The ADaSci article explicitly calls for developing “strong language models trained on KAN” as a next step ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=widespread%20adoption.%20,be%20key%20to%20their%20success)). We might expect to see research prototypes (perhaps a KAN-based transformer on a smaller dataset or a specific domain) in the near future.

- **Potential Hybrid Architectures:** One promising approach is a **hybrid KAN-Transformer**. Rather than a pure KAN network handling sequences (which would be conceptually possible, but nontrivial for long sequences), researchers could integrate KANs into the transformer architecture:
  - *KAN Feed-Forward Layers:* Each transformer block contains a feed-forward sub-layer (an MLP). We could replace this with a KAN layer. That means each token’s representation $h$ (after attention) goes through something like $g(\sum_i f_i(h_i))$ instead of $W_2 \sigma(W_1 h)$. This retains the transformer’s sequence modeling capabilities but swaps in KAN’s representational power at the position-wise level.
  - *Progressive or Mixture of Experts:* Another idea is using KANs as expert networks that specialize in certain features. For example, a mixture-of-experts language model could route certain inputs to a KAN expert that is particularly good at, say, arithmetic or dates (where a specific functional form might be needed), while using normal experts elsewhere.
  - *Convolutional KANs:* There is already work on *Convolutional KANs* (extending the idea to CNN layers) ([AntonioTepsich/Convolutional-KANs - GitHub](https://github.com/AntonioTepsich/Convolutional-KANs#:~:text=AntonioTepsich%2FConvolutional,to%20the%20Convolutional%20Layers)). For language, a convolutional KAN could be used for character-level or byte-level models where local patterns matter.

- **Interpretability and Safety:** For applications like LLMs that generate text, having interpretable components is valuable for safety (we might better understand why a model produced a certain output or how a certain toxic response was triggered). If KANs make it easier to analyze parts of the network, developers could identify problematic internal behaviors. For instance, one could inspect the edge functions leading to a “toxicity neuron” and possibly modify or constrain them if they learn an undesirable curve. This remains hypothetical until such LLMs are built, but it’s an enticing prospect for aligning AI behavior with human expectations.

In summary, KANs have clear theoretical appeal for large language models: they promise **greater flexibility** in function modeling and **less catastrophic forgetting**, both of which align with needs in NLP (where we have diverse, evolving data). However, turning that promise into reality will require overcoming efficiency challenges. The next section addresses these and other limitations, as well as future directions, including how the community can move toward integrating KANs with large-scale models.

## 6. Limitations & Future Directions

While Kolmogorov-Arnold Networks show much potential, it’s important to recognize their limitations and the avenues for future research:

**Current Limitations:**

- **Training Efficiency:** The most immediate limitation is that KANs are *computationally expensive* to train. Each weight being a function means orders of magnitude more parameters and compute per layer. As noted, KAN training was significantly slower than MLP training in experiments ([KAN: Kolmogorov–Arnold Networks | OpenReview](https://openreview.net/forum?id=Ozo7qJ5vZi#:~:text=interact%20with%20human%20users,make%20KANs%27%20training%20more%20efficient)). Without further optimization, this makes scaling KANs to very deep or large models difficult. There’s also the issue of GPU/TPU support – current hardware and ML libraries are highly optimized for matrix operations (like dense layers) but not for spline evaluations. This inefficiency is a barrier to using KANs in large-scale applications (e.g., training a 1B-parameter KAN would be daunting today).

- **Memory Footprint:** Relatedly, a KAN can have many more trainable parameters than an equivalent MLP (if each scalar weight becomes, say, a dozen spline parameters). Although KANs might need fewer *neurons*, the total parameter count can still be higher. For instance, a single fully-connected layer of size 1024×1024 in an MLP has about $10^6$ weights; if implemented as KAN with 8 control points per connection, that’s ~$8\times 10^6$ parameters. This can strain memory and may also increase the risk of overfitting if not carefully regularized.

- **Overfitting and Generalization:** The very flexibility that gives KANs power can also lead to overfitting on limited data. An MLP with ReLUs has a sort of built-in smoothing (it’s continuous piecewise linear with limited segments per neuron). A KAN with high-degree splines could, in the worst case, interpolate training points with weird oscillations (like any high-capacity function approximator). Without sufficient data or proper regularization, a KAN might fit noise. The original KAN paper tackled this by working with problems (like PDE solutions) that have underlying smoothness, and by employing pruning of unnecessary functions ([Demystifying Kolmogorov-Arnold Networks: A Beginner-Friendly Guide with Code · Daniel Bethell](https://daniel-bethell.co.uk/posts/kan/#:~:text=match%20at%20L240%20Image%3A%20KAN,KAN%20after%20training)). Still, generalization in KANs needs further study – e.g., how to best limit a spline’s complexity adaptively to the data.

- **Complexity of Architecture:** Implementing KANs is more complex than standard nets. It introduces hyperparameters (knot count, etc.) that a user must set without much guideline yet. There’s a risk that KANs could be *fragile* to hyperparameter choices, requiring more expertise to tune. This contrasts with off-the-shelf architectures like transformers that, while complex, have well-known recipes. Until a similar level of maturity and best practices is developed for KANs, this complexity is a hindrance to widespread adoption. The **lack of a mature ecosystem** (as ADaSci pointed out) means fewer plug-and-play tools or pre-trained models for others to build on ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=of%20well,KAN%20are%20necessary%20for%20its)).

- **Domain Specific Performance:** It’s not yet clear if KANs universally outperform or just match traditional networks in all domains. Some initial studies indicated that while KANs shine in tasks where interpretability or explicit functional structure matters, they did not outperform specialized architectures in domains like vision or audio ([Novel Architecture Makes Neural Networks More Understandable | Quanta Magazine](https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/#:~:text=Singapore%20was%20more%20mixed,Liu%2C%20those%20results%20were%20not)). For example, computer vision tasks often benefit from convolutional inductive biases that KANs (in their basic form) don’t have; an MLP is not state-of-the-art for vision either, but combining KAN with convolution is non-trivial. Similarly, sequence modeling might need recurrence or attention mechanisms beyond what a pure KAN provides. So, KANs might need to be *combined with other architectural ideas* to be competitive in those fields (e.g., a Conv-KAN hybrid).

- **Interpretability at Scale:** KANs are touted as interpretable, but this advantage might diminish as networks grow. It’s relatively easy to plot a handful of learned univariate functions for a small KAN and make sense of them. But imagine a very large KAN (say used in an LLM) – it could have thousands of learned functions. Interpreting that many curves could become impractical; the insight one gains might be local and not holistic. In other words, KANs turn some of the complexity into human-comprehensible pieces, but if there are too many pieces, we face a new kind of complexity. Future research might need to devise summarization methods for KANs (e.g., identifying which learned functions are most important or characteristic of the model’s behavior).

**Future Directions:**

- **Algorithmic Improvements:** A top priority is making KAN training faster and more stable. Research could focus on specialized optimization techniques for functional parameters. For example, using second-order methods or adaptive knot placement to speed convergence. Another idea is *progressive training*: start with a simpler model (fewer spline segments) and gradually increase complexity (refining the spline) as needed – somewhat analogous to progressive growing in GANs or curriculum learning. This could avoid wasting computation on complex functions until required ([A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)](https://arxiv.org/html/2407.11075v4#:~:text=KAN%20leverages%20the%20precise%20approximation,do%20not%20train%20on%20data)). There is also interest in exploring alternative function parameterizations: **Fourier series, wavelets, or neural implicit functions** instead of splines, if they offer computational benefits.

- **Software and Hardware Support:** We may see new libraries or extensions to support KANs. For instance, kernel fusion for spline evaluation on GPUs could cut down overhead. If KANs gain traction, hardware vendors might include primitives for piecewise polynomial evaluation, much like they did for attention mechanisms. On the software side, the community is already building open-source tools (e.g., `pykan` repository ([GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks](https://github.com/KindXiaoming/pykan#:~:text=This%20is%20the%20github%20repo,or%20read%20the%20documentation%20here)), and an emerging “awesome-KAN” list ([Awesome KAN(Kolmogorov-Arnold Network) - GitHub](https://github.com/mintisan/awesome-kan#:~:text=A%20curated%20list%20of%20awesome,Arnold%20Network%20%28KAN))). Over time, we can expect higher-level APIs where defining a KAN layer is as easy as defining a Dense layer today.

- **Integrating KANs with Other Architectures:** One promising direction is combining KANs with **Transformers, CNNs, RNNs**, etc. The *Convolutional KAN* (CKAN) mentioned in recent work extends the idea of learnable functions to convolution filters ([AntonioTepsich/Convolutional-KANs - GitHub](https://github.com/AntonioTepsich/Convolutional-KANs#:~:text=AntonioTepsich%2FConvolutional,to%20the%20Convolutional%20Layers)) – essentially each filter weight could be a function of spatial position, which might capture scale invariances or repeated patterns more efficiently. Similarly, a **Recurrent KAN** could use KAN function approximators in the recurrence equation, potentially modeling time-series with fewer timesteps. These hybrids aim to marry the domain-specific strengths of existing architectures with KAN’s flexibility and interpretability.

- **KAN 2.0 and Scientific Discovery:** The original authors hinted at “KAN 2.0: KANs meet science” ([GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks](https://github.com/KindXiaoming/pykan#:~:text=This%20is%20the%20github%20repo,or%20read%20the%20documentation%20here)), suggesting a continued push to use KANs for discovering scientific laws or simplifying complex data. Future work in this vein will likely apply KANs to real scientific datasets – for instance, using KANs to fit climate models, biological systems, or engineering simulations – and then analyzing the learned edge-functions to see if they correspond to known equations or reveal new insights. Success here could establish KANs as a go-to tool for *explainable AI in scientific computing*.

- **Large-Scale Experiments:** To truly test KANs, the community needs examples at scale. One future direction is training a moderately large language model (say, replacing MLPs with KAN layers in a transformer and training on a substantial text corpus) to see if the theoretical advantages hold up. If such a KAN-Transformer can achieve similar or better language understanding with fewer parameters or improved finetuning capability, it would be a landmark result, potentially steering mainstream development towards KANs. Even if the first attempts only match transformer performance, showing it’s *possible* will be valuable. The ADaSci article notes the importance of having “a competitive working model” to prevent KAN from “remaining purely a research project” ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=of%20well,KAN%20are%20necessary%20for%20its)) – this sentiment likely drives near-future efforts.

- **Complex-Valued and Advanced KANs:** Some very recent work explores **complex-valued KANs** (CKAN) ([[2502.02417] CVKAN: Complex-Valued Kolmogorov-Arnold Networks](https://arxiv.org/abs/2502.02417#:~:text=%5B2502.02417%5D%20CVKAN%3A%20Complex,Valued%20Neural)), where the learned functions operate on complex numbers. This could benefit signal processing or any domain where phase information is key (the paper cited suggests advantages in certain neural network properties by going complex ([[2502.02417] CVKAN: Complex-Valued Kolmogorov-Arnold Networks](https://arxiv.org/abs/2502.02417#:~:text=%5B2502.02417%5D%20CVKAN%3A%20Complex,Valued%20Neural))). Another extension is **probabilistic KANs** or Bayesian interpretation: treating the spline functions in a Bayesian way to quantify uncertainty in the learned function (useful for scientific applications that require error bars on discovered laws).

- **Theory – Understanding When KANs Excel:** On the theoretical side, researchers will examine what classes of problems are *particularly suited* to KANs. It’s been suggested that KANs are especially useful for functions with a **sparse compositional structure** ([Kolmogorov–Arnold Networks: Hype or Deep Learning Revolution?](https://machine-learning-made-simple.medium.com/understanding-kolmogorov-arnold-networks-possible-successors-to-mlps-4f2a912e69df#:~:text=Kolmogorov%E2%80%93Arnold%20Networks%3A%20Hype%20or%20Deep,)) – essentially, when the underlying function can be broken into a few compositions of 1D functions, KANs should learn very efficiently. For highly entangled or truly high-dimensional interactions, an MLP might be equally challenged. Formal results comparing approximation efficiency of KANs vs MLPs for various function classes would be enlightening. This could guide practitioners on when to deploy KANs.

- **Community and Education:** Finally, a softer but crucial direction is building a community knowledge base around KANs. This includes more tutorials, educational content simplifying the math (as several blogs and Medium posts have started doing), and perhaps inclusion of KAN modules in popular frameworks (TensorFlow, PyTorch) as experimental layers. If KANs enter the standard toolkit, more people will try them out in creative ways. In the long run, a feedback loop of theory, implementation, and application will determine how far KANs go. If they live up to their promise, we may see a future where “neural network” no longer just means perceptrons with ReLUs, but could mean a Kolmogorov-Arnold Network that is **mathematically grounded, accurate, and interpretable** – a true marriage of classical functional approximation theory with modern deep learning ([GitHub - KindXiaoming/pykan: Kolmogorov Arnold Networks](https://github.com/KindXiaoming/pykan#:~:text=Perceptrons%20%28MLPs%29,quick%20intro%20of%20KANs%20here)).

**Conclusion:** Kolmogorov-Arnold Networks represent a fascinating development in neural network architecture. They bring decades-old mathematical theorems into the heart of model design, potentially overcoming some limitations of current networks. While not a panacea or drop-in replacement for all tasks yet, they open up a rich design space. With ongoing research to address their challenges, KANs could play a significant role in next-generation AI models, offering a path to networks that are not just powerful, but also *understandable and continually learnable* – qualities highly desired in deploying AI in real-world, especially critical, applications ([Revolutionizing Language Models with KAN: A Deep Dive - Association of Data Scientists](https://adasci.org/revolutionizing-language-models-with-kan-a-deep-dive/#:~:text=KAN%20presents%20a%20promising%20alternative,technology%20in%20the%20AI%20landscape)).

 -->
