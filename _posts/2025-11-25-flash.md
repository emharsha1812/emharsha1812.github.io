---
layout: distill
title: FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers
description: We present a technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with diagrams and Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms.
tags: FlashAttention transformers GPU optimization machine-learning
giscus_comments: true
date: 2025-11-25
featured: true

authors:
  - name: Anonymous

bibliography: 2025-11-25-flash.bib

toc:
  - name: Introduction
  - name: Background
  - name: FlashAttentionV1
  - name: FlashAttentionV2
  - name: FlashAttentionV3
  - name: FlashAttentionV4
  - name: Open Problems and Future Directions
  - name: Appendix A Proving Standard Attention's Quadratic Complexity
  - name: Appendix B

---


## Introduction

The fundamental concept that underpins the transformer architecture is **Attention**. This was originally developed as an enhancement to RNNs for machine translation <d-cite key="bahdanau2016neuralmachinetranslationjointly"> </d-cite>.
However, in 2017, Vaswani et al. <d-cite key="vaswani2023attentionneed"></d-cite> showed that significantly improved performance could be obtained by eliminating the recurrence structure and instead focusing exclusively on the attention mechanism. 

The importance of this mechanism can be explained with the help of the following example

{% include figure.liquid path="assets/img/blog/Figure_1.jpg" class="img-fluid rounded z-depth-1" caption="Attention weights showing how the model resolves ambiguity in word meaning through context. The arrows indicate strong attention connections between 'bank' and contextually relevant words." %}

Consider the sentence **"I swam across the river to get to the other bank."** The word "**bank**" has multiple meanings—it could refer to a financial institution or a riverbank. The attention mechanism helps the model understand context by weighing relationships between words. In this case, the model attends strongly to words like "swam," "across," and "river," which provide contextual clues that "bank" refers to a riverbank rather than a financial institution.


Therefore, the Attention mechanism has become the single most important mechanism driving the growth of Large Language Models. Over the years, several variants of the attention mechanism have been proposed such as Multi Query Attention (MQA) (cite), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), etc. For instance, here's a non-exhaustive taxonomy of efficient attention mechanisms

{% include figure.liquid path="assets/img/attentiontaxonomy.png" class="img-fluid rounded z-depth-1" caption="Figure adapted from <d-cite key='sun2025efficient'></d-cite>" %}



However, the transformer's attention mechanism has a fatal flaw: it scales quadratically both in time and memory with sequence length, resulting in $O(n^2 \cdot d_{\text{model}})$ time complexity and $O(n^2)$ memory complexity. For example, a 2,048-token sequence requires 16 MB of memory for the attention matrix; at 16,384 tokens, this balloons to 1GB per layer. A rigorous mathematical proof is presented in Appendix A. This quadratic scaling makes it prohibitive for processing long sequences beyond 8K-16K tokens without specialized optimizations <d-cite key="keles2022computationalcomplexityselfattention"></d-cite>. While many works aim for sub-quadratic attention using approximations, including Linformer <d-cite key="wang2020linformerselfattentionlinearcomplexity"></d-cite>, Performer <d-cite key="choromanski2022rethinkingattentionperformers"></d-cite>, and Reformer <d-cite key="kitaev2020reformerefficienttransformer"></d-cite>, these methods have seen limited use in large language models. These are approximate attention methods that reduce cost through low-rank projections, kernel approximations, or sparse routing. These assumptions improve asymptotic complexity, but they introduce accuracy and hardware-efficiency tradeoffs, so large-scale models still rely on exact attention.



The FlashAttention series by Tri Dao <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention2"></d-cite><d-cite key="shah2024flashattention3"></d-cite>  looks at the attention bottleneck from a systems angle. Instead of approximating attention and hurting model quality, the idea is to rethink how attention moves data through the GPU. Modern GPUs have a very uneven memory hierarchy, so the cost of moving data often dominates the cost of doing the math. FlashAttention reduces this movement and gets closer to the limits of the hardware. In this blog, we walk through how this idea has evolved. FlashAttention v1 introduced tiled exact attention. FlashAttention v2 improved how work is split across the GPU. FlashAttention v3 added warp specialization, asynchrony, and low precision on Hopper to push utilization even higher. We also refer to what is known about FlashAttention 4, though an official paper is not public yet.


## Background

### GPU Memory Hierarchy

{% include figure.liquid path="assets/img/blog/Figure_2.png" class="img-fluid rounded z-depth-1" caption="Memory Hierarchy with Bandwidth & Memory Size" %}

Every modern processor faces the same fundamental challenge: fast storage is expense and small, while large storage is slow and cheap. 
Modern GPUs organise memory into a hierarchal form which has five distinct levels, each with different characteristics, different access patterns, and different implications for your code. Starting from the fastest and smallest and working towards the slowest and largest, these levels are: registers, shared memory, L1 cache, L2 cache, and global memory. (Figure 2). At the stop of the memory hierarchy sit registers, the fstest storage available on a GPU. Each thread runninng on the GPU has access to a private set of registers - typically up to 255 registers per thread on modern NVIDIA architectures. These registers feed directly into the computatinal units. When a thread performs an arithmatic operation, the operands come from registers and the result goes back to registers. There is no seperate "register access" operation visible to the programmer; registers are simply where the active data lives. The register file on a single Streaming Multiprocessor contains 65,536 registers with each register holding 32 bits. This gives 256 kilobytes of register storage per SM, and these registers are dynamically shared among all threads running on that SM

Unlike registers, which are private to each individual threads, shared memory (SRAM) is shared among all threads in a thread block. It is the primary mechanism for threads to cooperate and community and is **explicitly managed by the programmer rather than automatically managed by the compiler**. Shared memory provides a staging area for data that multiple threads need to access. Rather than having each thread read the same value from slow global memory, one thread can read it once into shared memory, synchronize with the other threads, and then all threads can read from fast shared memory. Secondly, it enables algorithms that require threads to exchange data. This is used by FlashAttention explicitly 


At the bottom of the hierarchy sits global memory - the large DRAM pool that provides the bulk of a GPU's storage capacity. An H100 for example, has 80GBs of HBM memory, operating at around 3,000 gigabytes per second of bandwidth. THis is where your input data starts, where your output data goes, and where any persistent state lives. It is also by far the slowest level of heirarchy, with individual access latencies reaching 400 to 800 clock cycles depending on architecture and access pattern.

<!-- Insert table here -->

When we look at the GPU memory hierarchy in detail, we see a sharp difference in both latency and bandwidth across levels. Registers and shared memory sit close to the compute units and respond within a few cycles. HBM sits hundreds of cycles away with higher bandwidth but much higher latency. This separation means that the location of data often dictates runtime. As an example, the A100 GPU has 40GB of high bandwidth memory (HBM2e) with bandwidth 1.6TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s <d-cite key="nvidia2020a100"></d-cite>. This shows that the on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size.

## Attention is memory bound despite $O(N^{2})$ compute

The efficiency of a kernel is governed by its arithmatic intensity, defined as the number of floating-point operations (FLOPs) performed per byte of memory access. Arithmatic Intensity is commonly used to measure whether operations can be classified as either compute-bound or memory-bound. A process is memory-bound when the execution speed is limited by how fast data can be moved between memory (HBM/RAM) and the processor cores, rather than by how fast the cores can compute.

$$
\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}}
$$

One of the hardware-efficient mechanisms that is now widely adopted in different providers is Fast and Memory-Efficient Exact Attention with IO-Awareness, or FlashAttention. The "IO-Awareness" part of the title describes its core technical principle, which involves optimizing data movement.  

To give a perspective on why it was needed, the standard attention mechanism, which is mathematically defined as 
For Q (Query), K (Key), V (Value) matrices  {belongs to} R^(NxDk) where N is ___ and D is the dimensions of the embedding of each token, has two major flaws.

1. It is very IO dependent (large number of memory accesses in standard attention).

2. The time and memory complexity of self-attention are quadratic (O(N^2)) in sequence length. (Proof given in Appendix A - Why Self Attention is quadratic)

These two major issues make transformers slow and memory-hungry, especially on long sequences. 

FlashAttention addresses this by rethinking attention algorithms through the lens of GPU memory heirarchy awarenesss which is minimizing the data movement between high-bandwidth memory (HBM) and on-chip SRM. 


Problem 1 - The HBM/Latency Problem 




The standard Attention Implementation requires $\Theta(Nd + N^2)$ HBM accesses. This can be computed as 

1. Computing $S = QK^T$: Reads $Q$ and $K$, writes $S$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$

2. Computing $P = \text{softmax}(S)$: Reads $S$, writes $P$ to HBM $\rightarrow$ $\Theta(N^2)$.

3. Computing $O = PV$: Reads $P$ and $V$, writes $O$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$.



## FlashAttention - V1 

FlashAttention v1, published at Neural Information Processing Systems 2022, by Tri Dao and collaborators, introducted two key innovations: tiled attention that processes blocks of queries, keys, and values entirely in SRAM, and an online softmax algorithm that computes exact softmax incrementally without materializing the full attention matrix.

### Online Softmax algorithm enables incremental computation

Standard softmax requires three passes over the data: one pass to find the maximum value for numerical stability. A second pass to compute exponentials and accumulate the normalization sum. A third pass to normalize each element. This can be shown mathematically as follows: 

Given a vector $(x \in \mathbb{R}^n\)$, the stable softmax is computed in three passes:

$$
\text{Pass 1:} \quad 
m = \max_{i} x_i
$$

$$
\text{Pass 2:} \quad 
Z = \sum_{i=1}^{n} e^{\,x_i - m}
$$

$$
\text{Pass 3:} \quad
\text{softmax}(x)_i = \frac{e^{\,x_i - m}}{Z}
$$

This approach gives us the numerically stable softmax. However 
the dependency forces us to materialize the entire attention matrix in the HBM. Moreover, softmax requires global information (Pass 2). Since the denominator sums over all $N$ elements,seemingly requiring the full row before computing any output. 

FlashAttention leverages the idea of online softmax; the idea of computing softmax in blocks, while maintaining a running maximum and a running sum of exponentials. It was discovered by Milakov et al <d-cite key="milakov2018online">  and then used by Child et al <d-cite key="child2019generatinglongsequencessparse"> </d-cite> 

FlashAttention introduced a new way to use the online softmax inside a tiled attention algorithm. This technique allows the softmax statistics to be computed incrementally

### Algebra of Online Softmax

Let vector $x$ be split into two blocks $x^{(1)}$ and $x^{(2)}$. We compute local statistics for each block. The block size is chosen such that 

Local max: $m_1 = \max(x^{(1)})$, $m_2 = \max(x^{(2)})$

Thus the local unnormalised sum becomes: $l_1 = \sum e^{x^{(1)} - m_1}$, $l_2 = \sum e^{x^{(2)} - m_2}$

To combine these, we define the global max $m_{new} = \max(m_1, m_2)$. 
The global sum $l_{new}$ can be updated without revisiting the raw data of block 1 which is $$l_{new} = e^{m_1 - m_{new}} l_1 + e^{m_2 - m_{new}} l_2$$

Let $O_{old}$ be the current accumulated output scaled by the old normalization factor. The correct update is:
$$ O_{new} = \text{diag}(l_{new})^{-1} \left( \text{diag}(l_{old}) e^{m_{old} - m_{new}} O_{old} + e^{m_{cur} - m_{new}} P_{cur} V_{cur} \right) $$

<!-- - **Sliding Window Attention:** Reduces to $O(nw)$ where $w$ is window size, but achieves only $\Omega(nw^{1-\epsilon})$ lower bound
- **Sparse Attention:** Similar rectangular lower bounds apply
- **Polynomial Approximations:** Can achieve $O(nd^p_k d_v)$ which is linear in $n$ but **exponential in polynomial order $p$**[^1] -->


The quadratic complexity of standard self-attention arises from three fundamental operations that each contribute $O(n^2)$ cost:

1. **Score computation:** $O(n^2 d_k)$ - computing all pairwise similarities
2. **Softmax normalization:** $O(n^2)$ - normalizing each of $n^2$ scores
3. **Value aggregation:** $O(n^2 d_v)$ - aggregating all $n^2$ weighted values

This 

## Looking through Hardware-aware lens

Standard implementations of attention, exacerbate this algorithmic complexity through inefficient utilization of hardware resources. These implementations are typically memory-bound, meaning their execution speed is limited not by the arithmetic throughput of the GPU's compute units, but by the bandwidth available to move data between the high-capacity High Bandwidth Memory (HBM) and the high-speed on-chip SRAM. This phenomenon, often referred to as the "memory wall," dictates that as compute capabilities (FLOPs) outpace memory bandwidth improvements, operations with low arithmetic intensity become increasingly expensive <d-cite key="gholami2024ai"> </d-cite>

The FlashAttention series (v1, v2, and v3) by Tri Dao represents a rigorous systems-level intervention to address this bottleneck. Rather than approximating the attention mechanism—a strategy that often degrades model quality—FlashAttention redefines the computation through the lens of IO-awareness. By meticulously accounting for the asymmetric memory hierarchy of modern GPUs, FlashAttention minimizes data movement, effectively breaking the memory wall for exact attention computation. Through this blog, we provide an exhaustive technical examination of this evolution, tracing the algorithmic tiling of version 1, the parallelism optimizations of version 2, and the hardware-specific asynchronous pipelining of version 3. We also touch upon FlashAttention-4, as gathered from sources, though no official paper has been released yet.



## Open Problems and Future Directions

Despite these advances, several challenges remain. One is scaling beyond on-chip limits. Current FlashAttention relies on fitting entire blocks in hundreds of KB of SRAM. But LLMs push contexts to hundreds of thousands or even a million tokens, far beyond what fits on one GPU’s chip. Techniques like PagedAttention <d-cite key="kwon2023efficientmemorymanagementlarge"> </d-cite> (streaming attention from host memory in blocks) or Hydragen <d-cite key="juravsky2024hydragenhighthroughputllminference"> </d-cite> (optimizing shared prefixes) are only beginning to address this, but a fully general solution for trillion-token context still awaits. In theory, FlashAttention is IO-optimal for a given SRAM size so beyond-chip hierarchies (CPU memory, disk) must come into play, raising new algorithmic questions about streaming, compression, and multi-node attention.

Another issue is programming and scheduling. The rapid improvements have largely come from hand-tuned CUDA kernels. Future efficiency will require better integration with compilers and ML frameworks. As Tri Dao notes, there is ongoing effort to make these optimizations “easily programmable” since current designs rely on manual warp scheduling and custom intrinsics. Moving to other platforms (AMD GPUs, TPUs, even CPUs) adds complexity: for example, AMD ROCm now supports FlashAttention via Triton, but the performance gap and engineering effort remain substantial. We see similar concerns in learned kernels: will XLA, MLIR, or DSLs be able to generate these tiled and overlapped patterns? Bridging the gap between compile-time scheduling (static tiling) and runtime adaptivity is an open compiler/hardware co-design problem.

Finally, precision limits and numerical issues persist. Pushing to FP4 or mixed-integer kernels could double throughput again, but needs new algorithmic care (e.g. stochastic rounding, specialized normalization). FlashAttention-4’s lesson – that even math functions can be rethought in software – suggests any future hardware bottleneck (e.g. FP4 support) will inspire creative software solution

### Implications for Long Context LLMs

Together, these principles directly serve the long-context frontier. Faster, memory-frugal attention means models can actually use very large windows of text. Today’s FlashAttention-enabled LLMs already handle contexts of 128K–1M tokens by carefully overlapping computation and memory. Tomorrow’s algorithms will push farther: for instance, if attention kernels reach multi-petaflop rates on next-gen GPUs, then 10× longer sequences become feasible in practice. In short, hardware-software co-design is the key enabler for ultra-long-context LLMs. By combining IO-efficient kernels (tiling and recompute), parallel pipelines, and smart approximations or sparsity, the community is paving the way for Transformer attention to scale to truly massive contexts with manageable compute and memory costs

## Appendix A 
The standard self-attention mechanism used in Transformers is formulated as
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Where:
- $Q \in \mathbb{R}^{n \times d_k}$ is the query matrix
- $K \in \mathbb{R}^{n \times d_k}$ is the key matrix
- $V \in \mathbb{R}^{n \times d_v}$ is the value matrix
- $n$ is the sequence length
- $d_k$ and $d_v$ are the key and value dimensions
- $\sqrt{d_k}$ is the scaling factor for numerical stability

Computing the score matrix involves:

$$
S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}
$$

This matrix multiplication requires computing the dot product between all pairs of query and key vectors. For each of the $n^2$ pairs $(i,j)$, we compute:

$$
S_{ij} = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}
$$

Since each dot product involves $d_k$ multiplications and $d_k - 1$ additions, the total computational cost is:[^1]

$$
\text{Time Complexity (Step 1)} = O(n^2 d_k)
$$

**Memory Complexity:** The $n \times n$ score matrix $S$ requires $O(n^2)$ space to store.

The softmax function is applied row-wise to normalize the attention scores
$$
\text{softmax}(S)_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^n \exp(S_{ik})}
$$

For each row $i \in [n]$, this involves:

- Computing $n$ exponentials: $O(n)$ operations
- Computing the row sum: $O(n)$ operations
- Normalizing each element: $O(n)$ operations

Since we have $n$ rows, the total cost is:

$$
\text{Time Complexity (Step 2)} = O(n^2)
$$


Finally, multiply the normalized attention matrix with the value matrix:

$$
\text{Output} = \text{softmax}(S) \cdot V
$$

Where $\text{softmax}(S) \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{n \times d_v}$.

This matrix multiplication requires computing:
$$
\text{Output}_{ij} = \sum_{k=1}^n \text{softmax}(S)_{ik} \cdot V_{kj}
$$

For each of the $n \times d_v$ elements in the output, we perform $n$ operations, yielding:

$$
\text{Time Complexity (Step 3)} = O(n^2 d_v)
$$

Combining all three steps:

$$
\text{Total Time} = O(n^2 d_k) + O(n^2) + O(n^2 d_v) = O(n^2(d_k + d_v))
$$

In standard Transformer configurations, the embedding dimension $d_{\text{model}}$ is typically divided equally among multiple attention heads. For a single attention head, $d_k \approx d_v \approx \frac{d_{\text{model}}}{h}$ where $h$ is the number of heads. Therefore, the complexity simplifies to:[^2]

$$
\boxed{\text{Time Complexity} = O(n^2 \cdot d_{\text{model}})}
$$

**Memory Complexity:**

The dominant memory requirement comes from storing the intermediate attention score matrix

$$
S \in \mathbb{R}^{n \times n} \quad \Rightarrow \quad \boxed{\text{Memory Complexity} = O(n^2)}
$$

Additional memory requirements include:

- Storing $Q, K, V$ matrices: $O(3 \cdot n \cdot d)$ which is $O(nd)$ overall
- Temporary buffers for computations: $O(nd)$

However, these are dominated by $O(n^2)$ for large sequence lengths.

The quadratic complexity creates severe bottlenecks for processing long sequences:

| Sequence Length | Relative Cost | Memory (GB at $d=768$) |
| :-- | :-- | :-- |
| 512 | 1× | 1.5 |
| 2,048 | 16× | 24 |
| 8,192 | 256× | 384 |
| 65,536 | 16,384× | $>6$ TB |

### Theoretical Lower Bounds

Research using complexity theory has proven that this quadratic complexity is **fundamental and unavoidable** under the Strong Exponential Time Hypothesis (SETH). The key theorem states:[^3][^1]

For the softmax dot-product self-attention with $d_q = \omega(\log n)$, for any $\epsilon > 0$:

$$
\text{Computing self-attention requires } \Omega(n^{2-\epsilon}) \text{ time}
$$

This holds even for:

- **Exact computation** of attention scores
- **Approximate computation** with multiplicative error $\mu$: $|\hat{Y}_{ij} - Y_{ij}| \leq \mu|Y_{ij}|$
- **Approximate computation** with additive error $\mu$: $|\hat{Y}_{ij} - Y_{ij}| \leq \mu$


### Why Self-Attention Cannot Escape Quadratic Complexity

The proof uses reductions from the **Orthogonal Vectors Problem (OVP)**, which is conjectured to require nearly quadratic time. The intuition is:

1. Computing attention requires evaluating pairwise interactions: $O(n^2)$ pairs
2. Each pair requires a dot product computation: $O(d_k)$ time
3. Even with approximations, one must examine enough pairs to distinguish correct answers from incorrect ones
4. Thus, the quadratic barrier in $n$ is unavoidable[^3]


