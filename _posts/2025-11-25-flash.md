---
layout: distill
title: FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers
description: We present a deeply mathematical & technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms.
tags: FlashAttention transformers GPU optimization machine-learning
giscus_comments: true
date: 2025-11-25
featured: true

authors:
  - name: Anonymous

bibliography: 2025-11-25-flash.bib

toc:
  - name: Introduction
  - name: Background
  - name: FlashAttentionV1
  - name: FlashAttentionV2
  - name: FlashAttentionV3
  - name: FlashAttentionV4
  - name: Open Problems and Future Directions
  - name: Appendix A Proving Standard Attention's Quadratic Complexity
  - name: Appendix B Proving Standard Attention is Memory Bound

---


## Introduction

The fundamental concept that underpins the transformer architecture is **Attention**. This was originally developed as an enhancement to RNNs for machine translation <d-cite key="bahdanau2016neuralmachinetranslationjointly"> </d-cite>.
However, in 2017, Vaswani et al. <d-cite key="vaswani2023attentionneed"></d-cite> showed that significantly improved performance could be obtained by eliminating the recurrence structure and instead focusing exclusively on the attention mechanism. 

The importance of this mechanism can be explained with the help of the following example

{% include figure.liquid path="assets/img/blog/Figure_1.jpg" class="img-fluid rounded z-depth-1" caption="Attention weights showing how the model resolves ambiguity in word meaning through context. The arrows indicate strong attention connections between 'bank' and contextually relevant words." %}

Consider the sentence **"I swam across the river to get to the other bank."** The word "**bank**" has multiple meanings—it could refer to a financial institution or a riverbank. The attention mechanism helps the model understand context by weighing relationships between words. In this case, the model attends strongly to words like "swam," "across," and "river," which provide contextual clues that "bank" refers to a riverbank rather than a financial institution.


Therefore, the Attention mechanism has become the single most important mechanism driving the growth of Large Language Models. Over the years, several variants of the attention mechanism have been proposed such as Multi Query Attention (MQA) (cite), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), etc. For instance, here's a non-exhaustive taxonomy of efficient attention mechanisms

{% include figure.liquid path="assets/img/attentiontaxonomy.png" class="img-fluid rounded z-depth-1" caption="Figure adapted from <d-cite key='sun2025efficient'></d-cite>" %}



However, the transformer's attention mechanism has a fatal flaw: it scales quadratically both in time and memory with sequence length, resulting in $O(n^2 \cdot d_{\text{model}})$ time complexity and $O(n^2)$ memory complexity. For example, a 2,048-token sequence requires 16 MB of memory for the attention matrix; at 16,384 tokens, this balloons to 1GB per layer. A rigorous mathematical proof is presented in Appendix A. This quadratic scaling makes it prohibitive for processing long sequences beyond 8K-16K tokens without specialized optimizations <d-cite key="keles2022computationalcomplexityselfattention"></d-cite>. While many works aim for sub-quadratic attention using approximations, including Linformer <d-cite key="wang2020linformerselfattentionlinearcomplexity"></d-cite>, Performer <d-cite key="choromanski2022rethinkingattentionperformers"></d-cite>, and Reformer <d-cite key="kitaev2020reformerefficienttransformer"></d-cite>, these methods have seen limited use in large language models. These are approximate attention methods that reduce cost through low-rank projections, kernel approximations, or sparse routing. These assumptions improve asymptotic complexity, but they introduce accuracy and hardware-efficiency tradeoffs, so large-scale models still rely on exact attention.



The FlashAttention series by Tri Dao <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention2"></d-cite><d-cite key="shah2024flashattention3"></d-cite>  looks at the attention bottleneck from a systems angle. Instead of approximating attention and hurting model quality, the idea is to rethink how attention moves data through the GPU. Modern GPUs have a very uneven memory hierarchy, so the cost of moving data often dominates the cost of doing the math. FlashAttention reduces this movement and gets closer to the limits of the hardware. In this blog, we walk through how this idea has evolved. FlashAttention v1 introduced tiled exact attention. FlashAttention v2 improved how work is split across the GPU. FlashAttention v3 added warp specialization, asynchrony, and low precision on Hopper to push utilization even higher. We also refer to what is known about FlashAttention 4, though an official paper is not public yet.


## Background

### GPU Memory Hierarchy

{% include figure.liquid path="assets/img/blog/Figure_2.png" class="img-fluid rounded z-depth-1" caption="Memory Hierarchy with Bandwidth & Memory Size" %}

Every modern processor faces the same fundamental challenge: fast storage is expense and small, while large storage is slow and cheap. 
Modern GPUs organise memory into a hierarchal form which has five distinct levels, each with different characteristics, different access patterns, and different implications for your code. Starting from the fastest and smallest and working towards the slowest and largest, these levels are: registers, shared memory, L1 cache, L2 cache, and global memory. (Figure 2). At the stop of the memory hierarchy sit registers, the fstest storage available on a GPU. Each thread runninng on the GPU has access to a private set of registers - typically up to 255 registers per thread on modern NVIDIA architectures. These registers feed directly into the computatinal units. When a thread performs an arithmatic operation, the operands come from registers and the result goes back to registers. There is no seperate "register access" operation visible to the programmer; registers are simply where the active data lives. The register file on a single Streaming Multiprocessor contains 65,536 registers with each register holding 32 bits. This gives 256 kilobytes of register storage per SM, and these registers are dynamically shared among all threads running on that SM

Unlike registers, which are private to each individual threads, shared memory (SRAM) is shared among all threads in a thread block. It is the primary mechanism for threads to cooperate and community and is **explicitly managed by the programmer rather than automatically managed by the compiler**. Shared memory provides a staging area for data that multiple threads need to access. Rather than having each thread read the same value from slow global memory, one thread can read it once into shared memory, synchronize with the other threads, and then all threads can read from fast shared memory. Secondly, it enables algorithms that require threads to exchange data. This is used by FlashAttention explicitly 


At the bottom of the hierarchy sits global memory - the large DRAM pool that provides the bulk of a GPU's storage capacity. An H100 for example, has 80GBs of HBM memory, operating at around 3,000 gigabytes per second of bandwidth. THis is where your input data starts, where your output data goes, and where any persistent state lives. It is also by far the slowest level of heirarchy, with individual access latencies reaching 400 to 800 clock cycles depending on architecture and access pattern.

| Memory Level | Capacity | Bandwidth | Latency | Programmer Control |
|:-------------|:---------|:----------|:--------|:-------------------|
| **Registers** | 256 KB/SM | ~100 TB/s | 0 cycles | Automatic |
| **SRAM** | 192 KB/SM | 19 TB/s | ~20 cycles | Explicit |
| **L2 Cache** | 40-50 MB | 12 TB/s | ~200 cycles | Transparent |
| **HBM** | 40-80 GB | 1.5-2 TB/s | ~500 cycles | Explicit |


When we look at the GPU memory hierarchy in detail, we see a sharp difference in both latency and bandwidth across levels. Registers and shared memory sit close to the compute units and respond within a few cycles. HBM sits hundreds of cycles away with higher bandwidth but much higher latency. This separation means that the location of data often dictates runtime. As an example, the A100 GPU has 40GB of high bandwidth memory (HBM2e) with bandwidth 1.6TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s <d-cite key="nvidia2020a100"></d-cite>. This shows that the on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As L2 Cache is not directly controllable by the programmer we focus on the HBM and SRAM

### Attention is memory bound despite $O(N^{2})$ compute

The efficiency of a kernel is governed by its arithmatic intensity, defined as the number of floating-point operations (FLOPs) performed per byte of memory access. Arithmatic Intensity is commonly used to measure whether operations can be classified as either compute-bound or memory-bound. A process is memory-bound when the execution speed is limited by how fast data can be moved between memory (HBM/RAM) and the processor cores, rather than by how fast the cores can compute. Typical examples include elementwise operations such as activation, dropout and reduction operations such as sum, softmax, batch norm, layer norm.
A process is compute-bound when the execution speed is limited by the raw processing power (FLOPS) of the cores such as Matrix Multiplication, Convolution 

$$
\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}}
$$

Standard Attention involves three primary stages
1. Matrix Multiplication: $ S = Q K^T $
2. Softmax: $ P = \text{softmax}(S) $
3. Matrix Multiplication: $ O = P V $

The matrix multiplications ($QK^T$ and $PV$) are compute-bound operations with high arithmetic intensity ($O(N^2 d)$ FLOPs vs $O(N^2)$ IO). However, the intermediate Softmax operation is memory-bound. It requires reading the entire $N \times N$ matrix $S$ from HBM, performing reduction operations, and writing the resulting $P$ matrix back to HBM. This $O(N^2)$ memory traffic saturates the HBM bandwidth, leaving the powerful Tensor Cores idle. (Detailed Proof provided in Appendix B)


## FlashAttention - V1 

One of the hardware-efficient mechanisms now widely adopted across different providers is Fast and Memory-Efficient Exact Attention with IO-Awareness, or FlashAttention. The "IO-Awareness" part of the title describes its core technical principle: optimizing data movement between GPU memory hierarchies.

FlashAttention addresses the dual challenges of speed and memory consumption in transformers, especially on long sequences, by rethinking attention algorithms through the lens of GPU memory hierarchy awareness. The key insight is minimizing data movement between high-bandwidth memory (HBM) and on-chip SRAM.

{% include figure.liquid path="assets/img/blog/Figure_3.png" class="img-fluid rounded z-depth-1" caption="Standard execution loads data from HBM for every step. Kernel fusion and tiling keep data in fast memory longer, reducing memory traffic and improving throughput." %}

FlashAttention v1, published at Neural Information Processing Systems 2022 by Tri Dao and collaborators, introduced two key innovations: **tiled** attention that processes blocks of queries, keys, and values entirely in SRAM, and an **online softmax algorithm** that computes exact softmax incrementally without materializing the full attention matrix.

### Online Softmax Algorithm Enables Incremental Computation

Standard softmax computation requires three sequential passes over the data, making it inherently memory-intensive. The first pass finds the maximum value for numerical stability. The second pass computes exponentials and accumulates the normalization sum. The third pass normalizes each element. This three-pass structure can be expressed mathematically as follows:

Given a vector $x \in \mathbb{R}^n$, the numerically stable softmax is computed in three passes:

$$
\text{Pass 1:} \quad 
m = \max_{i} x_i
$$

$$
\text{Pass 2:} \quad 
d = \sum_{i=1}^{n} e^{\,x_i - m}
$$

$$
\text{Pass 3:} \quad
\text{softmax}(x)_i = \frac{e^{\,x_i - m}}{d}
$$

This approach ensures numerical stability by preventing overflow in the exponential computation. However, this three-pass dependency creates a critical bottleneck: we must materialize the entire attention matrix in HBM before proceeding. The softmax operation requires global information—specifically, the denominator in Pass 2 must sum over all $N$ elements. This seemingly requires loading the full row into memory before computing any output, making the process extremely memory I/O intensive and defeating attempts to tile the computation efficiently.

The breakthrough came from **online softmax**, an algorithmic technique that computes softmax incrementally in blocks while maintaining running statistics for the maximum and the sum of exponentials. This method was originally discovered by Milakov and Gimelshein <d-cite key="milakov2018online"></d-cite> and later applied to sparse attention patterns by Child et al. <d-cite key="child2019generatinglongsequencessparse"></d-cite>. FlashAttention's key innovation was adapting online softmax to work within a tiled attention algorithm, enabling exact softmax computation without materializing the full attention matrix. Instead of waiting for the global maximum $m_N$ across all elements before computing the normalization sum, the algorithm maintains running statistics that are updated as each new block of data is processed. This incremental approach transforms softmax into a streaming computation that can be fused with the surrounding matrix operations.

### Algebra of Online Softmax

The elegance of online softmax lies in its ability to combine partial results without revisiting the raw data. In a streaming (block-wise) setting, we process the sequence in chunks. Let's assume we have processed the first block of keys and have a local maximum $m_{old}$ and a local unnormalized sum $\ell_{old}$ (where $\ell = \sum e^{x_j - m}$). Now, we load a new block of keys and compute their raw scores. From this new block, we find a local maximum $m_{block}$ and a local sum $\ell_{block}$.

For each block, we compute local statistics independently. The block size is chosen such that each block fits comfortably in SRAM, allowing us to process it entirely on-chip.

**Running maximum** (after processing new block):

$$
m_{new} = \max(m_{old}, m_{block})
$$

with initial condition: $m_{old} = -\infty$ for the first block

**Running sum** (relative to current maximum):

$$
\ell_{new} = \sum_{j} e^{x_j - m_{new}}
$$

with initial condition: $\ell_{old} = 0$ for the first block

#### Recurrence Relation for the Sum

The critical mathematical insight is the **rescaling formula** for $\ell_{new}$. To combine the old and new blocks into a valid global state without accessing the old data, we need to express the new sum in terms of the old statistics.

Starting from the definition:

$$
\ell_{new} = \sum_{j \in \text{old}} e^{x_j - m_{new}} + \sum_{j \in \text{block}} e^{x_j - m_{new}}
$$

**Key transformation:** Express terms using the old maximum $m_{old}$:

$$
e^{x_j - m_{new}} = e^{x_j - m_{old}} \cdot e^{m_{old} - m_{new}}
$$

Therefore, for the old block terms:

$$
\sum_{j \in \text{old}} e^{x_j - m_{new}} = e^{m_{old} - m_{new}} \sum_{j \in \text{old}} e^{x_j - m_{old}} = e^{m_{old} - m_{new}} \cdot \ell_{old}
$$

Similarly, for the new block terms:

$$
\sum_{j \in \text{block}} e^{x_j - m_{new}} = e^{m_{block} - m_{new}} \sum_{j \in \text{block}} e^{x_j - m_{block}} = e^{m_{block} - m_{new}} \cdot \ell_{block}
$$

Combining both:

$$
\boxed{\ell_{new} = e^{m_{old} - m_{new}} \cdot \ell_{old} + e^{m_{block} - m_{new}} \cdot \ell_{block}}
$$

We can define the correction factor $\alpha = e^{m_{old} - m_{new}}$. This is the **fundamental rescaling equation** for online softmax. It allows us to update the running sum using only the previous statistics ($m_{old}$ and $\ell_{old}$) and the new block statistics, without ever loading the earlier blocks from HBM again. The exponential correction terms ensure numerical stability throughout the incremental computation by properly rescaling all exponentials relative to the current global maximum. This reduces the operation from 3 passes to 2 passes <d-cite key="dukhan2020two"> </d-cite>

### FlashAttention : Forward Pass

{% include figure.liquid path="assets/img/blog/Figure_4.png" class="img-fluid rounded z-depth-1" caption="Algorithm for FlashAttention Forward Pass. The algorithm partitions inputs into blocks that fit in SRAM, computes attention incrementally using online softmax, and updates running statistics to avoid materializing the full attention matrix in HBM." %}

For each attention head, FlashAttention reduces memory reads and writes by tiling. It loads small blocks of queries, keys, and values from GPU HBM into fast on-chip SRAM, computes attention for that block, and updates the output before moving on to the next block. This limits how often data moves between slow and fast memory, which is the main bottleneck on modern GPUs. Cutting this movement often gives a 2–4× speedup in practice.

#### Mathematical Derivation

{% include figure.liquid path="assets/img/blog/Figure_5.png" class="img-fluid rounded z-depth-1"%}

We define the block sizes based on the available SRAM size $M$. Let $B_c$ be the block size for columns (dimension along $N$ for $K, V$), and let $B_r$ be the block size for rows (dimension along $N$ for $Q, O$). The key constraint is $4 B_c d \le M$ to ensure that $K, V$ blocks and various buffers fit in SRAM. Usually, we set $B_c \approx \lceil \frac{M}{4d} \rceil$ and $B_r \approx \min(\lceil \frac{M}{4d} \rceil, d)$.

The matrices are divided into blocks as follows:

- $Q$ is divided into $T_r = \lceil N/B_r \rceil$ blocks: $Q_1, \dots, Q_{T_r}$
- $K$ and $V$ are divided into $T_c = \lceil N/B_c \rceil$ blocks: $K_1, \dots, K_{T_c}$ and $V_1, \dots, V_{T_c}$
- $O$ is divided into $T_r$ blocks: $O_1, \dots, O_{T_r}$

Ideally, to minimize HBM writes of the output $O$, we want to load a block of $Q$, iterate over all blocks of $K, V$, accumulating the result, and then write $O$ once. However, FlashAttention V1 actually uses an outer loop over $K, V$ and an inner loop over $Q$ to better utilize the SRAM for the larger $K, V$ blocks, though conceptually the accumulation happens per query row.

**Initialization:**

{% include figure.liquid path="assets/img/blog/Figure_6.png" class="img-fluid rounded z-depth-1" caption="Q, K, and V blocks are stored in HBM (blue). Each K and V block is streamed into SRAM, where partial score matrices S(t) and exponentials A(t) are computed (orange). Softmax normalization is accumulated across blocks, and partial outputs O(t) are merged to form the final output." %}

We initialize the output matrix $O = 0 \in \mathbb{R}^{N \times d}$, the running sum vector $\ell = 0 \in \mathbb{R}^N$, and the running maximum vector $m = -\infty \in \mathbb{R}^N$. These statistics will be updated incrementally as we process each block.

**Outer Loop** (iterating over $K, V$ blocks, $j = 1, \dots, T_c$):

1. **Load Key-Value blocks:** Load $K_j, V_j \in \mathbb{R}^{B_c \times d}$ from HBM to on-chip SRAM. These blocks remain in SRAM throughout the inner loop.

   **Inner Loop** (iterating over $Q$ blocks, $i = 1, \dots, T_r$):

   1. **Load Query block and statistics:** Load $Q_i \in \mathbb{R}^{B_r \times d}$, $O_i \in \mathbb{R}^{B_r \times d}$, $\ell_i \in \mathbb{R}^{B_r}$, $m_i \in \mathbb{R}^{B_r}$ from HBM to SRAM.

   2. **Compute attention scores:** Compute $S_{ij} = Q_i K_j^\top \in \mathbb{R}^{B_r \times B_c}$. This matrix multiplication is performed entirely in SRAM, computing the raw attention scores between the current query block and key block.

   3. **Compute local statistics:** For the current block, we compute:
      
      $$\tilde{m}_{ij} = \text{rowmax}(S_{ij}) \in \mathbb{R}^{B_r}$$
      
      (maximum score in each row)
      
      $$\tilde{P}_{ij} = \exp(S_{ij} - \tilde{m}_{ij}) \in \mathbb{R}^{B_r \times B_c}$$
      
      (pointwise exponential with local normalization)
      
      $$\tilde{\ell}_{ij} = \text{rowsum}(\tilde{P}_{ij}) \in \mathbb{R}^{B_r}$$
      
      (sum of exponentials in each row)

   4. **Update running statistics:** Using the online softmax rescaling formula:
      
      $$m_i^{\text{new}} = \max(m_i, \tilde{m}_{ij})$$
      
      (update global maximum)
      
      $$\ell_i^{\text{new}} = e^{m_i - m_i^{\text{new}}} \ell_i + e^{\tilde{m}_{ij} - m_i^{\text{new}}} \tilde{\ell}_{ij}$$
      
      (rescale and accumulate sum)

   5. **Update output:** We incrementally update the attention output. First, compute partial output contribution:
      
      $$\tilde{V}_{ij} = \tilde{P}_{ij} V_j$$
      
      (weighted sum of values for current block)
      
      Then combine with running output using the rescaling formula:
      
      $$O_i^{\text{new}} = \text{diag}(\ell_i^{\text{new}})^{-1} \left( \text{diag}(\ell_i) e^{m_i - m_i^{\text{new}}} O_i + e^{\tilde{m}_{ij} - m_i^{\text{new}}} \tilde{V}_{ij} \right)$$
      
      This rescales the old output and adds the new contribution, then normalizes by the updated sum.

   6. **Write back to HBM:** Write $O_i^{\text{new}}, \ell_i^{\text{new}}, m_i^{\text{new}}$ back to HBM, updating the global state for this query block.


In practice, to avoid numerical instability from dividing by $\ell_i$ at every step, the algorithm often stores the unnormalized output (let's call it $U_i = O_i \cdot \ell_i$) and only divides by $\ell_i$ at the very end of the computation or maintains the invariant correctly. The formulation above effectively rescales the previous running average to match the new magnitude determined by the new maximum.

{% include figure.liquid path="assets/img/blog/Figure_7.png" class="img-fluid rounded z-depth-1" caption="Blockwise computation of attention using online softmax. Q remains in HBM while K and V are streamed in blocks. For each block, partial scores S(t) and exponentials A(t) are computed in SRAM. The running softmax denominator is updated across blocks, and partial outputs O(t) are rescaled and accumulated to form the final output." %}


### Complexity Analysis

The efficiency of FlashAttention is theoretically grounded in its IO complexity. We analyze the number of HBM accesses required by comparing standard attention with FlashAttention.

#### Standard Attention IO Complexity

{% include figure.liquid path="assets/img/blog/Figure_8.png" class="img-fluid rounded z-depth-1"%}


For standard attention, the HBM accesses are:

- Read $Q, K, V$: $O(Nd)$
- Write $S = QK^T$: $O(N^2)$
- Read $S$: $O(N^2)$
- Write $P = \text{softmax}(S)$: $O(N^2)$
- Read $P$ and $V$: $O(N^2) + O(Nd)$
- Write $O = PV$: $O(Nd)$

**Total:** $\Theta(N^2)$ HBM accesses (assuming $N \gg d$)

#### FlashAttention IO Complexity

FlashAttention's tiled algorithm significantly reduces memory traffic:

**Outer loop** (over $K, V$ blocks, $j = 1, \dots, T_c$): Each iteration loads blocks $K_j, V_j$ into SRAM. Since the outer loop iterates over all $K, V$ blocks, these matrices are loaded once in total: $O(Nd)$.

**Inner loop** (over $Q$ blocks, $i = 1, \dots, T_r$): For each outer iteration, the inner loop loads $Q_i$, $O_i$, $\ell_i$, $m_i$ from HBM.

- Size of $Q_i$: $B_r \times d$
- Total inner loop iterations: $T_c \times T_r$
- Total loading of $Q$: $T_c \times (T_r \times B_r d) = T_c \times Nd$

Since $T_c = \lceil N/B_c \rceil$ and $B_c = \Theta(M/d)$:

$$
\text{Total } Q \text{ loads} = \frac{N}{B_c} \times Nd = \frac{N}{M/d} \times Nd = \frac{N^2 d^2}{M}
$$

**Theorem:** For sequence length $N$, head dimension $d$, and SRAM size $M$, FlashAttention requires $O(N^2 d^2 M^{-1})$ HBM accesses.

Since $d^2$ is typically much smaller than $M$ (e.g., $d=64 \implies d^2=4096$, while $M \approx 10^5$ bytes), the ratio $d^2/M \ll 1$. Thus, FlashAttention provides a significant reduction in HBM accesses compared to standard attention's $O(N^2)$ complexity.

#### Lower Bound and Optimality

Dao et al. prove that this complexity is **asymptotically optimal**. <d-cite key="dao2022flashattentionfastmemoryefficientexact"></d-cite> The proof relies on the "Red-Blue Pebble Game," a standard model for analyzing memory hierarchy complexity <d-cite key="demaine2018red"></d-cite>. The attention computation graph involves computing $N^2$ pairwise interactions. To compute these interactions with a limited cache of size $M$, any algorithm must re-stream inputs multiple times. It has been proven that any algorithm computing exact attention must incur $\Omega(N^2 d^2 M^{-1})$ memory accesses. This confirms that FlashAttention is not just an improvement but **IO-optimal** for exact attention computation.

#### Complexity Comparison with Standard Attention

| Metric | Standard Attention | FlashAttention |
|:-------|:-------------------|:---------------|
| **Time Complexity (FLOPs)** | $O(N^2 d)$ | $O(N^2 d)$ |
| **Space Complexity (Memory)** | $O(N^2)$ (stores $S, P$) | $O(N)$ (stores $O, \ell, m$) |
| **IO Complexity (HBM Access)** | $O(N^2)$ | $O(N^2 d^2 M^{-1})$ |


### FlashAttention: Backward Pass

Training deep models requires a backward pass to compute gradients. Standard backpropagation requires the stored attention probability matrix $P$ (size $N \times N$) to compute gradients with respect to $Q$ and $K$. Storing $P$ for long sequences is prohibitively expensive ($O(N^2)$ memory).

FlashAttention solves this through **recomputation**. Instead of saving $P$, it saves only the final output $O$ and the normalization statistics $(\ell, m)$ from the forward pass—both of size $N \times 1$, plus the random seed for dropout. During the backward pass, the kernel reloads $Q$, $K$, $V$ from HBM and uses $m$ and $\ell$ to regenerate the attention scores $S$ and probabilities $P$ block-by-block in SRAM, exactly as they were computed in the forward pass. 

While this recomputation increases FLOPs by repeating the forward matrix multiplications, the reduction in HBM reads—avoiding $O(N^2)$ reads of $P$—results in a net speedup because the operation is memory-bound. The additional compute cost is more than offset by the savings in memory bandwidth.


#### Backward Pass Derivation

The backward pass must compute gradients $d\mathbf{Q}$, $d\mathbf{K}$, $d\mathbf{V}$ given $d\mathbf{O}$ from the loss. Standard backprop saves the $N \times N$ attention matrix $\mathbf{P}$ from the forward pass. FlashAttention instead saves only $\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{O} \in \mathbb{R}^{N \times d}$ and the log-sum-exp statistics $\mathbf{L} \in \mathbb{R}^N$ where $L_i = m_i + \log(\ell_i)$. The $O(N^2)$ matrices $\mathbf{S} = \mathbf{QK}^T$ and $\mathbf{P} = \text{softmax}(\mathbf{S})$ are recomputed on-the-fly in SRAM during backprop.

**Gradient computation.** Recall the forward equations:

$$
\mathbf{S} = \mathbf{QK}^T, \quad \mathbf{P} = \text{softmax}(\mathbf{S}), \quad \mathbf{O} = \mathbf{PV}
$$

Backpropagating through this chain via $\mathcal{L} \xleftarrow{\text{grad}} \mathbf{O} \xleftarrow{\mathbf{P,V}} \mathbf{P} \xleftarrow{\text{softmax}} \mathbf{S} \xleftarrow{\mathbf{Q,K}} \mathbf{Q}, \mathbf{K}$ gives:

**Gradient w.r.t. $\mathbf{V}$:** From $\mathbf{O} = \mathbf{PV}$,

$$
d\mathbf{V} = \mathbf{P}^T d\mathbf{O}
$$

**Gradient w.r.t. $\mathbf{P}$:** 

$$
d\mathbf{P} = d\mathbf{O} \mathbf{V}^T \in \mathbb{R}^{N \times N}
$$

**Gradient through softmax.** For row-wise softmax, the Jacobian is $\frac{\partial P_{ij}}{\partial S_{ik}} = P_{ij}(\delta_{jk} - P_{ik})$ where $\delta$ is the Kronecker delta. Applying the chain rule, for each row $i$:

$$
d\mathbf{S}_i = \mathbf{P}_i \odot d\mathbf{P}_i - \mathbf{P}_i (d\mathbf{P}_i^\top \mathbf{P}_i)
$$

where $\odot$ denotes element-wise multiplication. 

Define $D_i = d\mathbf{P}\_{i:} \cdot \mathbf{P}\_{i:} = \sum_j d\mathbf{P}\_{ij} \mathbf{P}\_{ij}$. Since $d\mathbf{P}\_{ij} = (d\mathbf{O}_i)^\top \mathbf{V}_j$ and $\sum_j \mathbf{P}\_{ij} \mathbf{V}_j = \mathbf{O}_i$:

$$
D_i = \sum_j (d\mathbf{O}_i^\top \mathbf{V}_j) \mathbf{P}_{ij} = d\mathbf{O}_i^\top \mathbf{O}_i
$$

Thus the gradient through softmax becomes:

$$
d\mathbf{S} = \mathbf{P} \odot d\mathbf{P} - \mathbf{P} \odot (d\mathbf{O} \odot \mathbf{O})
$$

**Gradients w.r.t. $\mathbf{Q}$ and $\mathbf{K}$.** Since $\mathbf{S} = \mathbf{QK}^T$:

$$
d\mathbf{Q} = d\mathbf{S} \mathbf{K} \in \mathbb{R}^{N \times d}
$$

$$
d\mathbf{K} = d\mathbf{S}^T \mathbf{Q} \in \mathbb{R}^{N \times d}
$$

**Tiled backward pass with recomputation.** FlashAttention avoids storing $O(N^2)$ attention matrices $\mathbf{P}$ and $\mathbf{S}$ by recomputing them block-by-block in SRAM during the backward pass. This strategy exchanges additional compute for reduced memory requirements. The approach is practical because attention computation is memory-bound—the memory bandwidth savings outweigh the recomputation cost.

#### Backward Pass Setup

**Saved state from forward pass:**

From the forward pass, we save only the minimal information required:
- $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ (all in HBM, size $N \times d$ each)
- $\mathbf{O}$ (output, size $N \times d$)
- Log-sum-exp statistics: $\mathbf{L} \in \mathbb{R}^N$ where $L_i = m_i + \log(\ell_i)$
- Dropout random seed (if used)

**Preprocessing step:**

Before the main loop, we compute an auxiliary vector $\mathbf{D} \in \mathbb{R}^N$:

$$
D_i = \sum_{j=1}^d dO_{ij} \cdot O_{ij}
$$

This vector encodes the diagonal of $(dO \odot O)$ and is reused throughout the backward pass to efficiently compute gradients through softmax for each block. Computing $\mathbf{D}$ requires one pass over $dO$ and $O$, costing $O(Nd)$ HBM accesses.

#### Backward Algorithm (FlashAttention V1)

The backward pass uses the same tiling structure as the forward pass. We iterate over $K, V$ blocks in the outer loop and $Q$ blocks in the inner loop, accumulating gradients into each block as we process them.

**Outer Loop** (over $K, V$ blocks, $j = 1, \dots, T_c$):

1. **Load key-value blocks:** Load $\mathbf{K}_j, \mathbf{V}_j \in \mathbb{R}^{B_c \times d}$ from HBM to SRAM.

2. **Initialize gradient accumulators:** Set $d\mathbf{K}_j = 0$ and $d\mathbf{V}_j = 0$ in SRAM. These accumulators will receive contributions from all query blocks in the inner loop.

   **Inner Loop** (over $Q$ blocks, $i = 1, \dots, T_r$):

   3. **Load query block and statistics:** Load from HBM into SRAM:
      - $\mathbf{Q}_i \in \mathbb{R}^{B_r \times d}$
      - $d\mathbf{Q}_i \in \mathbb{R}^{B_r \times d}$ (partial gradient, accumulated across $j$ iterations)
      - $d\mathbf{O}_i \in \mathbb{R}^{B_r \times d}$ (upstream gradient from backprop)
      - $\mathbf{O}_i \in \mathbb{R}^{B_r \times d}$ (output from forward pass)
      - Statistics: $\mathbf{L}_i, m_i, D_i$ (all size $B_r$)

   4. **Recompute attention scores and probabilities:**
   
      $$\mathbf{S}_{ij} = \mathbf{Q}_i \mathbf{K}_j^\top \in \mathbb{R}^{B_r \times B_c}$$
      
      Recover the attention probabilities using the saved log-sum-exp statistics:
      
      $$\mathbf{P}_{ij} = \exp(\mathbf{S}_{ij} - m_i) / \exp(\mathbf{L}_i - m_i)$$
      
      where subtraction and division are applied row-wise with broadcasting. If dropout was applied during the forward pass, regenerate the dropout mask using the saved random seed and apply it to $\mathbf{P}_{ij}$.

   5. **Compute value gradients:** Accumulate into $d\mathbf{V}_j$:
   
      $$d\mathbf{V}_j \leftarrow d\mathbf{V}_j + \mathbf{P}_{ij}^\top d\mathbf{O}_i$$
      
      This is a $(B_c \times B_r) \times (B_r \times d) \to (B_c \times d)$ matrix multiplication.

   6. **Compute score gradients through softmax:** The gradient flowing backward through softmax is:
   
      $$d\mathbf{S}_{ij} = \mathbf{P}_{ij} \odot \left( d\mathbf{O}_i \mathbf{V}_j^\top - \mathbf{P}_{ij}^\top (d\mathbf{O}_i \odot \mathbf{O}_i) \right)$$
      
      Computing this requires first forming the intermediate matrix:
      
      $$\mathbf{A}_{ij} = d\mathbf{O}_i \mathbf{V}_j^\top$$
      
      Then applying the softmax gradient formula:
      
      $$d\mathbf{S}_{ij} = \mathbf{P}_{ij} \odot \left( \mathbf{A}_{ij} - D_i \right)$$
      
      where $D_i$ is broadcast across the column dimension.

   7. **Compute query gradients:** Accumulate into $d\mathbf{Q}_i$:
   
      $$d\mathbf{Q}_i \leftarrow d\mathbf{Q}_i + d\mathbf{S}_{ij} \mathbf{K}_j$$
      
      This is $(B_r \times B_c) \times (B_c \times d) \to (B_r \times d)$.

   8. **Compute key gradients:** Accumulate into $d\mathbf{K}_j$:
   
      $$d\mathbf{K}_j \leftarrow d\mathbf{K}_j + d\mathbf{S}_{ij}^\top \mathbf{Q}_i$$
      
      This is $(B_c \times B_r) \times (B_r \times d) \to (B_c \times d)$.

   9. **Write query gradients back:** Write updated $d\mathbf{Q}_i$ to HBM. When the outer loop is parallelized across GPU threads or blocks, this requires atomic operations to safely accumulate contributions from multiple $j$ values into the same $d\mathbf{Q}_i$. In sequential execution, this is a standard write operation.

   **End Inner Loop**

3. **Write key-value gradients:** After processing all query blocks, write the accumulated gradients to HBM:

   $$\text{Write } d\mathbf{K}_j, d\mathbf{V}_j \text{ to HBM}$$

**End Outer Loop**

#### Backward Pass Complexity Analysis

**Floating-point operations:**

The backward pass requires recomputing the attention matrices and computing all gradient operations. The FLOP count is:
- Recomputing $\mathbf{S}_{ij} = \mathbf{Q}_i \mathbf{K}_j^\top$: $T_c \times T_r \times 2 B_r B_c d = 2 N^2 d$ FLOPs
- Gradient computations (matrix multiplies for $dV, dQ, dK$): $2 N^2 d$ FLOPs
- **Total backward FLOPs:** $\Theta(N^2 d)$, equivalent to the forward pass

**HBM access (IO complexity):**

- Load $Q, K, V$ for recomputation: $O(N^2 d^2 / M)$ (same scaling as forward pass)
- Load $dO, O$ (single pass): $O(Nd)$
- Load and write statistics $L, m, D$: $O(N)$
- Write $dQ, dK, dV$: $O(Nd)$

**Total backward IO:** $\Theta(N^2 d^2 / M)$ HBM accesses

This matches the forward pass complexity. Critically, we avoid storing and reading the $O(N^2)$ attention matrix, which is the dominant memory cost in standard backpropagation.

#### Comparison: Standard Backpropagation vs. FlashAttention

| Aspect | Standard Backprop | FlashAttention |
|:-------|:-----------------|:---------------|
| **Saved state** | $Q, K, V, P, S$ | $Q, K, V, O, L, m$ |
| **Memory footprint** | $O(Nd + 2N^2)$ | $O(Nd)$ |
| **Backward recomputation** | None | $S, P$ blocks on-the-fly |
| **HBM reads in backward** | $\Theta(N^2)$ from $P$ | $\Theta(N^2 d^2 / M)$ |
| **Backward FLOPs** | $\Theta(N^2 d)$ | $\Theta(N^2 d)$ |

The memory savings of $2N^2$ elements directly offset the recomputation cost of $\Theta(N^2 d)$ FLOPs. On memory-bound hardware where the memory bus saturates before compute cores reach full utilization, this trade-off consistently improves overall throughput.

**Preprocessing step:**

Before the main loop, compute a single auxiliary vector $\mathbf{D} \in \mathbb{R}^N$:

$$
D_i = \sum_{j=1}^d dO_{ij} \cdot O_{ij}
$$

This vector encodes the diagonal of $(dO \odot O)$ and is reused to efficiently compute gradients through softmax for each block. Computing $\mathbf{D}$ requires one HBM pass over $dO$ and $O$, costing $O(Nd)$ HBM accesses.

#### Backward Algorithm (FlashAttention V1)

The backward pass mirrors the forward tiling structure but with reversed loop order. We iterate over $K,V$ blocks in the outer loop and $Q$ blocks in the inner loop, accumulating gradients into each block.

**Outer Loop** (over $K, V$ blocks, $j = 1, \dots, T_c$):

1. **Load key-value blocks:** Load $\mathbf{K}_j, \mathbf{V}_j \in \mathbb{R}^{B_c \times d}$ from HBM to SRAM.

2. **Initialize gradient accumulators:** Set $d\mathbf{K}_j = 0 \in \mathbb{R}^{B_c \times d}$ and $d\mathbf{V}_j = 0 \in \mathbb{R}^{B_c \times d}$ in SRAM.

   **Inner Loop** (over $Q$ blocks, $i = 1, \dots, T_r$):

   3. **Load query blocks and gradients:** Load the following from HBM into SRAM:
      - $\mathbf{Q}_i \in \mathbb{R}^{B_r \times d}$
      - $d\mathbf{Q}_i \in \mathbb{R}^{B_r \times d}$ (partial gradient, accumulated across $j$ iterations)
      - $d\mathbf{O}_i \in \mathbb{R}^{B_r \times d}$ (gradient from next layer)
      - $\mathbf{O}_i \in \mathbb{R}^{B_r \times d}$ (output from forward pass)
      - Statistics: $\mathbf{L}_i, m_i, D_i$ (all size $B_r$)

   4. **Recompute attention scores and probabilities:**
   
      $$\mathbf{S}_{ij} = \mathbf{Q}_i \mathbf{K}_j^\top \in \mathbb{R}^{B_r \times B_c}$$
      
      Compute local exponents using the saved maximum and log-sum-exp:
      
      $$\mathbf{P}_{ij} = \exp(\mathbf{S}_{ij} - m_i) / \exp(\mathbf{L}_i - m_i)$$
      
      where division and subtraction are row-wise (broadcasting $m_i$ and $L_i$ across columns). If dropout was applied during the forward pass, re-generate the dropout mask using the same random seed and apply it to $\mathbf{P}_{ij}$.

   5. **Compute value gradients:** Accumulate into $d\mathbf{V}_j$:
   
      $$d\mathbf{V}_j \leftarrow d\mathbf{V}_j + \mathbf{P}_{ij}^\top d\mathbf{O}_i$$
      
      This is a $(B_c \times B_r) \times (B_r \times d) \to (B_c \times d)$ matrix multiplication.

   6. **Compute score gradients via softmax chain rule:** The gradient through the softmax operation is:
   
      $$d\mathbf{S}_{ij} = \mathbf{P}_{ij} \odot \left( d\mathbf{O}_i \mathbf{V}_j^\top - \mathbf{P}_{ij}^\top (d\mathbf{O}_i \odot \mathbf{O}_i) \right)$$
      
      Expanding this: first compute the intermediate $(B_r \times B_c)$ matrix:
      
      $$\mathbf{A}_{ij} = d\mathbf{O}_i \mathbf{V}_j^\top$$
      
      Then for each row $i$, compute the row-wise sum weighted by $\mathbf{P}_{ij}$:
      
      $$d\mathbf{S}_{ij} = \mathbf{P}_{ij} \odot \left( \mathbf{A}_{ij} - D_i \right)$$
      
      where $D_i$ is broadcast across the columns of the subtraction.

   7. **Compute query gradients:** Accumulate into $d\mathbf{Q}_i$:
   
      $$d\mathbf{Q}_i \leftarrow d\mathbf{Q}_i + d\mathbf{S}_{ij} \mathbf{K}_j$$
      
      This is $(B_r \times B_c) \times (B_c \times d) \to (B_r \times d)$.

   8. **Compute key gradients:** Accumulate into $d\mathbf{K}_j$:
   
      $$d\mathbf{K}_j \leftarrow d\mathbf{K}_j + d\mathbf{S}_{ij}^\top \mathbf{Q}_i$$
      
      This is $(B_c \times B_r) \times (B_r \times d) \to (B_c \times d)$.

   9. **Write query gradients back:** Write updated $d\mathbf{Q}_i$ to HBM. If the outer loop is parallelized across different GPU threads or blocks, this requires an atomic addition (since multiple $j$ values accumulate into the same $d\mathbf{Q}_i$). In sequential execution, a simple write suffices.

   **End Inner Loop**

3. **Write key-value gradients:** After the inner loop completes (all $Q$ blocks have been processed), write the accumulated gradients to HBM:

   $$\text{Write } d\mathbf{K}_j, d\mathbf{V}_j \text{ to HBM}$$

**End Outer Loop**

#### Backward Pass Complexity Analysis

**FLOP count:**
- Recomputing $\mathbf{S}_{ij} = \mathbf{Q}_i \mathbf{K}_j^\top$: $T_c \times T_r \times 2 B_r B_c d = 2 N^2 d$ FLOPs (same as forward)
- Gradient computations (matrix multiplies for $dV, dQ, dK$): $2 N^2 d$ FLOPs
- **Total backward FLOPs:** $\Theta(N^2 d)$, same as forward pass.

**HBM access (IO complexity):**

- Load $Q, K, V$ for recomputation: $T_c \times Nd = O(N^2 d^2 / M)$ (same scaling as forward)
- Load $dO, O$ (once per sequence): $O(Nd)$
- Load/write statistics $L, m, D$ (once): $O(N)$
- Write $dQ, dK, dV$: $O(Nd)$

**Total backward IO:** $\Theta(N^2 d^2 / M)$ HBM accesses

This is the same asymptotic complexity as the forward pass. Crucially, we **avoid storing the $O(N^2)$ attention matrix**, saving the dominant memory footprint.

#### Comparison: Standard vs. FlashAttention Backward Pass

| Aspect | Standard Backprop | FlashAttention |
|:-------|:-----------------|:---------------|
| **Saved state** | $Q, K, V, P, S$ | $Q, K, V, O, L, m$ |
| **Saved memory** | $O(Nd + 2N^2)$ | $O(Nd)$ |
| **Backward recomputes** | Nothing | $S, P$ (on-the-fly) |
| **Backward IO** | $\Theta(N^2)$ reads of $P$ | $\Theta(N^2 d^2 / M)$ |
| **Backward FLOPs** | $\Theta(N^2 d)$ | $\Theta(N^2 d)$ (same) |

The memory saved ($2N^2$ elements) outweighs the extra compute cost (recomputing $\Theta(N^2 d)$ FLOPs), resulting in a net speedup on memory-bound hardware.




## FlashAttention V2

### Background

Despite the success of v1, analysis revealed that it still achieved only 30-50% of the theoretical maximum FLOPs of the A100 GPU. The two primary causes were suboptimal parallelism leading to low occupancy and inefficient work partioning. 

#### Parallelism Bottleneck
FlashAttention V1 parallelized the computaton over batch size (B) and the number of heads (H). This means the total number of thread blocks launched is $B x H$. In many long context scenarios, to fit the model in memory, the batch size is reduced (e.g B=1). If the number of heads is also small (e.g 12 or 32), the GPU may only launch 12-32 thread blocks. 
To put things into perspective, an NVIDIA A100 has 108 SMs; if only 32 thread blocks are launched over 70% of the GPUs compute resources will sit idlt. THis is known as Low occupancy 

Furthermore, FlashAttention V1's online softmax performs rescaling at every iteration which is a non-matmul operation. On modern GPUs with Tensor Cores, non-matmul operations are 16x more expensive per FLOP than matrix multiplications. Ex - The A100 delivery 312 TFLOPS for FP16 matmul but only 19.5 TFLOPS for other FP32 operations.



### Improvements over FA1
FlashAttention V2 released in July 2023, achieves a ~2x speedup over V1 by addressing architectural inefficiencies invisible in the algorithm's complexity analysis. The paper identified two key limitations of v1: 1. each head or block is processed largely serially by one threadblock, so GPUs with many SMs were underutilized; (2) within a block, warps did unncessary communication.

#### Optimization 1: Reducing Non-Matmul FLOPs

FlashAttention-2 introduces algebraic simplifications to minimize non-matrix-multiply operations, which are significantly slower than matrix multiplications on Tensor Core GPUs. While matrix multiplications (GEMMs) run on Tensor Cores (very fast), operations like exp, sum, max and divison run on the Special Function Units (SFUs) or CUDA cores (slower)

**1. Deferring Normalization:**
In FlashAttention-1, the output matrix $O$ is rescaled at every iteration to maintain numerical stability:
$$
O^{\text{new}} = \text{diag}(\ell^{\text{new}})^{-1} \left( \text{diag}(\ell) e^{m - m^{\text{new}}} O + e^{\tilde{m} - m^{\text{new}}} \tilde{V} \right)
$$
This requires performing vector-matrix divisions at every step. FlashAttention-2 instead maintains an **un-normalized** output accumulator $\tilde{O}$ throughout the loop:
$$
\tilde{O}^{\text{new}} = \text{diag}(e^{m - m^{\text{new}}}) \tilde{O} + e^{\tilde{m} - m^{\text{new}}} \tilde{V}
$$
The expensive division operation is performed only once at the very end of the loop: $O = \text{diag}(\ell^{\text{final}})^{-1} \tilde{O}$. This simple reordering significantly reduces the number of non-matmul FLOPs.

**2. LogSumExp Storage:**
To further reduce memory overhead, FlashAttention-2 changes the statistics stored for the backward pass. Instead of storing both the maximum $m$ and the sum of exponentials $\ell$, it stores a single log-sum-exp value $L$:
$$
L = m + \log(\ell)
$$
During the backward pass, the required statistics can be derived as $\ell = \exp(L - m)$. This halves the memory footprint for metadata, allowing for larger block sizes and better occupancy.

### Work Partitioning Between Warps

A major innovation in v2 is parallelizing across the sequence dimension. Instead of one threadblock handling one $(i,j)$ pair at a time, FlashAttention-2 splits the work of a single attention head into multiple threadblocks and warps. For example, one can process different row-blocks $i$ (or even parts of a block) in parallel. The core idea is warp specialization: some warps (or threadblocks) act as producers (loading next $K/V$ block via TMA, the Tensor Memory Accelerator), while others act as consumers (doing GEMM/softmax on the currently loaded data). By staggering loads and computes, the kernel hides global memory latency. Within each threadblock, they also reorganize warp work to minimize shared-memory usage. Instead of all warps reading/writing the same $O_i,\ell_i$ buffers, they partition that space so that each warp updates a disjoint slice, reducing inter-warp sync and bank conflicts. The net effect is that multiple GPU blocks work on the same attention head concurrently, increasing SM occupancy.




## FlashAttention V3

### Improvements over FA1 & FA2





## Open Problems and Future Directions

Despite these advances, several challenges remain. One is scaling beyond on-chip limits. Current FlashAttention relies on fitting entire blocks in hundreds of KB of SRAM. But LLMs push contexts to hundreds of thousands or even a million tokens, far beyond what fits on one GPU’s chip. Techniques like PagedAttention <d-cite key="kwon2023efficientmemorymanagementlarge"> </d-cite> (streaming attention from host memory in blocks) or Hydragen <d-cite key="juravsky2024hydragenhighthroughputllminference"> </d-cite> (optimizing shared prefixes) are only beginning to address this, but a fully general solution for trillion-token context still awaits. In theory, FlashAttention is IO-optimal for a given SRAM size so beyond-chip hierarchies (CPU memory, disk) must come into play, raising new algorithmic questions about streaming, compression, and multi-node attention.

Another issue is programming and scheduling. The rapid improvements have largely come from hand-tuned CUDA kernels. Future efficiency will require better integration with compilers and ML frameworks. As Tri Dao notes, there is ongoing effort to make these optimizations “easily programmable” since current designs rely on manual warp scheduling and custom intrinsics. Moving to other platforms (AMD GPUs, TPUs, even CPUs) adds complexity: for example, AMD ROCm now supports FlashAttention via Triton, but the performance gap and engineering effort remain substantial. We see similar concerns in learned kernels: will XLA, MLIR, or DSLs be able to generate these tiled and overlapped patterns? Bridging the gap between compile-time scheduling (static tiling) and runtime adaptivity is an open compiler/hardware co-design problem.

Finally, precision limits and numerical issues persist. Pushing to FP4 or mixed-integer kernels could double throughput again, but needs new algorithmic care (e.g. stochastic rounding, specialized normalization). FlashAttention-4’s lesson – that even math functions can be rethought in software – suggests any future hardware bottleneck (e.g. FP4 support) will inspire creative software solution

### Implications for Long Context LLMs

Together, these principles directly serve the long-context frontier. Faster, memory-frugal attention means models can actually use very large windows of text. Today’s FlashAttention-enabled LLMs already handle contexts of 128K–1M tokens by carefully overlapping computation and memory. Tomorrow’s algorithms will push farther: for instance, if attention kernels reach multi-petaflop rates on next-gen GPUs, then 10× longer sequences become feasible in practice. In short, hardware-software co-design is the key enabler for ultra-long-context LLMs. By combining IO-efficient kernels (tiling and recompute), parallel pipelines, and smart approximations or sparsity, the community is paving the way for Transformer attention to scale to truly massive contexts with manageable compute and memory costs

## Appendix A: Proving Standard Attention's Quadratic Complexity

The standard self-attention mechanism in Transformers is formally defined as:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

where inputs $Q, K \in \mathbb{R}^{n \times d_k}$ and $V \in \mathbb{R}^{n \times d_v}$ represent the query, key, and value matrices respectively. The computation proceeds in three sequential stages, each contributing to the overall computational cost.

First, we compute the attention score matrix $S = \frac{QK^T}{\sqrt{d_k}}$. This operation involves a matrix multiplication between $Q$ and $K^T$, resulting in an $n \times n$ matrix. Since each of the $n^2$ entries requires a dot product of size $d_k$, the computational cost is proportional to the number of elements multiplied by the dimension size.

$$
\text{Time Complexity (Scores)} = O(n^2 d_k)
$$

Crucially, this step requires storing the intermediate matrix $S$, which scales quadratically with the sequence length $n$. This $O(n^2)$ memory requirement is the primary bottleneck for long sequences.

Second, we apply the softmax function row-wise to normalize the scores. For each of the $n$ rows, we compute exponentials, sum them, and divide to normalize. As this is performed for every element in the $n \times n$ matrix, the cost is quadratic.

$$
\text{Time Complexity (Softmax)} = O(n^2)
$$

Finally, we compute the weighted sum of values by multiplying the normalized probability matrix $P = \text{softmax}(S)$ with the value matrix $V$. This results in an output matrix of size $n \times d_v$. Similar to the first step, each of the $n \times d_v$ output elements requires a dot product of length $n$.

$$
\text{Time Complexity (Aggregation)} = O(n^2 d_v)
$$

Summing these components gives the total time complexity:

$$
\text{Total Time} = O(n^2 d_k) + O(n^2) + O(n^2 d_v) = O(n^2(d_k + d_v))
$$

In typical Transformer architectures, the head dimensions $d_k$ and $d_v$ are proportional to the model dimension $d_{\text{model}}$. Thus, the complexity is often simplified to $O(n^2 d_{\text{model}})$.

### Scalability Implications

The quadratic dependency on sequence length $n$ creates significant resource challenges as context length increases. The table below illustrates how computational cost and memory usage grow with sequence length, assuming a hidden dimension of $d=768$.

| Sequence Length | Relative Compute Cost | Memory (GB) |
| :-- | :-- | :-- |
| 512 | 1× | 0.001 |
| 2,048 | 16× | 0.016 |
| 8,192 | 256× | 0.25 |
| 65,536 | 16,384× | 16 |
| 1M | 4,000,000× | 4,000 |

### Theoretical Lower Bounds

One might ask if it is possible to compute attention more efficiently than $O(n^2)$. Research grounded in the Strong Exponential Time Hypothesis (SETH) suggests that this quadratic cost is fundamental. Specifically, it has been proven that for any $\epsilon > 0$, computing the softmax dot-product attention requires $\Omega(n^{2-\epsilon})$ time.

This lower bound holds for exact computation as well as for multiplicative and additive approximations. The proof relies on a reduction from the Orthogonal Vectors Problem (OVP). Intuitively, to accurately determine the attention distribution, the algorithm must evaluate pairwise interactions between query and key vectors. In high-dimensional spaces, distinguishing between orthogonal and nearly-orthogonal vectors requires checking all pairs, which establishes the $\Omega(n^2)$ barrier.


## Appendix B : Proving Standard Attention's Memory Bound


The standard Attention Implementation requires $\Theta(Nd + N^2)$ HBM accesses. This can be computed as 

1. Computing $S = QK^T$: Reads $Q$ and $K$, writes $S$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$

2. Computing $P = \text{softmax}(S)$: Reads $S$, writes $P$ to HBM $\rightarrow$ $\Theta(N^2)$.

3. Computing $O = PV$: Reads $P$ and $V$, writes $O$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$.


