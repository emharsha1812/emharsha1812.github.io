---
layout: distill
title: FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers
description: We present a deeply mathematical & technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms.
tags: FlashAttention transformers GPU optimization machine-learning
giscus_comments: true
date: 2025-11-25
featured: true

authors:
  - name: Anonymous

bibliography: 2025-11-25-flash.bib

toc:
  - name: Introduction
  - name: Background
  - name: FlashAttentionV1
  - name: FlashAttentionV2
  - name: FlashAttentionV3
  - name: FlashAttentionV4
  - name: Open Problems and Future Directions
  - name: Appendix A Proving Standard Attention's Quadratic Complexity
  - name: Appendix B Proving Standard Attention is Memory Bound

---


## Introduction

The fundamental concept that underpins the transformer architecture is **Attention**. This was originally developed as an enhancement to RNNs for machine translation <d-cite key="bahdanau2016neuralmachinetranslationjointly"> </d-cite>.
However, in 2017, Vaswani et al. <d-cite key="vaswani2023attentionneed"></d-cite> showed that significantly improved performance could be obtained by eliminating the recurrence structure and instead focusing exclusively on the attention mechanism. 

The importance of this mechanism can be explained with the help of the following example

{% include figure.liquid path="assets/img/blog/Figure_1.jpg" class="img-fluid rounded z-depth-1" caption="Attention weights showing how the model resolves ambiguity in word meaning through context. The arrows indicate strong attention connections between 'bank' and contextually relevant words." %}

Consider the sentence **"I swam across the river to get to the other bank."** The word "**bank**" has multiple meanings—it could refer to a financial institution or a riverbank. The attention mechanism helps the model understand context by weighing relationships between words. In this case, the model attends strongly to words like "swam," "across," and "river," which provide contextual clues that "bank" refers to a riverbank rather than a financial institution.


Therefore, the Attention mechanism has become the single most important mechanism driving the growth of Large Language Models. Over the years, several variants of the attention mechanism have been proposed such as Multi Query Attention (MQA) (cite), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), etc. For instance, here's a non-exhaustive taxonomy of efficient attention mechanisms

{% include figure.liquid path="assets/img/attentiontaxonomy.png" class="img-fluid rounded z-depth-1" caption="Figure adapted from <d-cite key='sun2025efficient'></d-cite>" %}



However, the transformer's attention mechanism has a fatal flaw: it scales quadratically both in time and memory with sequence length, resulting in $O(n^2 \cdot d_{\text{model}})$ time complexity and $O(n^2)$ memory complexity. For example, a 2,048-token sequence requires 16 MB of memory for the attention matrix; at 16,384 tokens, this balloons to 1GB per layer. A rigorous mathematical proof is presented in Appendix A. This quadratic scaling makes it prohibitive for processing long sequences beyond 8K-16K tokens without specialized optimizations <d-cite key="keles2022computationalcomplexityselfattention"></d-cite>. While many works aim for sub-quadratic attention using approximations, including Linformer <d-cite key="wang2020linformerselfattentionlinearcomplexity"></d-cite>, Performer <d-cite key="choromanski2022rethinkingattentionperformers"></d-cite>, and Reformer <d-cite key="kitaev2020reformerefficienttransformer"></d-cite>, these methods have seen limited use in large language models. These are approximate attention methods that reduce cost through low-rank projections, kernel approximations, or sparse routing. These assumptions improve asymptotic complexity, but they introduce accuracy and hardware-efficiency tradeoffs, so large-scale models still rely on exact attention.



The FlashAttention series by Tri Dao <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention2"></d-cite><d-cite key="shah2024flashattention3"></d-cite>  looks at the attention bottleneck from a systems angle. Instead of approximating attention and hurting model quality, the idea is to rethink how attention moves data through the GPU. Modern GPUs have a very uneven memory hierarchy, so the cost of moving data often dominates the cost of doing the math. FlashAttention reduces this movement and gets closer to the limits of the hardware. In this blog, we walk through how this idea has evolved. FlashAttention v1 introduced tiled exact attention. FlashAttention v2 improved how work is split across the GPU. FlashAttention v3 added warp specialization, asynchrony, and low precision on Hopper to push utilization even higher. We also refer to what is known about FlashAttention 4, though an official paper is not public yet.


## Background

### GPU Memory Hierarchy

{% include figure.liquid path="assets/img/blog/Figure_2.png" class="img-fluid rounded z-depth-1" caption="Memory Hierarchy with Bandwidth & Memory Size" %}

Every modern processor faces the same fundamental challenge: fast storage is expense and small, while large storage is slow and cheap. 
Modern GPUs organise memory into a hierarchal form which has five distinct levels, each with different characteristics, different access patterns, and different implications for your code. Starting from the fastest and smallest and working towards the slowest and largest, these levels are: registers, shared memory, L1 cache, L2 cache, and global memory. (Figure 2). At the stop of the memory hierarchy sit registers, the fstest storage available on a GPU. Each thread runninng on the GPU has access to a private set of registers - typically up to 255 registers per thread on modern NVIDIA architectures. These registers feed directly into the computatinal units. When a thread performs an arithmatic operation, the operands come from registers and the result goes back to registers. There is no seperate "register access" operation visible to the programmer; registers are simply where the active data lives. The register file on a single Streaming Multiprocessor contains 65,536 registers with each register holding 32 bits. This gives 256 kilobytes of register storage per SM, and these registers are dynamically shared among all threads running on that SM

Unlike registers, which are private to each individual threads, shared memory (SRAM) is shared among all threads in a thread block. It is the primary mechanism for threads to cooperate and community and is **explicitly managed by the programmer rather than automatically managed by the compiler**. Shared memory provides a staging area for data that multiple threads need to access. Rather than having each thread read the same value from slow global memory, one thread can read it once into shared memory, synchronize with the other threads, and then all threads can read from fast shared memory. Secondly, it enables algorithms that require threads to exchange data. This is used by FlashAttention explicitly 


At the bottom of the hierarchy sits global memory - the large DRAM pool that provides the bulk of a GPU's storage capacity. An H100 for example, has 80GBs of HBM memory, operating at around 3,000 gigabytes per second of bandwidth. THis is where your input data starts, where your output data goes, and where any persistent state lives. It is also by far the slowest level of heirarchy, with individual access latencies reaching 400 to 800 clock cycles depending on architecture and access pattern.

<!-- Insert table here -->

When we look at the GPU memory hierarchy in detail, we see a sharp difference in both latency and bandwidth across levels. Registers and shared memory sit close to the compute units and respond within a few cycles. HBM sits hundreds of cycles away with higher bandwidth but much higher latency. This separation means that the location of data often dictates runtime. As an example, the A100 GPU has 40GB of high bandwidth memory (HBM2e) with bandwidth 1.6TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s <d-cite key="nvidia2020a100"></d-cite>. This shows that the on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size.

### Attention is memory bound despite $O(N^{2})$ compute

The efficiency of a kernel is governed by its arithmatic intensity, defined as the number of floating-point operations (FLOPs) performed per byte of memory access. Arithmatic Intensity is commonly used to measure whether operations can be classified as either compute-bound or memory-bound. A process is memory-bound when the execution speed is limited by how fast data can be moved between memory (HBM/RAM) and the processor cores, rather than by how fast the cores can compute. Typical examples include elementwise operations such as activation, dropout and reduction operations such as sum, softmax, batch norm, layer norm.
A process is compute-bound when the execution speed is limited by the raw processing power (FLOPS) of the cores such as Matrix Multiplication, Convolution 

$$
\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}}
$$

Standard Attention involves three primary stages
1. Matrix Multiplication: $ S = Q K^T $
2. Softmax: $ P = \text{softmax}(S) $
3. Matrix Multiplication: $ O = P V $

The matrix multiplications ($QK^T$ and $PV$) are compute-bound operations with high arithmetic intensity ($O(N^2 d)$ FLOPs vs $O(N^2)$ IO). However, the intermediate Softmax operation is memory-bound. It requires reading the entire $N \times N$ matrix $S$ from HBM, performing reduction operations, and writing the resulting $P$ matrix back to HBM. This $O(N^2)$ memory traffic saturates the HBM bandwidth, leaving the powerful Tensor Cores idle. (Detailed Proof provided in Appendix B)


## FlashAttention - V1 

One of the hardware-efficient mechanisms now widely adopted across different providers is Fast and Memory-Efficient Exact Attention with IO-Awareness, or FlashAttention. The "IO-Awareness" part of the title describes its core technical principle: optimizing data movement between GPU memory hierarchies.

FlashAttention addresses the dual challenges of speed and memory consumption in transformers, especially on long sequences, by rethinking attention algorithms through the lens of GPU memory hierarchy awareness. The key insight is minimizing data movement between high-bandwidth memory (HBM) and on-chip SRAM.

{% include figure.liquid path="assets/img/blog/Figure_3.png" class="img-fluid rounded z-depth-1" caption="Standard execution loads data from HBM for every step. Kernel fusion and tiling keep data in fast memory longer, reducing memory traffic and improving throughput." %}

FlashAttention v1, published at Neural Information Processing Systems 2022 by Tri Dao and collaborators, introduced two key innovations: **tiled** attention that processes blocks of queries, keys, and values entirely in SRAM, and an **online softmax algorithm** that computes exact softmax incrementally without materializing the full attention matrix.

### Online Softmax Algorithm Enables Incremental Computation

Standard softmax computation requires three sequential passes over the data, making it inherently memory-intensive. The first pass finds the maximum value for numerical stability. The second pass computes exponentials and accumulates the normalization sum. The third pass normalizes each element. This three-pass structure can be expressed mathematically as follows:

Given a vector $x \in \mathbb{R}^n$, the numerically stable softmax is computed in three passes:

$$
\text{Pass 1:} \quad 
m = \max_{i} x_i
$$

$$
\text{Pass 2:} \quad 
d = \sum_{i=1}^{n} e^{\,x_i - m}
$$

$$
\text{Pass 3:} \quad
\text{softmax}(x)_i = \frac{e^{\,x_i - m}}{d}
$$

This approach ensures numerical stability by preventing overflow in the exponential computation. However, this three-pass dependency creates a critical bottleneck: we must materialize the entire attention matrix in HBM before proceeding. The softmax operation requires global information—specifically, the denominator in Pass 2 must sum over all $N$ elements. This seemingly requires loading the full row into memory before computing any output, making the process extremely memory I/O intensive and defeating attempts to tile the computation efficiently.

The breakthrough came from **online softmax**, an algorithmic technique that computes softmax incrementally in blocks while maintaining running statistics for the maximum and the sum of exponentials. This method was originally discovered by Milakov and Gimelshein <d-cite key="milakov2018online"></d-cite> and later applied to sparse attention patterns by Child et al. <d-cite key="child2019generatinglongsequencessparse"></d-cite>. FlashAttention's key innovation was adapting online softmax to work within a tiled attention algorithm, enabling exact softmax computation without materializing the full attention matrix. Instead of waiting for the global maximum $m_N$ across all elements before computing the normalization sum, the algorithm maintains running statistics that are updated as each new block of data is processed. This incremental approach transforms softmax into a streaming computation that can be fused with the surrounding matrix operations.

### Algebra of Online Softmax

The elegance of online softmax lies in its ability to combine partial results without revisiting the raw data. In a streaming (block-wise) setting, we process the sequence in chunks. Let's assume we have processed the first block of keys and have a local maximum $m_{old}$ and a local unnormalized sum $\ell_{old}$ (where $\ell = \sum e^{x_j - m}$). Now, we load a new block of keys and compute their raw scores. From this new block, we find a local maximum $m_{block}$ and a local sum $\ell_{block}$.

For each block, we compute local statistics independently. The block size is chosen such that each block fits comfortably in SRAM, allowing us to process it entirely on-chip.

**Running maximum** (after processing new block):

$$
m_{new} = \max(m_{old}, m_{block})
$$

with initial condition: $m_{old} = -\infty$ for the first block

**Running sum** (relative to current maximum):

$$
\ell_{new} = \sum_{j} e^{x_j - m_{new}}
$$

with initial condition: $\ell_{old} = 0$ for the first block

#### Recurrence Relation for the Sum

The critical mathematical insight is the **rescaling formula** for $\ell_{new}$. To combine the old and new blocks into a valid global state without accessing the old data, we need to express the new sum in terms of the old statistics.

Starting from the definition:

$$
\ell_{new} = \sum_{j \in \text{old}} e^{x_j - m_{new}} + \sum_{j \in \text{block}} e^{x_j - m_{new}}
$$

**Key transformation:** Express terms using the old maximum $m_{old}$:

$$
e^{x_j - m_{new}} = e^{x_j - m_{old}} \cdot e^{m_{old} - m_{new}}
$$

Therefore, for the old block terms:

$$
\sum_{j \in \text{old}} e^{x_j - m_{new}} = e^{m_{old} - m_{new}} \sum_{j \in \text{old}} e^{x_j - m_{old}} = e^{m_{old} - m_{new}} \cdot \ell_{old}
$$

Similarly, for the new block terms:

$$
\sum_{j \in \text{block}} e^{x_j - m_{new}} = e^{m_{block} - m_{new}} \sum_{j \in \text{block}} e^{x_j - m_{block}} = e^{m_{block} - m_{new}} \cdot \ell_{block}
$$

Combining both:

$$
\boxed{\ell_{new} = e^{m_{old} - m_{new}} \cdot \ell_{old} + e^{m_{block} - m_{new}} \cdot \ell_{block}}
$$

We can define the correction factor $\alpha = e^{m_{old} - m_{new}}$. This is the **fundamental rescaling equation** for online softmax. It allows us to update the running sum using only the previous statistics ($m_{old}$ and $\ell_{old}$) and the new block statistics, without ever loading the earlier blocks from HBM again. The exponential correction terms ensure numerical stability throughout the incremental computation by properly rescaling all exponentials relative to the current global maximum. This reduces the operation from 3 passes to 2 passes <d-cite key="dukhan2020two"> </d-cite>

### FlashAttention : Forward Pass

{% include figure.liquid path="assets/img/blog/Figure_4.png" class="img-fluid rounded z-depth-1" caption="Algorithm for FlashAttention Forward Pass. The algorithm partitions inputs into blocks that fit in SRAM, computes attention incrementally using online softmax, and updates running statistics to avoid materializing the full attention matrix in HBM." %}

For each attention head, FlashAttention reduces memory reads and writes by tiling. It loads small blocks of queries, keys, and values from GPU HBM into fast on-chip SRAM, computes attention for that block, and updates the output before moving on to the next block. This limits how often data moves between slow and fast memory, which is the main bottleneck on modern GPUs. Cutting this movement often gives a 2–4× speedup in practice.

#### Mathematical Derivation

{% include figure.liquid path="assets/img/blog/Figure_5.png" class="img-fluid rounded z-depth-1"%}

We define the block sizes based on the available SRAM size $M$. Let $B_c$ be the block size for columns (dimension along $N$ for $K, V$), and let $B_r$ be the block size for rows (dimension along $N$ for $Q, O$). The key constraint is $4 B_c d \le M$ to ensure that $K, V$ blocks and various buffers fit in SRAM. Usually, we set $B_c \approx \lceil \frac{M}{4d} \rceil$ and $B_r \approx \min(\lceil \frac{M}{4d} \rceil, d)$.

The matrices are divided into blocks as follows:

- $Q$ is divided into $T_r = \lceil N/B_r \rceil$ blocks: $Q_1, \dots, Q_{T_r}$
- $K$ and $V$ are divided into $T_c = \lceil N/B_c \rceil$ blocks: $K_1, \dots, K_{T_c}$ and $V_1, \dots, V_{T_c}$
- $O$ is divided into $T_r$ blocks: $O_1, \dots, O_{T_r}$

Ideally, to minimize HBM writes of the output $O$, we want to load a block of $Q$, iterate over all blocks of $K, V$, accumulating the result, and then write $O$ once. However, FlashAttention V1 actually uses an outer loop over $K, V$ and an inner loop over $Q$ to better utilize the SRAM for the larger $K, V$ blocks, though conceptually the accumulation happens per query row.

**Initialization:**

{% include figure.liquid path="assets/img/blog/Figure_6.png" class="img-fluid rounded z-depth-1" caption="Q, K, and V blocks are stored in HBM (blue). Each K and V block is streamed into SRAM, where partial score matrices S(t) and exponentials A(t) are computed (orange). Softmax normalization is accumulated across blocks, and partial outputs O(t) are merged to form the final output." %}

We initialize the output matrix $O = 0 \in \mathbb{R}^{N \times d}$, the running sum vector $\ell = 0 \in \mathbb{R}^N$, and the running maximum vector $m = -\infty \in \mathbb{R}^N$. These statistics will be updated incrementally as we process each block.

**Outer Loop** (iterating over $K, V$ blocks, $j = 1, \dots, T_c$):

1. **Load Key-Value blocks:** Load $K_j, V_j \in \mathbb{R}^{B_c \times d}$ from HBM to on-chip SRAM. These blocks remain in SRAM throughout the inner loop.

   **Inner Loop** (iterating over $Q$ blocks, $i = 1, \dots, T_r$):

   1. **Load Query block and statistics:** Load $Q_i \in \mathbb{R}^{B_r \times d}$, $O_i \in \mathbb{R}^{B_r \times d}$, $\ell_i \in \mathbb{R}^{B_r}$, $m_i \in \mathbb{R}^{B_r}$ from HBM to SRAM.

   2. **Compute attention scores:** Compute $S_{ij} = Q_i K_j^\top \in \mathbb{R}^{B_r \times B_c}$. This matrix multiplication is performed entirely in SRAM, computing the raw attention scores between the current query block and key block.

   3. **Compute local statistics:** For the current block, we compute:
      
      $$\tilde{m}_{ij} = \text{rowmax}(S_{ij}) \in \mathbb{R}^{B_r}$$
      
      (maximum score in each row)
      
      $$\tilde{P}_{ij} = \exp(S_{ij} - \tilde{m}_{ij}) \in \mathbb{R}^{B_r \times B_c}$$
      
      (pointwise exponential with local normalization)
      
      $$\tilde{\ell}_{ij} = \text{rowsum}(\tilde{P}_{ij}) \in \mathbb{R}^{B_r}$$
      
      (sum of exponentials in each row)

   4. **Update running statistics:** Using the online softmax rescaling formula:
      
      $$m_i^{\text{new}} = \max(m_i, \tilde{m}_{ij})$$
      
      (update global maximum)
      
      $$\ell_i^{\text{new}} = e^{m_i - m_i^{\text{new}}} \ell_i + e^{\tilde{m}_{ij} - m_i^{\text{new}}} \tilde{\ell}_{ij}$$
      
      (rescale and accumulate sum)

   5. **Update output:** We incrementally update the attention output. First, compute partial output contribution:
      
      $$\tilde{V}_{ij} = \tilde{P}_{ij} V_j$$
      
      (weighted sum of values for current block)
      
      Then combine with running output using the rescaling formula:
      
      $$O_i^{\text{new}} = \text{diag}(\ell_i^{\text{new}})^{-1} \left( \text{diag}(\ell_i) e^{m_i - m_i^{\text{new}}} O_i + e^{\tilde{m}_{ij} - m_i^{\text{new}}} \tilde{V}_{ij} \right)$$
      
      This rescales the old output and adds the new contribution, then normalizes by the updated sum.

   6. **Write back to HBM:** Write $O_i^{\text{new}}, \ell_i^{\text{new}}, m_i^{\text{new}}$ back to HBM, updating the global state for this query block.

{% include figure.liquid path="assets/img/blog/Figure_7.png" class="img-fluid rounded z-depth-1" caption="Blockwise computation of attention using online softmax. Q remains in HBM while K and V are streamed in blocks. For each block, partial scores S(t) and exponentials A(t) are computed in SRAM. The running softmax denominator is updated across blocks, and partial outputs O(t) are rescaled and accumulated to form the final output." %}

#### Complexity Comparison with Standard Attention

| Metric | Standard Attention | FlashAttention |
|:-------|:-------------------|:---------------|
| **Time Complexity (FLOPs)** | $O(N^2 d)$ | $O(N^2 d)$ |
| **Space Complexity (Memory)** | $O(N^2)$ (stores $S, P$) | $O(N)$ (stores $O, \ell, m$) |
| **IO Complexity (HBM Access)** | $O(N^2)$ | $O(N^2 d^2 M^{-1})$ |


### FlashAttention: Backward Pass

Training deep models requires a backward pass to compute gradients. Standard backpropagation requires the stored attention probability matrix $P$ (size $N \times N$) to compute gradients with respect to $Q$ and $K$. Storing $P$ for long sequences is prohibitively expensive ($O(N^2)$ memory).

FlashAttention solves this through **recomputation**. Instead of saving $P$, it saves only the final output $O$ and the normalization statistics $(\ell, m)$ from the forward pass—both of size $N \times 1$, plus the random seed for dropout. During the backward pass, the kernel reloads $Q$, $K$, $V$ from HBM and uses $m$ and $\ell$ to regenerate the attention scores $S$ and probabilities $P$ block-by-block in SRAM, exactly as they were computed in the forward pass. 

While this recomputation increases FLOPs by repeating the forward matrix multiplications, the reduction in HBM reads—avoiding $O(N^2)$ reads of $P$—results in a net speedup because the operation is memory-bound. The additional compute cost is more than offset by the savings in memory bandwidth.


## FlashAttention V2


### Improvements over FA1


## FlashAttention V3

### Improvements over FA1 & FA2





<!-- - **Sliding Window Attention:** Reduces to $O(nw)$ where $w$ is window size, but achieves only $\Omega(nw^{1-\epsilon})$ lower bound
- **Sparse Attention:** Similar rectangular lower bounds apply
- **Polynomial Approximations:** Can achieve $O(nd^p_k d_v)$ which is linear in $n$ but **exponential in polynomial order $p$**[^1] -->


<!-- The quadratic complexity of standard self-attention arises from three fundamental operations that each contribute $O(n^2)$ cost:

1. **Score computation:** $O(n^2 d_k)$ - computing all pairwise similarities
2. **Softmax normalization:** $O(n^2)$ - normalizing each of $n^2$ scores
3. **Value aggregation:** $O(n^2 d_v)$ - aggregating all $n^2$ weighted values -->





## Open Problems and Future Directions

Despite these advances, several challenges remain. One is scaling beyond on-chip limits. Current FlashAttention relies on fitting entire blocks in hundreds of KB of SRAM. But LLMs push contexts to hundreds of thousands or even a million tokens, far beyond what fits on one GPU’s chip. Techniques like PagedAttention <d-cite key="kwon2023efficientmemorymanagementlarge"> </d-cite> (streaming attention from host memory in blocks) or Hydragen <d-cite key="juravsky2024hydragenhighthroughputllminference"> </d-cite> (optimizing shared prefixes) are only beginning to address this, but a fully general solution for trillion-token context still awaits. In theory, FlashAttention is IO-optimal for a given SRAM size so beyond-chip hierarchies (CPU memory, disk) must come into play, raising new algorithmic questions about streaming, compression, and multi-node attention.

Another issue is programming and scheduling. The rapid improvements have largely come from hand-tuned CUDA kernels. Future efficiency will require better integration with compilers and ML frameworks. As Tri Dao notes, there is ongoing effort to make these optimizations “easily programmable” since current designs rely on manual warp scheduling and custom intrinsics. Moving to other platforms (AMD GPUs, TPUs, even CPUs) adds complexity: for example, AMD ROCm now supports FlashAttention via Triton, but the performance gap and engineering effort remain substantial. We see similar concerns in learned kernels: will XLA, MLIR, or DSLs be able to generate these tiled and overlapped patterns? Bridging the gap between compile-time scheduling (static tiling) and runtime adaptivity is an open compiler/hardware co-design problem.

Finally, precision limits and numerical issues persist. Pushing to FP4 or mixed-integer kernels could double throughput again, but needs new algorithmic care (e.g. stochastic rounding, specialized normalization). FlashAttention-4’s lesson – that even math functions can be rethought in software – suggests any future hardware bottleneck (e.g. FP4 support) will inspire creative software solution

### Implications for Long Context LLMs

Together, these principles directly serve the long-context frontier. Faster, memory-frugal attention means models can actually use very large windows of text. Today’s FlashAttention-enabled LLMs already handle contexts of 128K–1M tokens by carefully overlapping computation and memory. Tomorrow’s algorithms will push farther: for instance, if attention kernels reach multi-petaflop rates on next-gen GPUs, then 10× longer sequences become feasible in practice. In short, hardware-software co-design is the key enabler for ultra-long-context LLMs. By combining IO-efficient kernels (tiling and recompute), parallel pipelines, and smart approximations or sparsity, the community is paving the way for Transformer attention to scale to truly massive contexts with manageable compute and memory costs

## Appendix A 
The standard self-attention mechanism used in Transformers is formulated as
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Where:
- $Q \in \mathbb{R}^{n \times d_k}$ is the query matrix
- $K \in \mathbb{R}^{n \times d_k}$ is the key matrix
- $V \in \mathbb{R}^{n \times d_v}$ is the value matrix
- $n$ is the sequence length
- $d_k$ and $d_v$ are the key and value dimensions
- $\sqrt{d_k}$ is the scaling factor for numerical stability

Computing the score matrix involves:

$$
S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}
$$

This matrix multiplication requires computing the dot product between all pairs of query and key vectors. For each of the $n^2$ pairs $(i,j)$, we compute:

$$
S_{ij} = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}
$$

Since each dot product involves $d_k$ multiplications and $d_k - 1$ additions, the total computational cost is:[^1]

$$
\text{Time Complexity (Step 1)} = O(n^2 d_k)
$$

**Memory Complexity:** The $n \times n$ score matrix $S$ requires $O(n^2)$ space to store.

The softmax function is applied row-wise to normalize the attention scores
$$
\text{softmax}(S)_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^n \exp(S_{ik})}
$$

For each row $i \in [n]$, this involves:

- Computing $n$ exponentials: $O(n)$ operations
- Computing the row sum: $O(n)$ operations
- Normalizing each element: $O(n)$ operations

Since we have $n$ rows, the total cost is:

$$
\text{Time Complexity (Step 2)} = O(n^2)
$$


Finally, multiply the normalized attention matrix with the value matrix:

$$
\text{Output} = \text{softmax}(S) \cdot V
$$

Where $\text{softmax}(S) \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{n \times d_v}$.

This matrix multiplication requires computing:
$$
\text{Output}_{ij} = \sum_{k=1}^n \text{softmax}(S)_{ik} \cdot V_{kj}
$$

For each of the $n \times d_v$ elements in the output, we perform $n$ operations, yielding:

$$
\text{Time Complexity (Step 3)} = O(n^2 d_v)
$$

Combining all three steps:

$$
\text{Total Time} = O(n^2 d_k) + O(n^2) + O(n^2 d_v) = O(n^2(d_k + d_v))
$$

In standard Transformer configurations, the embedding dimension $d_{\text{model}}$ is typically divided equally among multiple attention heads. For a single attention head, $d_k \approx d_v \approx \frac{d_{\text{model}}}{h}$ where $h$ is the number of heads. Therefore, the complexity simplifies to:[^2]

$$
\boxed{\text{Time Complexity} = O(n^2 \cdot d_{\text{model}})}
$$

**Memory Complexity:**

The dominant memory requirement comes from storing the intermediate attention score matrix

$$
S \in \mathbb{R}^{n \times n} \quad \Rightarrow \quad \boxed{\text{Memory Complexity} = O(n^2)}
$$

Additional memory requirements include:

- Storing $Q, K, V$ matrices: $O(3 \cdot n \cdot d)$ which is $O(nd)$ overall
- Temporary buffers for computations: $O(nd)$

However, these are dominated by $O(n^2)$ for large sequence lengths.

The quadratic complexity creates severe bottlenecks for processing long sequences:

| Sequence Length | Relative Cost | Memory (GB at $d=768$) |
| :-- | :-- | :-- |
| 512 | 1× | 1.5 |
| 2,048 | 16× | 24 |
| 8,192 | 256× | 384 |
| 65,536 | 16,384× | $>6$ TB |

### Theoretical Lower Bounds

Research using complexity theory has proven that this quadratic complexity is **fundamental and unavoidable** under the Strong Exponential Time Hypothesis (SETH). The key theorem states:[^3][^1]

For the softmax dot-product self-attention with $d_q = \omega(\log n)$, for any $\epsilon > 0$:

$$
\text{Computing self-attention requires } \Omega(n^{2-\epsilon}) \text{ time}
$$

This holds even for:

- **Exact computation** of attention scores
- **Approximate computation** with multiplicative error $\mu$: $|\hat{Y}_{ij} - Y_{ij}| \leq \mu|Y_{ij}|$
- **Approximate computation** with additive error $\mu$: $|\hat{Y}_{ij} - Y_{ij}| \leq \mu$


### Why Self-Attention Cannot Escape Quadratic Complexity

The proof uses reductions from the **Orthogonal Vectors Problem (OVP)**, which is conjectured to require nearly quadratic time. The intuition is:

1. Computing attention requires evaluating pairwise interactions: $O(n^2)$ pairs
2. Each pair requires a dot product computation: $O(d_k)$ time
3. Even with approximations, one must examine enough pairs to distinguish correct answers from incorrect ones
4. Thus, the quadratic barrier in $n$ is unavoidable[^3]


## Appendix B : Proving Standard Attention's Memory Bound


The standard Attention Implementation requires $\Theta(Nd + N^2)$ HBM accesses. This can be computed as 

1. Computing $S = QK^T$: Reads $Q$ and $K$, writes $S$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$

2. Computing $P = \text{softmax}(S)$: Reads $S$, writes $P$ to HBM $\rightarrow$ $\Theta(N^2)$.

3. Computing $O = PV$: Reads $P$ and $V$, writes $O$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$.


