---
layout: distill
title: FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers
description: We present a technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with diagrams and Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms. 
giscus_comments: true
date: 2025-11-25
featured: true
draft: false
authors:
  - name: Anonymous

bibliography: 2025-11-25-flash.bib

toc:
  - name: Introduction
---


## Introduction

The fundamental concept that underpins the transformer architecture is **Attention**. This was originally developed as an enhancement to RNNs for machine translation <d-cite key="bahdanau2016neuralmachinetranslationjointly"> </d-cite>.
However, in 2017, Vaswani et al. <d-cite key="vaswani2023attentionneed"></d-cite> showed that significantly improved performance could be obtained by eliminating the recurrence structure and instead focusing exclusively on the attention mechanism. 

The importance of this mechanism can be explained with the help of the following example

{% include figure.liquid path="assets/img/blog/Figure_1.jpg" class="img-fluid rounded z-depth-1" caption="Attention weights showing how the model resolves ambiguity in word meaning through context. The arrows indicate strong attention connections between 'bank' and contextually relevant words." %}

Consider the sentence "I swam across the river to get to the other bank." The word "bank" has multiple meanings—it could refer to a financial institution or a riverbank. The attention mechanism helps the model understand context by weighing relationships between words. In this case, the model attends strongly to words like "swam," "across," and "river," which provide contextual clues that "bank" refers to a riverbank rather than a financial institution.


Therefore, the Attention mechanism has become the single most important mechanism driving the growth of Large Language Models. Over the years, several variants of the attention mechanism have been proposed such as Multi Query Attention (MQA) (cite), Grouped-Query Attention (GQA), Multi-Head Latent Attention (MLA), etc. For instance, here's a non-exhaustive taxonomy of efficient attention mechanisms

{% include figure.liquid path="assets/img/attentiontaxonomy.png" class="img-fluid rounded z-depth-1" caption="Figure adapted from <d-cite key='sun2025efficient'></d-cite>" %}



However the transformer's attention mechanism has a fatal flaw: it scales quadratically both in time and memory with sequence length. For example - For a 2,048-token sequence, the attention matrix consumes 16 MB of memory; at 16,384 tokens, it balloons to 1GB per layer. A Rigorous mathematical proof is presented in Appendix A (link here). There have been attempts at sub-quadratic complexity using approximate attention methods such as Linformer <d-cite key="wang2020linformerselfattentionlinearcomplexity"></d-cite>, Performer <d-cite key="choromanski2022rethinkingattentionperformers"></d-cite>, Reformer <d-cite key=efficienttransformer"> </d-cite>, etc., but these haven't been adopted in large language models due to their fundamental tradeoffs

results in $O(n^2 \cdot d_{\text{model}})$ time complexity and $O(n^2)$ memory complexity, making it prohibitive for processing long sequences beyond 8K-16K tokens without specialized optimizations <d-cite key="keles2022computationalcomplexityselfattention"> </d-cite>. 

## Background

One of the hardware-efficient mechanisms that is now widely adopted in different providers is Fast and Memory-Efficient Exact Attention with IO-Awareness, or FlashAttention. The "IO-Awareness" part of the title describes its core technical principle, which involves optimizing data movement.  

To give a perspective on why it was needed, the standard attention mechanism, which is mathematically defined as 
For Q (Query), K (Key), V (Value) matrices  {belongs to} R^(NxDk) where N is ___ and D is the dimensions of the embedding of each token, has two major flaws.

1. It is very IO dependent (large number of memory accesses in standard attention).

2. The time and memory complexity of self-attention are quadratic (O(N^2)) in sequence length. (Proof given in Appendix A - Why Self Attention is quadratic)

These two major issues make transformers slow and memory-hungry, especially on long sequences. 

FlashAttention addresses this by rethinking attention algorithms through the lens of GPU memory heirarchy awarenesss which is minimizing the data movement between high-bandwidth memory (HBM) and on-chip SRM. 


Problem 1 - The HBM/Latency Problem 

{% include figure.liquid path="assets/img/Pyramid.png" class="img-fluid rounded z-depth-1" %}


When we look at the GPU memory hierarchy in detail, we see a sharp difference in both latency and bandwidth across levels. Registers and shared memory sit close to the compute units and respond within a few cycles. HBM sits hundreds of cycles away with higher bandwidth but much higher latency. This separation means that the location of data often dictates runtime. As an example, the H100 GPU has 80GB of high bandwidth memory (HBM3) with bandwidth 3.35TB/s and 256KB of on-chip SRAM per each of 132 streaming multiprocessors with bandwidth estimated around 33TB/s <d-cite key="nvidia2022h100"></d-cite>. This shows that the on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size.

The standard Attention Implementation requires $\Theta(Nd + N^2)$ HBM accesses. This can be computed as 

1. Computing $S = QK^T$: Reads $Q$ and $K$, writes $S$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$

2. Computing $P = \text{softmax}(S)$: Reads $S$, writes $P$ to HBM $\rightarrow$ $\Theta(N^2)$.

3. Computing $O = PV$: Reads $P$ and $V$, writes $O$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$.



## FlashAttention - V1 

FlashAttention v1, published at Neural Information Processing Systems 2022, by Tri Dao and collaborators, introducted two key innovations: tiled attention that processes blocks of queries, keys, and values entirely in SRAM, and an online softmax algorithm that computes exact softmax incrementally without materializing the full attention matrix.

### Online Softmax algorithm enables incremental computation

Standard softmax requires three passes over the data: one pass to find the maximum value for numerical stability. A second pass to compute exponentials and accumulate the normalization sum. A third pass to normalize each element. This can be shown mathematically as follows: 

Given a vector $(x \in \mathbb{R}^n\)$, the stable softmax is computed in three passes:

$$
\text{Pass 1:} \quad 
m = \max_{i} x_i
$$

$$
\text{Pass 2:} \quad 
Z = \sum_{i=1}^{n} e^{\,x_i - m}
$$

$$
\text{Pass 3:} \quad
\text{softmax}(x)_i = \frac{e^{\,x_i - m}}{Z}
$$

This approach gives us the numerically stable softmax. However 
the dependency forces us to materialize the entire attention matrix in the HBM. Moreover, softmax requires global information (Pass 2). Since the denominator sums over all $N$ elements,seemingly requiring the full row before computing any output. 

FlashAttention leverages the idea of online softmax; the idea of computing softmax in blocks, while maintaining a running maximum and a running sum of exponentials. It was discovered by Milakov et al <d-cite key="milakov2018online">  and then used by Child et al <d-cite key="child2019generatinglongsequencessparse"> </d-cite> 

FlashAttention introduced a new way to use the online softmax inside a tiled attention algorithm. This technique allows the softmax statistics to be computed incrementally

### Algebra of Online Softmax

Let vector $x$ be split into two blocks $x^{(1)}$ and $x^{(2)}$. We compute local statistics for each block. The block size is chosen such that 

Local max: $m_1 = \max(x^{(1)})$, $m_2 = \max(x^{(2)})$

Thus the local unnormalised sum becomes: $l_1 = \sum e^{x^{(1)} - m_1}$, $l_2 = \sum e^{x^{(2)} - m_2}$

To combine these, we define the global max $m_{new} = \max(m_1, m_2)$. 
The global sum $l_{new}$ can be updated without revisiting the raw data of block 1 which is $$l_{new} = e^{m_1 - m_{new}} l_1 + e^{m_2 - m_{new}} l_2$$

Let $O_{old}$ be the current accumulated output scaled by the old normalization factor. The correct update is:
$$ O_{new} = \text{diag}(l_{new})^{-1} \left( \text{diag}(l_{old}) e^{m_{old} - m_{new}} O_{old} + e^{m_{cur} - m_{new}} P_{cur} V_{cur} \right) $$

<!-- - **Sliding Window Attention:** Reduces to $O(nw)$ where $w$ is window size, but achieves only $\Omega(nw^{1-\epsilon})$ lower bound
- **Sparse Attention:** Similar rectangular lower bounds apply
- **Polynomial Approximations:** Can achieve $O(nd^p_k d_v)$ which is linear in $n$ but **exponential in polynomial order $p$**[^1] -->


The quadratic complexity of standard self-attention arises from three fundamental operations that each contribute $O(n^2)$ cost:

1. **Score computation:** $O(n^2 d_k)$ - computing all pairwise similarities
2. **Softmax normalization:** $O(n^2)$ - normalizing each of $n^2$ scores
3. **Value aggregation:** $O(n^2 d_v)$ - aggregating all $n^2$ weighted values

This 

## Looking through Hardware-aware lens

Standard implementations of attention, exacerbate this algorithmic complexity through inefficient utilization of hardware resources. These implementations are typically memory-bound, meaning their execution speed is limited not by the arithmetic throughput of the GPU's compute units, but by the bandwidth available to move data between the high-capacity High Bandwidth Memory (HBM) and the high-speed on-chip SRAM. This phenomenon, often referred to as the "memory wall," dictates that as compute capabilities (FLOPs) outpace memory bandwidth improvements, operations with low arithmetic intensity become increasingly expensive <d-cite key="gholami2024ai"> </d-cite>

The FlashAttention series (v1, v2, and v3) by Tri Dao represents a rigorous systems-level intervention to address this bottleneck. Rather than approximating the attention mechanism—a strategy that often degrades model quality—FlashAttention redefines the computation through the lens of IO-awareness. By meticulously accounting for the asymmetric memory hierarchy of modern GPUs, FlashAttention minimizes data movement, effectively breaking the memory wall for exact attention computation. Through this blog, we provide an exhaustive technical examination of this evolution, tracing the algorithmic tiling of version 1, the parallelism optimizations of version 2, and the hardware-specific asynchronous pipelining of version 3. We also touch upon FlashAttention-4, as gathered from sources, though no official paper has been released yet.




## Appendix A 
The standard self-attention mechanism used in Transformers is formulated as
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Where:
- $Q \in \mathbb{R}^{n \times d_k}$ is the query matrix
- $K \in \mathbb{R}^{n \times d_k}$ is the key matrix
- $V \in \mathbb{R}^{n \times d_v}$ is the value matrix
- $n$ is the sequence length
- $d_k$ and $d_v$ are the key and value dimensions
- $\sqrt{d_k}$ is the scaling factor for numerical stability

Computing the score matrix involves:

$$
S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}
$$

This matrix multiplication requires computing the dot product between all pairs of query and key vectors. For each of the $n^2$ pairs $(i,j)$, we compute:

$$
S_{ij} = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}
$$

Since each dot product involves $d_k$ multiplications and $d_k - 1$ additions, the total computational cost is:[^1]

$$
\text{Time Complexity (Step 1)} = O(n^2 d_k)
$$

**Memory Complexity:** The $n \times n$ score matrix $S$ requires $O(n^2)$ space to store.

The softmax function is applied row-wise to normalize the attention scores
$$
\text{softmax}(S)_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^n \exp(S_{ik})}
$$

For each row $i \in [n]$, this involves:

- Computing $n$ exponentials: $O(n)$ operations
- Computing the row sum: $O(n)$ operations
- Normalizing each element: $O(n)$ operations

Since we have $n$ rows, the total cost is:

$$
\text{Time Complexity (Step 2)} = O(n^2)
$$


Finally, multiply the normalized attention matrix with the value matrix:

$$
\text{Output} = \text{softmax}(S) \cdot V
$$

Where $\text{softmax}(S) \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{n \times d_v}$.

This matrix multiplication requires computing:
$$
\text{Output}_{ij} = \sum_{k=1}^n \text{softmax}(S)_{ik} \cdot V_{kj}
$$

For each of the $n \times d_v$ elements in the output, we perform $n$ operations, yielding:

$$
\text{Time Complexity (Step 3)} = O(n^2 d_v)
$$

Combining all three steps:

$$
\text{Total Time} = O(n^2 d_k) + O(n^2) + O(n^2 d_v) = O(n^2(d_k + d_v))
$$

In standard Transformer configurations, the embedding dimension $d_{\text{model}}$ is typically divided equally among multiple attention heads. For a single attention head, $d_k \approx d_v \approx \frac{d_{\text{model}}}{h}$ where $h$ is the number of heads. Therefore, the complexity simplifies to:[^2]

$$
\boxed{\text{Time Complexity} = O(n^2 \cdot d_{\text{model}})}
$$

**Memory Complexity:**

The dominant memory requirement comes from storing the intermediate attention score matrix

$$
S \in \mathbb{R}^{n \times n} \quad \Rightarrow \quad \boxed{\text{Memory Complexity} = O(n^2)}
$$

Additional memory requirements include:

- Storing $Q, K, V$ matrices: $O(3 \cdot n \cdot d)$ which is $O(nd)$ overall
- Temporary buffers for computations: $O(nd)$

However, these are dominated by $O(n^2)$ for large sequence lengths.

The quadratic complexity creates severe bottlenecks for processing long sequences:

| Sequence Length | Relative Cost | Memory (GB at $d=768$) |
| :-- | :-- | :-- |
| 512 | 1× | 1.5 |
| 2,048 | 16× | 24 |
| 8,192 | 256× | 384 |
| 65,536 | 16,384× | $>6$ TB |

### Theoretical Lower Bounds

Research using complexity theory has proven that this quadratic complexity is **fundamental and unavoidable** under the Strong Exponential Time Hypothesis (SETH). The key theorem states:[^3][^1]

For the softmax dot-product self-attention with $d_q = \omega(\log n)$, for any $\epsilon > 0$:

$$
\text{Computing self-attention requires } \Omega(n^{2-\epsilon}) \text{ time}
$$

This holds even for:

- **Exact computation** of attention scores
- **Approximate computation** with multiplicative error $\mu$: $|\hat{Y}_{ij} - Y_{ij}| \leq \mu|Y_{ij}|$
- **Approximate computation** with additive error $\mu$: $|\hat{Y}_{ij} - Y_{ij}| \leq \mu$


### Why Self-Attention Cannot Escape Quadratic Complexity

The proof uses reductions from the **Orthogonal Vectors Problem (OVP)**, which is conjectured to require nearly quadratic time. The intuition is:

1. Computing attention requires evaluating pairwise interactions: $O(n^2)$ pairs
2. Each pair requires a dot product computation: $O(d_k)$ time
3. Even with approximations, one must examine enough pairs to distinguish correct answers from incorrect ones
4. Thus, the quadratic barrier in $n$ is unavoidable[^3]



## Flash Attention 

[^1]: https://arxiv.org/pdf/2209.04881.pdf

[^2]: https://apxml.com/courses/foundations-transformers-architecture/chapter-6-advanced-architectural-variants-analysis/self-attention-complexity

[^3]: https://proceedings.mlr.press/v201/duman-keles23a/duman-keles23a.pdf

[^4]: https://codesignal.com/learn/courses/sequence-models-the-dawn-of-attention-1/lessons/scaled-dot-product-attention-and-masking-in-transformers-1

[^5]: https://arxiv.org/abs/2209.04881

[^6]: https://en.wikipedia.org/wiki/Attention_(machine_learning)

[^7]: https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model

[^8]: https://www.reddit.com/r/LanguageTechnology/comments/9gulm9/complexity_of_transformer_attention_network/

[^9]: https://www.abhik.xyz/concepts/attention/scaled-dot-product

[^10]: https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html



## References

{% bibliography --cited %}