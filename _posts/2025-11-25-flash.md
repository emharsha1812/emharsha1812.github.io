---
layout: distill
title: FlashAttention Through the Years, How IO-Aware Kernels Reshaped Scalable Transformers
description: We present a technical overview of FlashAttention and its evolution across versions 1 to 4. We explain why IO-aware design became central to scalable transformers and how these kernels shape modern long-context LLMs as memory patterns and hardware limits shift. We then describe the changes across versions with diagrams and Triton examples and place these kernels in the context of recent work on efficient attention. We close by outlining principles that can guide the next generation of attention algorithms. 
giscus_comments: true
date: 2025-11-25
featured: true
draft: false
authors:
  - name: Anonymous

bibliography: 2025-11-25-flash.bib

toc:
  - name: Introduction
---


## Introduction

The fundamental concept that underpins the transformer architecture is Attention <d-cite key="vaswani2023attentionneed"></d-cite> . This was originally developed as an enhancement to RNNs for machine translation (cite - Bahdanau attention)
However in 2017, Vaswani et al. showed that significantly improved performance could be obtained by eliminating the recurrence strucutre and instead focusing exclusively on the attention mechanism. 

The importance of this mechanism can be explained with the help of the following example




Therefore, the Attention mechanism has become the single most important mechanism driving the growth of Large Language Models. Over the years several variants of attention mechanism has been proposed such as Multi Query Attention (MQA) (cite), Grouped-Query Attention (MQA), Multi-Head Latent Attention (MLA) etc. For instance, here's a non-exhaustive taxonomy of efficient attention mechanisms

{% include figure.liquid path="assets/img/attentiontaxonomy.png" class="img-fluid rounded z-depth-1" caption="Figure adapted from <d-cite key='sun2025efficient'></d-cite>" %}



## Background

One of the hardware-efficient mechanism that is now widely adopted in different providers is a Fast and Memory-Efficient Exact Attention with IO-Awareness or FlashAttention. The "IO-Awareness" part of the title describes its core technical principle, which involves optimizing the data movement. 

To give a perspective of why it was needed, the standard attention mechanism which mathematically is defined as 
For Q (Query), K (Key), V (Value) matrices  {belongs to} R^(NxDk) where N is ___ and D is the dimensions of the embedding of each token has two major flaws.

1. It is very IO dependent,(large number of memory access of standard attention)

2. The time and memory complexity of self-attention are quadaratic (O(N^2)) in sequence length. (Proof given in Appendix A - Why Self Attention is quadratic)

These two major issues make the transformers slow and memory-hungry especially on long sequences. Flash attention solves these two bottlenecks in very efficient "IO Aware" manner


Problem 1 - The HBM/Latency Problem 

{% include figure.liquid path="assets/img/Pyramid.png" class="img-fluid rounded z-depth-1" %}


When we look at the GPU memory hierarchy in detail, we see a sharp difference in both latency and bandwidth across levels. Registers and shared memory sit close to the compute units and respond within a few cycles. HBM sits hundreds of cycles away with higher bandwidth but much higher latency. This separation means that the location of data often dictates runtime. As an example, the H100 GPU has 80GB of high bandwidth memory (HBM3) with bandwidth 3.35TB/s and 256KB of on-chip SRAM per each of 132 streaming multiprocessors with bandwidth estimated around 33TB/s <d-cite key="nvidia2022h100"></d-cite>.This shows that the on-chip Sram is an order of magnitude faster than HBM but many orders of magnitude smaller in size.

The standard Attention Implementation requires $\Theta(Nd + N^2)$ HBM accesses. This can be computed as 

1. Computing $S = QK^T$: Reads $Q$ and $K$, writes $S$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$

2. Computing $P = \text{softmax}(S)$: Reads $S$, writes $P$ to HBM $\rightarrow$ $\Theta(N^2)$.

3. Computing $O = PV$: Reads $P$ and $V$, writes $O$ to HBM $\rightarrow$ $\Theta(Nd + N^2)$.



Problem 2 - Quadratic Complexity of Attention 

The standard self-attention mechanism used in Transformers is formulated as
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Where:
- $Q \in \mathbb{R}^{n \times d_k}$ is the query matrix
- $K \in \mathbb{R}^{n \times d_k}$ is the key matrix
- $V \in \mathbb{R}^{n \times d_v}$ is the value matrix
- $n$ is the sequence length
- $d_k$ and $d_v$ are the key and value dimensions
- $\sqrt{d_k}$ is the scaling factor for numerical stability

Computing the score matrix involves:

$$
S = \frac{QK^T}{\sqrt{d_k}} \in \mathbb{R}^{n \times n}
$$

This matrix multiplication requires computing the dot product between all pairs of query and key vectors. For each of the $n^2$ pairs $(i,j)$, we compute:

$$
S_{ij} = \frac{Q_i \cdot K_j^T}{\sqrt{d_k}}
$$

Since each dot product involves $d_k$ multiplications and $d_k - 1$ additions, the total computational cost is:[^1]

$$
\text{Time Complexity (Step 1)} = O(n^2 d_k)
$$

**Memory Complexity:** The $n \times n$ score matrix $S$ requires $O(n^2)$ space to store.

The softmax function is applied row-wise to normalize the attention scores
$$
\text{softmax}(S)_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^n \exp(S_{ik})}
$$

For each row $i \in [n]$, this involves:

- Computing $n$ exponentials: $O(n)$ operations
- Computing the row sum: $O(n)$ operations
- Normalizing each element: $O(n)$ operations

Since we have $n$ rows, the total cost is:

$$
\text{Time Complexity (Step 2)} = O(n^2)
$$


Finally, multiply the normalized attention matrix with the value matrix:

$$
\text{Output} = \text{softmax}(S) \cdot V
$$

Where $\text{softmax}(S) \in \mathbb{R}^{n \times n}$ and $V \in \mathbb{R}^{n \times d_v}$.

This matrix multiplication requires computing:
$$
\text{Output}_{ij} = \sum_{k=1}^n \text{softmax}(S)_{ik} \cdot V_{kj}
$$

For each of the $n \times d_v$ elements in the output, we perform $n$ operations, yielding:

$$
\text{Time Complexity (Step 3)} = O(n^2 d_v)
$$

Combining all three steps:

$$
\text{Total Time} = O(n^2 d_k) + O(n^2) + O(n^2 d_v) = O(n^2(d_k + d_v))
$$

In standard Transformer configurations, the embedding dimension $d_{\text{model}}$ is typically divided equally among multiple attention heads. For a single attention head, $d_k \approx d_v \approx \frac{d_{\text{model}}}{h}$ where $h$ is the number of heads. Therefore, the complexity simplifies to:[^2]

$$
\boxed{\text{Time Complexity} = O(n^2 \cdot d_{\text{model}})}
$$

**Memory Complexity:**

The dominant memory requirement comes from storing the intermediate attention score matrix

$$
S \in \mathbb{R}^{n \times n} \quad \Rightarrow \quad \boxed{\text{Memory Complexity} = O(n^2)}
$$

Additional memory requirements include:

- Storing $Q, K, V$ matrices: $O(3 \cdot n \cdot d)$ which is $O(nd)$ overall
- Temporary buffers for computations: $O(nd)$

However, these are dominated by $O(n^2)$ for large sequence lengths.

The quadratic complexity creates severe bottlenecks for processing long sequences:

| Sequence Length | Relative Cost | Memory (GB at $d=768$) |
| :-- | :-- | :-- |
| 512 | 1× | 1.5 |
| 2,048 | 16× | 24 |
| 8,192 | 256× | 384 |
| 65,536 | 16,384× | $>6$ TB |

### Theoretical Lower Bounds

Research using complexity theory has proven that this quadratic complexity is **fundamental and unavoidable** under the Strong Exponential Time Hypothesis (SETH). The key theorem states:[^3][^1]

For the softmax dot-product self-attention with $d_q = \omega(\log n)$, for any $\epsilon > 0$:

$$
\text{Computing self-attention requires } \Omega(n^{2-\epsilon}) \text{ time}
$$

This holds even for:

- **Exact computation** of attention scores
- **Approximate computation** with multiplicative error $\mu$: $|\hat{Y}_{ij} - Y_{ij}| \leq \mu|Y_{ij}|$
- **Approximate computation** with additive error $\mu$: $|\hat{Y}_{ij} - Y_{ij}| \leq \mu$


### Why Self-Attention Cannot Escape Quadratic Complexity

The proof uses reductions from the **Orthogonal Vectors Problem (OVP)**, which is conjectured to require nearly quadratic time. The intuition is:

1. Computing attention requires evaluating pairwise interactions: $O(n^2)$ pairs
2. Each pair requires a dot product computation: $O(d_k)$ time
3. Even with approximations, one must examine enough pairs to distinguish correct answers from incorrect ones
4. Thus, the quadratic barrier in $n$ is unavoidable[^3]

### Attempts at Sub-Quadratic Complexity

While various methods claim to reduce complexity, they involve fundamental trade-offs:[^1]

- **Sliding Window Attention:** Reduces to $O(nw)$ where $w$ is window size, but achieves only $\Omega(nw^{1-\epsilon})$ lower bound
- **Sparse Attention:** Similar rectangular lower bounds apply
- **Polynomial Approximations:** Can achieve $O(nd^p_k d_v)$ which is linear in $n$ but **exponential in polynomial order $p$**[^1]


### Summary

The quadratic complexity of standard self-attention arises from three fundamental operations that each contribute $O(n^2)$ cost:

1. **Score computation:** $O(n^2 d_k)$ - computing all pairwise similarities
2. **Softmax normalization:** $O(n^2)$ - normalizing each of $n^2$ scores
3. **Value aggregation:** $O(n^2 d_v)$ - aggregating all $n^2$ weighted values

This results in $O(n^2 \cdot d_{\text{model}})$ time complexity and $O(n^2)$ memory complexity, making it prohibitive for processing long sequences beyond 8K-16K tokens without specialized optimizations.

[^1]: https://arxiv.org/pdf/2209.04881.pdf

[^2]: https://apxml.com/courses/foundations-transformers-architecture/chapter-6-advanced-architectural-variants-analysis/self-attention-complexity

[^3]: https://proceedings.mlr.press/v201/duman-keles23a/duman-keles23a.pdf

[^4]: https://codesignal.com/learn/courses/sequence-models-the-dawn-of-attention-1/lessons/scaled-dot-product-attention-and-masking-in-transformers-1

[^5]: https://arxiv.org/abs/2209.04881

[^6]: https://en.wikipedia.org/wiki/Attention_(machine_learning)

[^7]: https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model

[^8]: https://www.reddit.com/r/LanguageTechnology/comments/9gulm9/complexity_of_transformer_attention_network/

[^9]: https://www.abhik.xyz/concepts/attention/scaled-dot-product

[^10]: https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html



## References

{% bibliography --cited %}