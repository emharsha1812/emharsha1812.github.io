- id: vaswani2023attentionneed
  title: "Attention Is All You Need"
  authors: "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
  year: 2017
  venue: "NeurIPS"
  url: "https://arxiv.org/abs/1706.03762"
  pdf: "https://arxiv.org/pdf/1706.03762.pdf"
  tags: [transformer, attention, nlp]
  summary: "Introduces the Transformer architecture, demonstrating that self-attention alone can outperform recurrent and convolutional models for sequence transduction."
  thumbnail: ""
- id: dao2022flashattention
  title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
  authors: "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher RÃ©"
  year: 2022
  venue: "arXiv"
  url: "https://arxiv.org/abs/2205.14135"
  pdf: "https://arxiv.org/pdf/2205.14135.pdf"
  tags: [attention, efficiency, gpu]
  summary: "Proposes an IO-aware tiled attention kernel that reduces memory traffic and speeds up training while remaining exact."
  thumbnail: ""
- id: sun2025efficient
  title: "Efficient Attention Mechanisms for Large Language Models: A Survey"
  authors: "Yutao Sun, Zhenyu Li, Yike Zhang, Tengyu Pan, Bowen Dong, Yuyi Guo, Jianyong Wang"
  year: 2025
  venue: "arXiv"
  url: "https://arxiv.org/abs/2507.19595"
  pdf: "https://arxiv.org/pdf/2507.19595.pdf"
  tags: [attention, survey, efficiency]
  summary: "Surveys the design space of efficient attention variants for LLMs, covering algorithmic approaches and hardware implications."
  thumbnail: ""
